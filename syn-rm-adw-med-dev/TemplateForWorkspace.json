{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "syn-rm-adw-med-dev"
		},
		"AzureSqlDatabase1_password": {
			"type": "secureString",
			"metadata": "Secure string for 'password' of 'AzureSqlDatabase1'"
		},
		"syn-rm-adw-med-dev-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'syn-rm-adw-med-dev-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:syn-rm-adw-med-dev.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"AzureSqlDatabase1_properties_typeProperties_server": {
			"type": "string",
			"defaultValue": "syn-rm-adw-med-dev.database.windows.net"
		},
		"AzureSqlDatabase1_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "master"
		},
		"AzureSqlDatabase1_properties_typeProperties_userName": {
			"type": "string",
			"defaultValue": "sqladminuser"
		},
		"syn-rm-adw-med-dev-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://adlsrmadwdev.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "LookupMetadata",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "ParquetSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": false,
									"wildcardFolderPath": "audit/csv_scr_file_inf.parquet",
									"wildcardFileName": "*",
									"enablePartitionDiscovery": true
								},
								"formatSettings": {
									"type": "ParquetReadSettings"
								}
							},
							"dataset": {
								"referenceName": "ds_parquet_csv_meta",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "FilterToImport",
						"type": "Filter",
						"state": "Inactive",
						"onInactiveMarkAs": "Succeeded",
						"dependsOn": [
							{
								"activity": "LookupMetadata",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('LookupMetadata').output.value",
								"type": "Expression"
							},
							"condition": {
								"value": "@equals(string(item().Toimport), '1')",
								"type": "Expression"
							}
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"state": "Inactive",
						"onInactiveMarkAs": "Succeeded",
						"dependsOn": [
							{
								"activity": "FilterToImport",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('FilterToImport').output.value",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "Copy_CSV_To_Parquet",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "DelimitedTextSource",
											"storeSettings": {
												"type": "AzureBlobFSReadSettings",
												"recursive": true,
												"enablePartitionDiscovery": false
											},
											"formatSettings": {
												"type": "DelimitedTextReadSettings"
											}
										},
										"sink": {
											"type": "ParquetSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings",
												"copyBehavior": "FlattenHierarchy"
											},
											"formatSettings": {
												"type": "ParquetWriteSettings"
											}
										},
										"enableStaging": false,
										"translator": {
											"type": "TabularTranslator",
											"typeConversion": true,
											"typeConversionSettings": {
												"allowDataTruncation": true,
												"treatBooleanAsNumber": false
											}
										}
									},
									"inputs": [
										{
											"referenceName": "ds_adls_csv_param",
											"type": "DatasetReference",
											"parameters": {
												"folderPath": "''",
												"fileName": {
													"value": "@item().Target_Path",
													"type": "Expression"
												}
											}
										}
									],
									"outputs": [
										{
											"referenceName": "ds_adls_parquet_param",
											"type": "DatasetReference",
											"parameters": {
												"folderPath": "bronze",
												"fileName": {
													"value": "@concat(replace(item().CSV_File_Name, '.csv', ''), '.parquet')",
													"type": "Expression"
												}
											}
										}
									]
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/ds_parquet_csv_meta')]",
				"[concat(variables('workspaceId'), '/datasets/ds_adls_csv_param')]",
				"[concat(variables('workspaceId'), '/datasets/ds_adls_parquet_param')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pl_csv_scr_config_gith')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "csv_scr_config",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "dev_csv_scr_config",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkrmadwdev",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": false
							},
							"driverSize": "Small"
						}
					},
					{
						"name": "csv_scr_file_inf",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "csv_scr_config",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "csv_scr_file_inf",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkrmadwdev",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/dev_csv_scr_config')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sparkrmadwdev')]",
				"[concat(variables('workspaceId'), '/notebooks/csv_scr_file_inf')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ds_adls_csv_param')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "syn-rm-adw-med-dev-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"folderPath": {
						"type": "string"
					},
					"fileName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().fileName",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().folderPath",
							"type": "Expression"
						}
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": false,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/syn-rm-adw-med-dev-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ds_adls_parquet_param')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "syn-rm-adw-med-dev-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"folderPath": {
						"type": "string"
					},
					"fileName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "dataset().fileName",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().folderPath",
							"type": "Expression"
						}
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/syn-rm-adw-med-dev-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ds_parquet_csv_meta')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "syn-rm-adw-med-dev-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "csv_scr_file_inf.parquet/*.parquet",
						"folderPath": "csv_metadata",
						"fileSystem": "audit"
					},
					"compressionCodec": "snappy"
				},
				"schema": [
					{
						"name": "CSV_File_Name",
						"type": "UTF8"
					},
					{
						"name": "Raw_GitHub_URL",
						"type": "UTF8"
					},
					{
						"name": "Target_Path",
						"type": "UTF8"
					},
					{
						"name": "Has_Header",
						"type": "BOOLEAN"
					},
					{
						"name": "Delimiter",
						"type": "UTF8"
					},
					{
						"name": "Schema_Definition",
						"type": "UTF8"
					},
					{
						"name": "Load_Status",
						"type": "UTF8"
					},
					{
						"name": "Load_Timestamp",
						"type": "TIMESTAMP_MICROS"
					},
					{
						"name": "Comments",
						"type": "UTF8"
					},
					{
						"name": "Toimport",
						"type": "INT32"
					},
					{
						"name": "DataLayer",
						"type": "UTF8"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/syn-rm-adw-med-dev-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlDatabase1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"server": "[parameters('AzureSqlDatabase1_properties_typeProperties_server')]",
					"database": "[parameters('AzureSqlDatabase1_properties_typeProperties_database')]",
					"encrypt": "true",
					"trustServerCertificate": false,
					"authenticationType": "SQL",
					"userName": "[parameters('AzureSqlDatabase1_properties_typeProperties_userName')]",
					"password": {
						"type": "SecureString",
						"value": "[parameters('AzureSqlDatabase1_password')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/syn-rm-adw-med-dev-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('syn-rm-adw-med-dev-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/syn-rm-adw-med-dev-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('syn-rm-adw-med-dev-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [CSV_File_Name]\n,[Raw_GitHub_URL]\n,[Target_Path]\n,[Has_Header]\n,[Delimiter]\n,[Schema_Definition]\n,[Load_Status]\n,[Load_Timestamp]\n,[Comments]\n FROM [default].[dbo].[csv_scr_file_inf]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "default",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 4')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT count(*)\n FROM [default].[dbo].[csv_scr_file_inf]\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "default",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 5')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [sno]\n,[CSV_File_Name]\n,[Raw_GitHub_URL]\n FROM [default].[dbo].[dev_csv_scr_config]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Dev_advdw",
						"poolName": "Built-in"
					},
					"resultLimit": -1
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkrmadwdev",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "450ebbcf-699f-413e-b2a3-b2c3fe835370"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f2dcf14e-c010-4080-bb14-de61bf467b98/resourceGroups/rg-rm-adw-med-dev/providers/Microsoft.Synapse/workspaces/syn-rm-adw-med-dev/bigDataPools/sparkrmadwdev",
						"name": "sparkrmadwdev",
						"type": "Spark",
						"endpoint": "https://syn-rm-adw-med-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkrmadwdev",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.5",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# ---------- Repair: drop + recreate audit table as DELTA with Toimport/DataLayer, then update rows ----------\n",
							"import traceback\n",
							"from pyspark.sql import Row\n",
							"from pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType, IntegerType\n",
							"try:\n",
							"    from notebookutils import mssparkutils\n",
							"except Exception:\n",
							"    import mssparkutils\n",
							"\n",
							"# Config (match your environment)\n",
							"ADLS_ACCOUNT = \"adlsrmadwdev\"\n",
							"AUDIT_CONTAINER = \"audit\"\n",
							"META_TABLE = \"csv_scr_file_inf\"\n",
							"PARQUET_AUDIT_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/csv_scr_file_inf.parquet\"\n",
							"AUDIT_WAREHOUSE_TABLE_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/warehouse/{META_TABLE}\"\n",
							"EXTERNAL_SQL_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/scripts/create_external_csv_scr_file_inf.sql\"\n",
							"\n",
							"def abfss(container, account, *parts):\n",
							"    suffix = \"/\".join([p.strip(\"/\") for p in parts if p is not None and p != \"\"])\n",
							"    return f\"abfss://{container}@{account}.dfs.core.windows.net/{suffix}\"\n",
							"\n",
							"print(\"1) Show existing table (if any):\")\n",
							"try:\n",
							"    tables = spark.catalog.listTables()\n",
							"    for t in tables:\n",
							"        if t.name == META_TABLE:\n",
							"            print(\" - Found table:\", t)\n",
							"except Exception as e:\n",
							"    print(\"Could not list tables:\", e)\n",
							"\n",
							"# 2) Read parquet snapshot (if exists)\n",
							"print(\"\\n2) Attempting to read audit parquet snapshot:\", PARQUET_AUDIT_PATH)\n",
							"try:\n",
							"    df_par = spark.read.parquet(PARQUET_AUDIT_PATH)\n",
							"    print(\"   Parquet read OK. Rows:\", df_par.count())\n",
							"except Exception as e:\n",
							"    print(\"   Parquet read failed:\", e)\n",
							"    # Create an empty DF with expected schema so we can still create table\n",
							"    schema_meta = StructType([\n",
							"        StructField(\"CSV_File_Name\", StringType(), True),\n",
							"        StructField(\"Raw_GitHub_URL\", StringType(), True),\n",
							"        StructField(\"Target_Path\", StringType(), True),\n",
							"        StructField(\"Has_Header\", BooleanType(), True),\n",
							"        StructField(\"Delimiter\", StringType(), True),\n",
							"        StructField(\"Schema_Definition\", StringType(), True),\n",
							"        StructField(\"Load_Status\", StringType(), True),\n",
							"        StructField(\"Load_Timestamp\", TimestampType(), True),\n",
							"        StructField(\"Comments\", StringType(), True),\n",
							"        StructField(\"Toimport\", IntegerType(), True),\n",
							"        StructField(\"DataLayer\", StringType(), True)\n",
							"    ])\n",
							"    df_par = spark.createDataFrame([], schema_meta)\n",
							"    print(\"   Created empty dataframe with expected schema.\")\n",
							"\n",
							"# 3) Drop existing table if exists (to remove old-schema table)\n",
							"print(\"\\n3) Dropping existing table if present to avoid schema mismatch...\")\n",
							"try:\n",
							"    spark.sql(f\"DROP TABLE IF EXISTS {META_TABLE}\")\n",
							"    print(\"   Dropped existing table (if present).\")\n",
							"except Exception as e:\n",
							"    print(\"   DROP TABLE failed:\", e)\n",
							"\n",
							"# 4) Create the new Delta table at the audit warehouse location using df_par\n",
							"print(\"\\n4) Writing Delta table with correct schema to:\", AUDIT_WAREHOUSE_TABLE_PATH)\n",
							"try:\n",
							"    # ensure folder exists\n",
							"    try:\n",
							"        mssparkutils.fs.mkdirs(abfss(AUDIT_CONTAINER, ADLS_ACCOUNT, \"warehouse\", META_TABLE))\n",
							"    except:\n",
							"        pass\n",
							"\n",
							"    # write as Delta and register as table\n",
							"    df_par.write.format(\"delta\").mode(\"overwrite\").option(\"path\", AUDIT_WAREHOUSE_TABLE_PATH).saveAsTable(META_TABLE)\n",
							"    print(\"   Delta table created/overwritten:\", META_TABLE)\n",
							"except Exception as e:\n",
							"    print(\"   Failed to write delta table:\", e)\n",
							"    traceback.print_exc()\n",
							"    raise\n",
							"\n",
							"# 5) Refresh table and show schema\n",
							"try:\n",
							"    spark.sql(f\"REFRESH TABLE {META_TABLE}\")\n",
							"except:\n",
							"    pass\n",
							"print(\"\\n5) Table description:\")\n",
							"try:\n",
							"    spark.sql(f\"DESCRIBE EXTENDED {META_TABLE}\").show(truncate=False)\n",
							"    print(\"\\nPreview rows (top 20):\")\n",
							"    display(spark.sql(f\"SELECT * FROM {META_TABLE} LIMIT 20\"))\n",
							"except Exception as e:\n",
							"    print(\"Could not describe/preview table:\", e)\n",
							"\n",
							"# 6) Run UPDATE to set Toimport=0 for specific files\n",
							"print(\"\\n6) Running UPDATE to set Toimport=0 for selected files...\")\n",
							"try:\n",
							"    spark.sql(f\"\"\"\n",
							"        UPDATE {META_TABLE}\n",
							"        SET Toimport = 0\n",
							"        WHERE CSV_File_Name IN ('JobCandidate_TOREMOVE.csv','ProductModelorg.csv','AWBuildVersion.csv')\n",
							"    \"\"\")\n",
							"    print(\"   UPDATE completed.\")\n",
							"except Exception as e:\n",
							"    print(\"   UPDATE failed:\", e)\n",
							"    traceback.print_exc()\n",
							"\n",
							"# 7) Verify update\n",
							"print(\"\\n7) Verify the Toimport values for those files:\")\n",
							"try:\n",
							"    df_check = spark.sql(f\"\"\"\n",
							"        SELECT CSV_File_Name, Toimport, Load_Status\n",
							"        FROM {META_TABLE}\n",
							"        WHERE CSV_File_Name IN ('JobCandidate_TOREMOVE.csv','ProductModelorg.csv','AWBuildVersion.csv')\n",
							"    \"\"\")\n",
							"    display(df_check)\n",
							"except Exception as e:\n",
							"    print(\"   Verify query failed:\", e)\n",
							"\n",
							"# 8) Re-write serverless SQL script (so external table can be created easily)\n",
							"print(\"\\n8) Writing Serverless SQL script to:\", EXTERNAL_SQL_PATH)\n",
							"external_db=\"RM_Audit\"\n",
							"external_data_source=\"ADLS_Audit\"\n",
							"external_file_format=\"ParquetFormat\"\n",
							"external_table=\"csv_scr_file_inf\"\n",
							"\n",
							"sql_text = f\"\"\"\n",
							"IF DB_ID('{external_db}') IS NULL\n",
							"    CREATE DATABASE {external_db};\n",
							"GO\n",
							"USE {external_db};\n",
							"GO\n",
							"IF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = '{external_data_source}')\n",
							"BEGIN\n",
							"    CREATE EXTERNAL DATA SOURCE {external_data_source}\n",
							"    WITH ( LOCATION = 'abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net' );\n",
							"END\n",
							"GO\n",
							"IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = '{external_file_format}')\n",
							"BEGIN\n",
							"    CREATE EXTERNAL FILE FORMAT {external_file_format}\n",
							"    WITH ( FORMAT_TYPE = PARQUET );\n",
							"END\n",
							"GO\n",
							"IF OBJECT_ID('{external_db}.dbo.{external_table}', 'U') IS NOT NULL\n",
							"    DROP EXTERNAL TABLE dbo.{external_table};\n",
							"GO\n",
							"CREATE EXTERNAL TABLE dbo.{external_table}\n",
							"(\n",
							"    CSV_File_Name NVARCHAR(1000),\n",
							"    Raw_GitHub_URL NVARCHAR(2000),\n",
							"    Target_Path NVARCHAR(2000),\n",
							"    Has_Header BIT,\n",
							"    Delimiter NVARCHAR(10),\n",
							"    Schema_Definition NVARCHAR(2000),\n",
							"    Load_Status NVARCHAR(100),\n",
							"    Load_Timestamp DATETIME2,\n",
							"    Comments NVARCHAR(2000),\n",
							"    Toimport INT,\n",
							"    DataLayer NVARCHAR(100)\n",
							")\n",
							"WITH (\n",
							"    LOCATION = 'csv_metadata/csv_scr_file_inf.parquet',\n",
							"    DATA_SOURCE = {external_data_source},\n",
							"    FILE_FORMAT = {external_file_format}\n",
							");\n",
							"GO\n",
							"\n",
							"SELECT TOP (200) * FROM dbo.{external_table};\n",
							"\"\"\"\n",
							"\n",
							"try:\n",
							"    mssparkutils.fs.put(EXTERNAL_SQL_PATH, sql_text, True)\n",
							"    print(\"   SQL script saved.\")\n",
							"except Exception as e:\n",
							"    print(\"   Failed to save SQL script:\", e)\n",
							"\n",
							"print(\"\\nDONE â€” table recreated as DELTA, updated, and SQL script saved.\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/csv_scr_file_inf')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkrmadwdev",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "47d03e08-b066-494a-8175-036ad0fac5bf"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f2dcf14e-c010-4080-bb14-de61bf467b98/resourceGroups/rg-rm-adw-med-dev/providers/Microsoft.Synapse/workspaces/syn-rm-adw-med-dev/bigDataPools/sparkrmadwdev",
						"name": "sparkrmadwdev",
						"type": "Spark",
						"endpoint": "https://syn-rm-adw-med-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkrmadwdev",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.5",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"'''\n",
							"# ---------- Run this cell to download CSVs referenced by dev_csv_scr_config (or fallback) ----------\n",
							"import requests, tempfile, os, json, traceback\n",
							"from datetime import datetime\n",
							"from pyspark.sql import Row\n",
							"from pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType, IntegerType\n",
							"from pyspark.sql.functions import lit\n",
							"\n",
							"# ---------- CONFIG ----------\n",
							"ADLS_ACCOUNT = \"adlsrmadwdev\"\n",
							"AUDIT_CONTAINER = \"audit\"\n",
							"RAW_CONTAINER = \"raw\"\n",
							"RAW_FOLDER = \"raw\"\n",
							"SPARK_META_TABLE = \"dev_csv_scr_config\"   # source metadata table\n",
							"PARQUET_META_FALLBACK = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/csv_scr_config.parquet\"\n",
							"RAW_BASE_ABFSS = f\"abfss://{RAW_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/{RAW_FOLDER}\"\n",
							"META_AUDIT_TABLE = \"csv_scr_file_inf\"\n",
							"PARQUET_AUDIT_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/csv_scr_file_inf.parquet\"\n",
							"\n",
							"# GitHub fallback info (rebuild metadata if table/parquet empty)\n",
							"G_OWNER = \"microsoft\"\n",
							"G_REPO = \"sql-server-samples\"\n",
							"G_PATH = \"samples/databases/adventure-works/oltp-install-script\"\n",
							"G_BRANCH = \"master\"\n",
							"# ------------------------\n",
							"\n",
							"# Helpers & imports for Synapse utils\n",
							"try:\n",
							"    from notebookutils import mssparkutils\n",
							"except Exception:\n",
							"    import mssparkutils\n",
							"\n",
							"def abfss(container, account, *parts):\n",
							"    return \"abfss://{}@{}.dfs.core.windows.net/{}\".format(container, account, \"/\".join([p.strip(\"/\") for p in parts if p and p!=\"\"]))\n",
							"\n",
							"def list_recursive_abfss(root):\n",
							"    try:\n",
							"        items = mssparkutils.fs.ls(root)\n",
							"    except Exception as e:\n",
							"        print(\"Could not list\", root, \":\", e)\n",
							"        return []\n",
							"    files=[]\n",
							"    for it in items:\n",
							"        if it.isDir:\n",
							"            files += list_recursive_abfss(it.path)\n",
							"        else:\n",
							"            files.append(it.path)\n",
							"    return files\n",
							"\n",
							"def ensure_folder(abfss_path):\n",
							"    try:\n",
							"        mssparkutils.fs.mkdirs(abfss_path)\n",
							"    except Exception:\n",
							"        pass\n",
							"\n",
							"# 1) Read metadata: priority table -> audit parquet -> GitHub API\n",
							"def read_metadata_df():\n",
							"    # try table\n",
							"    try:\n",
							"        df = spark.table(SPARK_META_TABLE)\n",
							"        cnt = df.count()\n",
							"        print(f\"Read Spark table {SPARK_META_TABLE}, rows={cnt}\")\n",
							"        if cnt > 0:\n",
							"            return df.select(\"sno\",\"CSV_File_Name\",\"Raw_GitHub_URL\")\n",
							"    except Exception as e:\n",
							"        print(\"Spark table read failed or missing:\", e)\n",
							"    # try fallback parquet\n",
							"    try:\n",
							"        print(\"Trying fallback parquet:\", PARQUET_META_FALLBACK)\n",
							"        df2 = spark.read.parquet(PARQUET_META_FALLBACK)\n",
							"        cnt2 = df2.count()\n",
							"        print(\"Fallback parquet rows=\", cnt2)\n",
							"        if cnt2 > 0:\n",
							"            # normalize names\n",
							"            cols = df2.columns\n",
							"            df2 = df2.withColumnRenamed(cols[0], \"sno\") if \"sno\" not in cols and len(cols)>0 else df2\n",
							"            # ensure expected columns\n",
							"            from pyspark.sql.functions import lit\n",
							"            for c in [\"sno\",\"CSV_File_Name\",\"Raw_GitHub_URL\"]:\n",
							"                if c not in df2.columns:\n",
							"                    df2 = df2.withColumn(c, lit(None))\n",
							"            return df2.select(\"sno\",\"CSV_File_Name\",\"Raw_GitHub_URL\")\n",
							"    except Exception as e:\n",
							"        print(\"Fallback parquet read failed or missing:\", e)\n",
							"\n",
							"    # fallback: call GitHub API and build DataFrame\n",
							"    print(\"Querying GitHub to rebuild metadata list from repository...\")\n",
							"    try:\n",
							"        url = f\"https://api.github.com/repos/{G_OWNER}/{G_REPO}/contents/{G_PATH}?ref={G_BRANCH}\"\n",
							"        r = requests.get(url, timeout=30)\n",
							"        r.raise_for_status()\n",
							"        items = r.json()\n",
							"        csv_files = [item[\"name\"] for item in items if item[\"name\"].lower().endswith(\".csv\")]\n",
							"        base_raw = f\"https://raw.githubusercontent.com/{G_OWNER}/{G_REPO}/{G_BRANCH}/{G_PATH}/\"\n",
							"        csv_urls = [{\"file\": f, \"url\": base_raw+f} for f in csv_files]\n",
							"        rows = [Row(sno=i+1, CSV_File_Name=c[\"file\"], Raw_GitHub_URL=c[\"url\"]) for i,c in enumerate(csv_urls)]\n",
							"        df_meta = spark.createDataFrame(rows)\n",
							"        print(\"Rebuilt metadata from GitHub, rows=\", df_meta.count())\n",
							"        return df_meta.select(\"sno\",\"CSV_File_Name\",\"Raw_GitHub_URL\")\n",
							"    except Exception as e:\n",
							"        print(\"GitHub query failed:\", e)\n",
							"        raise\n",
							"\n",
							"# Run metadata read\n",
							"meta_df = read_metadata_df()\n",
							"meta_count = 0\n",
							"try:\n",
							"    meta_count = meta_df.count()\n",
							"except:\n",
							"    meta_count = 0\n",
							"if meta_count == 0:\n",
							"    print(\"No metadata rows found (table/parquet empty and GitHub returned none). Nothing to download.\")\n",
							"    raise SystemExit(\"No source metadata to download.\")\n",
							"\n",
							"print(\"Metadata rows to process:\", meta_count)\n",
							"\n",
							"# 2) Prepare raw folder\n",
							"ensure_folder(abfss(RAW_CONTAINER, ADLS_ACCOUNT, RAW_FOLDER))\n",
							"\n",
							"existing_raw = list_recursive_abfss(RAW_BASE_ABFSS)\n",
							"existing_set = set([p.split(\"/\")[-1] for p in existing_raw])\n",
							"print(\"Existing files in raw folder:\", len(existing_set))\n",
							"\n",
							"# 3) Loop and download\n",
							"meta_rows = meta_df.collect()\n",
							"audit_rows = []\n",
							"for r in meta_rows:\n",
							"    rdict = r.asDict()\n",
							"    fname = (rdict.get(\"CSV_File_Name\") or \"\").strip()\n",
							"    furl = (rdict.get(\"Raw_GitHub_URL\") or \"\").strip()\n",
							"    if not fname:\n",
							"        if furl:\n",
							"            fname = furl.split(\"/\")[-1]\n",
							"        else:\n",
							"            print(\"Skipping row with no filename and no URL:\", rdict)\n",
							"            continue\n",
							"    target_abfss = abfss(RAW_CONTAINER, ADLS_ACCOUNT, RAW_FOLDER, fname)\n",
							"    status = \"Unknown\"\n",
							"    target_path = None\n",
							"    try:\n",
							"        if fname in existing_set or (hasattr(mssparkutils.fs, \"exists\") and mssparkutils.fs.exists(target_abfss)):\n",
							"            status = \"AlreadyExists\"\n",
							"            target_path = target_abfss\n",
							"            print(f\"SKIP: {fname} already exists.\")\n",
							"        else:\n",
							"            if not furl:\n",
							"                status = \"NoURL\"\n",
							"                print(f\"NO URL for {fname}; skipping download.\")\n",
							"            else:\n",
							"                print(f\"Downloading {fname} from {furl} ...\")\n",
							"                resp = requests.get(furl, timeout=60)\n",
							"                resp.raise_for_status()\n",
							"                # write to temp local then copy\n",
							"                tmp = tempfile.NamedTemporaryFile(delete=False)\n",
							"                tmp.write(resp.content)\n",
							"                tmp.close()\n",
							"                local_tmp = tmp.name\n",
							"                try:\n",
							"                    # attempt mssparkutils cp file: -> abfss:\n",
							"                    mssparkutils.fs.cp(f\"file:{local_tmp}\", target_abfss)\n",
							"                    status = \"Downloaded\"\n",
							"                    target_path = target_abfss\n",
							"                    print(f\"   Saved to {target_abfss}\")\n",
							"                except Exception as cp_e:\n",
							"                    print(\"   mssparkutils.cp failed:\", cp_e)\n",
							"                    # fallback: write with spark saveAsTextFile (works for csv)\n",
							"                    try:\n",
							"                        txt = resp.content.decode('utf-8', errors='replace')\n",
							"                        spark.sparkContext.parallelize([txt]).coalesce(1).saveAsTextFile(target_abfss + \".tmp\")\n",
							"                        # move part file\n",
							"                        parts = list_recursive_abfss(target_abfss + \".tmp\")\n",
							"                        part = None\n",
							"                        for p in parts:\n",
							"                            if p.split(\"/\")[-1].startswith(\"part-\"):\n",
							"                                part = p; break\n",
							"                        if part:\n",
							"                            mssparkutils.fs.cp(part, target_abfss)\n",
							"                            status = \"DownloadedViaSpark\"\n",
							"                            target_path = target_abfss\n",
							"                            print(\"   Saved via Spark fallback to\", target_abfss)\n",
							"                        else:\n",
							"                            status = \"SaveFallbackFailed\"\n",
							"                            print(\"   Could not find part file in tmp folder.\")\n",
							"                    except Exception as fallback_e:\n",
							"                        status = \"SaveFailed\"\n",
							"                        print(\"   Fallback write failed:\", fallback_e)\n",
							"                finally:\n",
							"                    try:\n",
							"                        os.remove(local_tmp)\n",
							"                    except:\n",
							"                        pass\n",
							"    except Exception as ex:\n",
							"        status = \"DownloadError\"\n",
							"        target_path = None\n",
							"        print(\"Error processing\", fname, ex, traceback.format_exc())\n",
							"\n",
							"    # detect delimiter if downloaded/available\n",
							"    delim = None\n",
							"    schema_def = None\n",
							"    has_header = False\n",
							"    if target_path:\n",
							"        try:\n",
							"            sample_df = spark.read.text(target_path).limit(5)\n",
							"            sample_lines = [x.value for x in sample_df.collect() if x.value and x.value.strip()!='']\n",
							"            if sample_lines:\n",
							"                candidates=[\",\",\"|\",\"\\t\",\";\"]\n",
							"                scores={}\n",
							"                for d in candidates:\n",
							"                    try:\n",
							"                        counts=[len(l.split(d)) for l in sample_lines]\n",
							"                        modal=max(set(counts), key=counts.count)\n",
							"                        scores[d]=counts.count(modal)*modal\n",
							"                    except:\n",
							"                        scores[d]=0\n",
							"                best=max(scores, key=lambda k: scores[k])\n",
							"                delim=best if scores[best]>0 else \",\"\n",
							"                schema_def = json.dumps([f\"col_{i+1}\" for i in range(len(sample_lines[0].split(delim)))])\n",
							"        except Exception as e:\n",
							"            print(\"Sampling failed for\", fname, e)\n",
							"\n",
							"    audit_rows.append({\n",
							"        \"CSV_File_Name\": fname,\n",
							"        \"Raw_GitHub_URL\": furl,\n",
							"        \"Target_Path\": target_path,\n",
							"        \"Has_Header\": has_header,\n",
							"        \"Delimiter\": delim,\n",
							"        \"Schema_Definition\": schema_def,\n",
							"        \"Load_Status\": status,\n",
							"        \"Load_Timestamp\": datetime.utcnow(),\n",
							"        \"Comments\": None\n",
							"    })\n",
							"\n",
							"# 4) Write audit rows (MERGE if delta exists else append)\n",
							"if audit_rows:\n",
							"    schema_meta = StructType([\n",
							"        StructField(\"CSV_File_Name\", StringType(), True),\n",
							"        StructField(\"Raw_GitHub_URL\", StringType(), True),\n",
							"        StructField(\"Target_Path\", StringType(), True),\n",
							"        StructField(\"Has_Header\", BooleanType(), True),\n",
							"        StructField(\"Delimiter\", StringType(), True),\n",
							"        StructField(\"Schema_Definition\", StringType(), True),\n",
							"        StructField(\"Load_Status\", StringType(), True),\n",
							"        StructField(\"Load_Timestamp\", TimestampType(), True),\n",
							"        StructField(\"Comments\", StringType(), True)\n",
							"    ])\n",
							"    df_updates = spark.createDataFrame([Row(**r) for r in audit_rows], schema=schema_meta)\n",
							"    df_updates.createOrReplaceTempView(\"tmp_audit_updates\")\n",
							"    merged=False\n",
							"    try:\n",
							"        from delta.tables import DeltaTable\n",
							"        if META_AUDIT_TABLE in [t.name for t in spark.catalog.listTables()]:\n",
							"            try:\n",
							"                delta_target = DeltaTable.forName(spark, META_AUDIT_TABLE)\n",
							"                merge_sql = f\"\"\"\n",
							"                MERGE INTO {META_AUDIT_TABLE} AS target\n",
							"                USING tmp_audit_updates AS source\n",
							"                ON target.CSV_File_Name = source.CSV_File_Name\n",
							"                WHEN MATCHED THEN UPDATE SET *\n",
							"                WHEN NOT MATCHED THEN INSERT *\n",
							"                \"\"\"\n",
							"                spark.sql(merge_sql)\n",
							"                merged=True\n",
							"                print(\"MERGE into audit table applied.\")\n",
							"            except Exception as e:\n",
							"                print(\"Delta MERGE failed:\", e)\n",
							"    except Exception as e:\n",
							"        print(\"Delta not available or MERGE check failed:\", e)\n",
							"\n",
							"    if not merged:\n",
							"        try:\n",
							"            df_updates.write.mode(\"append\").saveAsTable(META_AUDIT_TABLE)\n",
							"            print(\"Appended audit rows to\", META_AUDIT_TABLE)\n",
							"        except Exception as e:\n",
							"            print(\"Failed to append audit rows:\", e, traceback.format_exc())\n",
							"\n",
							"    # write parquet snapshot\n",
							"    try:\n",
							"        df_updates.write.mode(\"overwrite\").parquet(PARQUET_AUDIT_PATH)\n",
							"        print(\"Wrote audit parquet snapshot to:\", PARQUET_AUDIT_PATH)\n",
							"    except Exception as e:\n",
							"        print(\"Failed to write audit parquet:\", e, traceback.format_exc())\n",
							"\n",
							"# 5) Summary\n",
							"print(\"\\n==== Summary ====\")\n",
							"total = len(audit_rows)\n",
							"downloaded = len([r for r in audit_rows if r[\"Load_Status\"] and r[\"Load_Status\"].lower().startswith(\"download\")])\n",
							"exists = len([r for r in audit_rows if r[\"Load_Status\"]==\"AlreadyExists\"])\n",
							"errors = len([r for r in audit_rows if r[\"Load_Status\"] in (\"DownloadError\",\"SaveFailed\",\"SaveFallbackFailed\")])\n",
							"no_url = len([r for r in audit_rows if r[\"Load_Status\"]==\"NoURL\"])\n",
							"print(f\"Total processed: {total}\")\n",
							"print(f\"Downloaded: {downloaded}\")\n",
							"print(f\"Already existed: {exists}\")\n",
							"print(f\"Errors: {errors}\")\n",
							"print(f\"No URL: {no_url}\")\n",
							"print(\"Detailed statuses:\")\n",
							"for r in audit_rows:\n",
							"    print(r[\"CSV_File_Name\"], \"=>\", r[\"Load_Status\"], r[\"Target_Path\"])\n",
							"print(\"\\nYou can now inspect the raw folder in Storage Explorer or list with:\")\n",
							"print(\"  mssparkutils.fs.ls('\"+RAW_BASE_ABFSS+\"')\")\n",
							"\n",
							"# End cell\n",
							"'''"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"source": [
							"# ---------- Enhanced CSV downloader + audit writer ----------\n",
							"import requests, tempfile, os, json, traceback\n",
							"from datetime import datetime\n",
							"from pyspark.sql import Row\n",
							"from pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType, IntegerType\n",
							"from pyspark.sql.functions import lit\n",
							"\n",
							"# ---------- CONFIG ----------\n",
							"ADLS_ACCOUNT = \"adlsrmadwdev\"\n",
							"AUDIT_CONTAINER = \"audit\"\n",
							"RAW_CONTAINER = \"raw\"\n",
							"RAW_FOLDER = \"raw\"\n",
							"SPARK_META_TABLE = \"dev_csv_scr_config\"\n",
							"META_AUDIT_TABLE = \"csv_scr_file_inf\"\n",
							"PARQUET_META_FALLBACK = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/csv_scr_config.parquet\"\n",
							"PARQUET_AUDIT_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/csv_scr_file_inf.parquet\"\n",
							"RAW_BASE_ABFSS = f\"abfss://{RAW_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/{RAW_FOLDER}\"\n",
							"# GitHub fallback\n",
							"G_OWNER = \"microsoft\"\n",
							"G_REPO = \"sql-server-samples\"\n",
							"G_PATH = \"samples/databases/adventure-works/oltp-install-script\"\n",
							"G_BRANCH = \"master\"\n",
							"# ------------------------\n",
							"\n",
							"try:\n",
							"    from notebookutils import mssparkutils\n",
							"except Exception:\n",
							"    import mssparkutils\n",
							"\n",
							"def abfss(container, account, *parts):\n",
							"    return f\"abfss://{container}@{account}.dfs.core.windows.net/\" + \"/\".join([p.strip(\"/\") for p in parts if p and p!=\"\"])\n",
							"\n",
							"def ensure_folder(abfss_path):\n",
							"    try:\n",
							"        mssparkutils.fs.mkdirs(abfss_path)\n",
							"    except Exception:\n",
							"        pass\n",
							"\n",
							"def list_recursive_abfss(root):\n",
							"    try:\n",
							"        items = mssparkutils.fs.ls(root)\n",
							"    except Exception as e:\n",
							"        print(\"Could not list\", root, \":\", e)\n",
							"        return []\n",
							"    files=[]\n",
							"    for it in items:\n",
							"        if it.isDir:\n",
							"            files += list_recursive_abfss(it.path)\n",
							"        else:\n",
							"            files.append(it.path)\n",
							"    return files\n",
							"\n",
							"# -------- Read metadata --------\n",
							"def read_metadata_df():\n",
							"    try:\n",
							"        df = spark.table(SPARK_META_TABLE)\n",
							"        cnt = df.count()\n",
							"        print(f\"Read Spark table {SPARK_META_TABLE}, rows={cnt}\")\n",
							"        if cnt > 0:\n",
							"            return df.select(\"sno\",\"CSV_File_Name\",\"Raw_GitHub_URL\")\n",
							"    except Exception as e:\n",
							"        print(\"Table read failed:\", e)\n",
							"    try:\n",
							"        print(\"Trying fallback parquet:\", PARQUET_META_FALLBACK)\n",
							"        df2 = spark.read.parquet(PARQUET_META_FALLBACK)\n",
							"        cnt2 = df2.count()\n",
							"        print(\"Fallback parquet rows=\", cnt2)\n",
							"        if cnt2 > 0:\n",
							"            for c in [\"sno\",\"CSV_File_Name\",\"Raw_GitHub_URL\"]:\n",
							"                if c not in df2.columns:\n",
							"                    df2 = df2.withColumn(c, lit(None))\n",
							"            return df2.select(\"sno\",\"CSV_File_Name\",\"Raw_GitHub_URL\")\n",
							"    except Exception as e:\n",
							"        print(\"Fallback parquet read failed:\", e)\n",
							"    # GitHub rebuild\n",
							"    print(\"Building metadata from GitHub API ...\")\n",
							"    try:\n",
							"        url = f\"https://api.github.com/repos/{G_OWNER}/{G_REPO}/contents/{G_PATH}?ref={G_BRANCH}\"\n",
							"        r = requests.get(url, timeout=30)\n",
							"        r.raise_for_status()\n",
							"        items = r.json()\n",
							"        csv_files = [item[\"name\"] for item in items if item[\"name\"].lower().endswith(\".csv\")]\n",
							"        base_raw = f\"https://raw.githubusercontent.com/{G_OWNER}/{G_REPO}/{G_BRANCH}/{G_PATH}/\"\n",
							"        rows = [Row(sno=i+1, CSV_File_Name=f, Raw_GitHub_URL=base_raw+f) for i, f in enumerate(csv_files)]\n",
							"        df_meta = spark.createDataFrame(rows)\n",
							"        print(\"Built from GitHub, rows=\", df_meta.count())\n",
							"        return df_meta\n",
							"    except Exception as e:\n",
							"        print(\"GitHub fetch failed:\", e)\n",
							"        raise\n",
							"\n",
							"meta_df = read_metadata_df()\n",
							"meta_count = meta_df.count()\n",
							"if meta_count == 0:\n",
							"    raise SystemExit(\"âŒ No metadata rows found. Nothing to download.\")\n",
							"print(\"Metadata rows to process:\", meta_count)\n",
							"\n",
							"# -------- Download & audit build --------\n",
							"ensure_folder(abfss(RAW_CONTAINER, ADLS_ACCOUNT, RAW_FOLDER))\n",
							"existing_raw = list_recursive_abfss(RAW_BASE_ABFSS)\n",
							"existing_set = set([p.split(\"/\")[-1] for p in existing_raw])\n",
							"print(\"Existing raw files:\", len(existing_set))\n",
							"\n",
							"audit_rows=[]\n",
							"for r in meta_df.collect():\n",
							"    rd = r.asDict()\n",
							"    fname = (rd.get(\"CSV_File_Name\") or \"\").strip()\n",
							"    furl = (rd.get(\"Raw_GitHub_URL\") or \"\").strip()\n",
							"    if not fname and furl:\n",
							"        fname = furl.split(\"/\")[-1]\n",
							"    if not fname:\n",
							"        continue\n",
							"\n",
							"    target_abfss = abfss(RAW_CONTAINER, ADLS_ACCOUNT, RAW_FOLDER, fname)\n",
							"    status=\"Unknown\"; target_path=None; toimport=0\n",
							"    try:\n",
							"        if fname in existing_set or (hasattr(mssparkutils.fs,\"exists\") and mssparkutils.fs.exists(target_abfss)):\n",
							"            status=\"AlreadyExists\"; target_path=target_abfss; toimport=1\n",
							"        else:\n",
							"            if not furl:\n",
							"                status=\"NoURL\"\n",
							"            else:\n",
							"                print(f\"â¬‡ï¸ Downloading {fname} from {furl}\")\n",
							"                resp=requests.get(furl,timeout=60); resp.raise_for_status()\n",
							"                tmp=tempfile.NamedTemporaryFile(delete=False)\n",
							"                tmp.write(resp.content); tmp.close()\n",
							"                try:\n",
							"                    mssparkutils.fs.cp(f\"file:{tmp.name}\", target_abfss)\n",
							"                    status=\"Downloaded\"; target_path=target_abfss; toimport=1\n",
							"                except Exception as cp_e:\n",
							"                    print(\"   mssparkutils.cp failed:\", cp_e)\n",
							"                    try:\n",
							"                        txt = resp.content.decode('utf-8', errors='replace')\n",
							"                        spark.sparkContext.parallelize([txt]).coalesce(1).saveAsTextFile(target_abfss+\".tmp\")\n",
							"                        parts = list_recursive_abfss(target_abfss+\".tmp\")\n",
							"                        for p in parts:\n",
							"                            if p.split(\"/\")[-1].startswith(\"part-\"):\n",
							"                                mssparkutils.fs.cp(p, target_abfss)\n",
							"                                status=\"DownloadedViaSpark\"; target_path=target_abfss; toimport=1\n",
							"                                break\n",
							"                    except Exception as e2:\n",
							"                        status=\"SaveFailed\"\n",
							"                        print(\"   Fallback failed:\", e2)\n",
							"                finally:\n",
							"                    try: os.remove(tmp.name)\n",
							"                    except: pass\n",
							"    except Exception as e:\n",
							"        status=\"DownloadError\"\n",
							"        print(\"Error:\", e)\n",
							"\n",
							"    # Detect delimiter/schema\n",
							"    delim=None; schema_def=None; has_header=False\n",
							"    if target_path:\n",
							"        try:\n",
							"            sample_df = spark.read.text(target_path).limit(5)\n",
							"            lines=[x.value for x in sample_df.collect() if x.value and x.value.strip()!='']\n",
							"            if lines:\n",
							"                candidates=[\",\",\"|\",\"\\t\",\";\"]; scores={}\n",
							"                for d in candidates:\n",
							"                    try:\n",
							"                        counts=[len(l.split(d)) for l in lines]\n",
							"                        modal=max(set(counts), key=counts.count)\n",
							"                        scores[d]=counts.count(modal)*modal\n",
							"                    except: scores[d]=0\n",
							"                best=max(scores, key=lambda k: scores[k])\n",
							"                delim=best if scores[best]>0 else \",\"\n",
							"                schema_def=json.dumps([f\"col_{i+1}\" for i in range(len(lines[0].split(delim)))])\n",
							"        except Exception as e:\n",
							"            print(\"Sample read failed for\", fname, e)\n",
							"\n",
							"    audit_rows.append({\n",
							"        \"CSV_File_Name\": fname,\n",
							"        \"Raw_GitHub_URL\": furl,\n",
							"        \"Target_Path\": target_path,\n",
							"        \"Has_Header\": has_header,\n",
							"        \"Delimiter\": delim,\n",
							"        \"Schema_Definition\": schema_def,\n",
							"        \"Load_Status\": status,\n",
							"        \"Load_Timestamp\": datetime.utcnow(),\n",
							"        \"Comments\": None,\n",
							"        \"Toimport\": toimport,\n",
							"        \"DataLayer\": \"Raw\"\n",
							"    })\n",
							"\n",
							"print(f\"Prepared {len(audit_rows)} audit rows.\")\n",
							"\n",
							"# -------- Write audit table + parquet --------\n",
							"if audit_rows:\n",
							"    schema_meta = StructType([\n",
							"        StructField(\"CSV_File_Name\", StringType(), True),\n",
							"        StructField(\"Raw_GitHub_URL\", StringType(), True),\n",
							"        StructField(\"Target_Path\", StringType(), True),\n",
							"        StructField(\"Has_Header\", BooleanType(), True),\n",
							"        StructField(\"Delimiter\", StringType(), True),\n",
							"        StructField(\"Schema_Definition\", StringType(), True),\n",
							"        StructField(\"Load_Status\", StringType(), True),\n",
							"        StructField(\"Load_Timestamp\", TimestampType(), True),\n",
							"        StructField(\"Comments\", StringType(), True),\n",
							"        StructField(\"Toimport\", IntegerType(), True),\n",
							"        StructField(\"DataLayer\", StringType(), True)\n",
							"    ])\n",
							"    df_updates = spark.createDataFrame([Row(**r) for r in audit_rows], schema=schema_meta)\n",
							"    df_updates.createOrReplaceTempView(\"tmp_audit_updates\")\n",
							"\n",
							"    merged=False\n",
							"    try:\n",
							"        from delta.tables import DeltaTable\n",
							"        if META_AUDIT_TABLE in [t.name for t in spark.catalog.listTables()]:\n",
							"            delta_target = DeltaTable.forName(spark, META_AUDIT_TABLE)\n",
							"            merge_sql = f\"\"\"\n",
							"            MERGE INTO {META_AUDIT_TABLE} AS target\n",
							"            USING tmp_audit_updates AS source\n",
							"            ON target.CSV_File_Name = source.CSV_File_Name\n",
							"            WHEN MATCHED THEN UPDATE SET *\n",
							"            WHEN NOT MATCHED THEN INSERT *\n",
							"            \"\"\"\n",
							"            spark.sql(merge_sql); merged=True\n",
							"            print(\"âœ… MERGE applied into\", META_AUDIT_TABLE)\n",
							"    except Exception as e:\n",
							"        print(\"MERGE failed:\", e)\n",
							"\n",
							"    if not merged:\n",
							"        try:\n",
							"            df_updates.write.mode(\"append\").saveAsTable(META_AUDIT_TABLE)\n",
							"            print(\"âœ… Appended audit rows to\", META_AUDIT_TABLE)\n",
							"        except Exception as e:\n",
							"            print(\"Append failed:\", e)\n",
							"\n",
							"    try:\n",
							"        df_updates.write.mode(\"overwrite\").parquet(PARQUET_AUDIT_PATH)\n",
							"        print(\"âœ… Wrote audit parquet snapshot to:\", PARQUET_AUDIT_PATH)\n",
							"    except Exception as e:\n",
							"        print(\"Parquet write failed:\", e)\n",
							"else:\n",
							"    print(\"No audit rows to write.\")\n",
							"\n",
							"# -------- Post-update: mark some Toimport=0 --------\n",
							"try:\n",
							"    spark.sql(\"\"\"\n",
							"        UPDATE csv_scr_file_inf\n",
							"        SET Toimport = 0\n",
							"        WHERE CSV_File_Name IN ('JobCandidate_TOREMOVE.csv','ProductModelorg.csv','AWBuildVersion.csv')\n",
							"    \"\"\")\n",
							"    print(\"âœ… Updated Toimport=0 for selected files.\")\n",
							"except Exception as e:\n",
							"    print(\"âš ï¸ Update failed:\", e)\n",
							"\n",
							"# -------- Summary --------\n",
							"print(\"\\n==== SUMMARY ====\")\n",
							"total=len(audit_rows)\n",
							"ok=len([r for r in audit_rows if r[\"Toimport\"]==1])\n",
							"failed=len([r for r in audit_rows if r[\"Toimport\"]==0])\n",
							"print(f\"Total processed: {total}\")\n",
							"print(f\"Success/Ready ToImport: {ok}\")\n",
							"print(f\"Failed/Missing: {failed}\")\n",
							"print(f\"Audit table: {META_AUDIT_TABLE}\")\n",
							"print(f\"Parquet: {PARQUET_AUDIT_PATH}\")\n",
							"print(f\"Raw folder: {RAW_BASE_ABFSS}\")\n",
							"print(\"âœ… Done.\")\n",
							""
						],
						"outputs": [],
						"execution_count": 31
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dev_csv_scr_config')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkrmadwdev",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fe869059-4e48-4760-b404-1c4c8941faa8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f2dcf14e-c010-4080-bb14-de61bf467b98/resourceGroups/rg-rm-adw-med-dev/providers/Microsoft.Synapse/workspaces/syn-rm-adw-med-dev/bigDataPools/sparkrmadwdev",
						"name": "sparkrmadwdev",
						"type": "Spark",
						"endpoint": "https://syn-rm-adw-med-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkrmadwdev",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.5",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Single end-to-end Synapse PySpark cell\n",
							"import requests, json\n",
							"from pyspark.sql import Row\n",
							"from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
							"import sys\n",
							"import traceback\n",
							"\n",
							"# ---------- CONFIG ----------\n",
							"ADLS_ACCOUNT = \"adlsrmadwdev\"\n",
							"AUDIT_CONTAINER = \"audit\"\n",
							"# Parquet metadata path (inside audit container)\n",
							"PARQUET_META_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/csv_scr_config.parquet\"\n",
							"# Spark table physical location (ensure under audit)\n",
							"SPARK_TABLE_LOCATION = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/warehouse/dev_csv_scr_config\"\n",
							"SPARK_TABLE_NAME = \"dev_csv_scr_config\"\n",
							"# SQL script path to create external table (place the .sql in audit/scripts/)\n",
							"EXTERNAL_SQL_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/scripts/create_external_dev_csv_scr_config.sql\"\n",
							"# External table names/database you'd like to create in serverless SQL (operators will run this script)\n",
							"EXTERNAL_DB = \"RM_Audit\"   # database name in serverless SQL\n",
							"EXTERNAL_TABLE = \"dev_csv_scr_config\"  # external table name in serverless SQL\n",
							"EXTERNAL_DATA_SOURCE_NAME = \"ADLS_Audit\"\n",
							"EXTERNAL_FILE_FORMAT_NAME = \"ParquetFormat\"\n",
							"# GitHub source config (your existing)\n",
							"owner = \"microsoft\"\n",
							"repo = \"sql-server-samples\"\n",
							"path = \"samples/databases/adventure-works/oltp-install-script\"\n",
							"branch = \"master\"\n",
							"# ----------------------------\n",
							"\n",
							"try:\n",
							"    # 1) Build metadata DataFrame from GitHub\n",
							"    url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}?ref={branch}\"\n",
							"    resp = requests.get(url, timeout=30)\n",
							"    resp.raise_for_status()\n",
							"    items = resp.json()\n",
							"    csv_files = [item[\"name\"] for item in items if item[\"name\"].lower().endswith(\".csv\")]\n",
							"    base_raw = f\"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{path}/\"\n",
							"    csv_urls = [{\"file\": f, \"url\": base_raw + f} for f in csv_files]\n",
							"    rows = [Row(sno=i+1, CSV_File_Name=r[\"file\"], Raw_GitHub_URL=r[\"url\"]) for i, r in enumerate(csv_urls)]\n",
							"    df_meta = spark.createDataFrame(rows)\n",
							"\n",
							"    print(f\"Built df_meta with {df_meta.count()} rows.\")\n",
							"    display(df_meta.limit(20))\n",
							"\n",
							"    # 2) Write Parquet metadata to audit container\n",
							"    print(\"Writing Parquet metadata to:\", PARQUET_META_PATH)\n",
							"    df_meta.write.mode(\"overwrite\").parquet(PARQUET_META_PATH)\n",
							"    print(\"Parquet written.\")\n",
							"\n",
							"    # 3) Create Spark table AND force its data files to live under the audit/warehouse path.\n",
							"    #    Using .option(\"path\", SPARK_TABLE_LOCATION) ensures the table data is placed there.\n",
							"    print(f\"Creating Spark table '{SPARK_TABLE_NAME}' with physical location: {SPARK_TABLE_LOCATION}\")\n",
							"    df_meta.write.mode(\"overwrite\").option(\"path\", SPARK_TABLE_LOCATION).saveAsTable(SPARK_TABLE_NAME)\n",
							"    print(f\"Spark table '{SPARK_TABLE_NAME}' created/overwritten.\")\n",
							"\n",
							"    # Confirm table exists and show its storage location (DESCRIBE EXTENDED)\n",
							"    try:\n",
							"        print(\"Table info (DESCRIBE EXTENDED):\")\n",
							"        desc = spark.sql(f\"DESCRIBE EXTENDED {SPARK_TABLE_NAME}\").collect()\n",
							"        for r in desc:\n",
							"            print(r)\n",
							"    except Exception as e:\n",
							"        print(\"Could not DESCRIBE EXTENDED (non-fatal):\", e)\n",
							"\n",
							"    # 4) Produce Serverless SQL script for external table (user can run in Serverless SQL)\n",
							"    #    This script will:\n",
							"    #      - CREATE DATABASE (IF NOT EXISTS)\n",
							"    #      - CREATE EXTERNAL DATA SOURCE (pointing to the audit container)\n",
							"    #      - CREATE EXTERNAL FILE FORMAT (Parquet)\n",
							"    #      - CREATE EXTERNAL TABLE pointing to the parquet metadata file\n",
							"    #\n",
							"    #   NOTE: Serverless SQL needs proper permissions to read the ABFSS path; the SQL runner must\n",
							"    #   either use an account key or managed identity configured in the workspace.\n",
							"    sql_lines = []\n",
							"    sql_lines.append(f\"IF DB_ID('{EXTERNAL_DB}') IS NULL\\n    CREATE DATABASE {EXTERNAL_DB};\\nGO\\n\")\n",
							"    sql_lines.append(f\"USE {EXTERNAL_DB};\\nGO\\n\")\n",
							"    sql_lines.append(f\"IF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = '{EXTERNAL_DATA_SOURCE_NAME}')\\nBEGIN\\n    CREATE EXTERNAL DATA SOURCE {EXTERNAL_DATA_SOURCE_NAME}\\n    WITH ( LOCATION = 'abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net' );\\nEND\\nGO\\n\")\n",
							"    sql_lines.append(f\"IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = '{EXTERNAL_FILE_FORMAT_NAME}')\\nBEGIN\\n    CREATE EXTERNAL FILE FORMAT {EXTERNAL_FILE_FORMAT_NAME}\\n    WITH ( FORMAT_TYPE = PARQUET );\\nEND\\nGO\\n\")\n",
							"    # Define columns (based on df_meta schema)\n",
							"    # We use conservative NVARCHAR for URLs and names\n",
							"    columns_ddl = \"\"\"\n",
							"(\n",
							"    sno INT,\n",
							"    CSV_File_Name NVARCHAR(1000),\n",
							"    Raw_GitHub_URL NVARCHAR(2000)\n",
							")\n",
							"\"\"\"\n",
							"    sql_lines.append(f\"IF OBJECT_ID('{EXTERNAL_DB}.dbo.{EXTERNAL_TABLE}', 'U') IS NOT NULL\\n    DROP EXTERNAL TABLE {EXTERNAL_TABLE};\\nGO\\n\")\n",
							"    sql_lines.append(f\"CREATE EXTERNAL TABLE {EXTERNAL_TABLE} {columns_ddl}\\nWITH (\\n    LOCATION = 'csv_metadata/csv_scr_config.parquet',\\n    DATA_SOURCE = {EXTERNAL_DATA_SOURCE_NAME},\\n    FILE_FORMAT = {EXTERNAL_FILE_FORMAT_NAME}\\n);\\nGO\\n\")\n",
							"    sql_lines.append(f\"SELECT TOP (100) * FROM {EXTERNAL_TABLE};\\n\")\n",
							"    sql_script = \"\\n\".join(sql_lines)\n",
							"\n",
							"    # Save SQL script to audit scripts folder so operator can run it\n",
							"    print(\"Saving Serverless SQL script to:\", EXTERNAL_SQL_PATH)\n",
							"    # mssparkutils.fs.put expects a path and text content; ensure mssparkutils available\n",
							"    try:\n",
							"        from notebookutils import mssparkutils\n",
							"    except Exception:\n",
							"        import mssparkutils\n",
							"    # Create scripts folder if not exists (best-effort)\n",
							"    try:\n",
							"        # mssparkutils.fs.mkdirs may not exist; use try-except; cp or put will create file\n",
							"        mssparkutils.fs.put(EXTERNAL_SQL_PATH, sql_script, True)\n",
							"        print(\"SQL script written to audit container.\")\n",
							"    except Exception as e:\n",
							"        # Fallback: write via Spark to a text file and then move\n",
							"        print(\"mssparkutils.fs.put failed, trying Spark fallback to write SQL file:\", e)\n",
							"        tmp_df = spark.createDataFrame([Row(line=sql_script)])\n",
							"        tmp_local_tmp = \"/tmp/external_sql_tmp.txt\"\n",
							"        tmp_df.coalesce(1).write.mode(\"overwrite\").text(tmp_local_tmp)\n",
							"        # Try copying the part file into the target abfss path\n",
							"        try:\n",
							"            # find the part file created\n",
							"            parts = mssparkutils.fs.ls(tmp_local_tmp)\n",
							"            part_path = None\n",
							"            for p in parts:\n",
							"                if p.path.endswith(\".txt\") or p.path.endswith(\".part\") or p.path.endswith(\".crc\"):\n",
							"                    part_path = p.path\n",
							"                    break\n",
							"            if part_path:\n",
							"                mssparkutils.fs.cp(part_path, EXTERNAL_SQL_PATH)\n",
							"                print(\"SQL script written via Spark fallback.\")\n",
							"            else:\n",
							"                print(\"Could not find part file to move; SQL script not saved.\")\n",
							"        except Exception as e2:\n",
							"            print(\"Fallback write also failed:\", e2)\n",
							"\n",
							"    # 5) Verification: read back the Parquet and show Spark table preview\n",
							"    print(\"\\nVerification reads:\")\n",
							"    try:\n",
							"        df_back = spark.read.parquet(PARQUET_META_PATH)\n",
							"        print(\"Parquet read OK. Rows:\", df_back.count())\n",
							"        display(df_back.limit(50))\n",
							"    except Exception as e:\n",
							"        print(\"Parquet read failed:\", e)\n",
							"        traceback.print_exc()\n",
							"\n",
							"    try:\n",
							"        print(f\"\\nSpark table preview: SELECT * FROM {SPARK_TABLE_NAME} LIMIT 50\")\n",
							"        display(spark.sql(f\"SELECT * FROM {SPARK_TABLE_NAME} LIMIT 50\"))\n",
							"    except Exception as e:\n",
							"        print(\"Could not read spark table:\", e)\n",
							"        traceback.print_exc()\n",
							"\n",
							"    print(\"\\nEnd-to-end complete.\")\n",
							"    print(\"  - Parquet metadata:\", PARQUET_META_PATH)\n",
							"    print(\"  - Spark table:\", SPARK_TABLE_NAME, \"physical location:\", SPARK_TABLE_LOCATION)\n",
							"    print(\"  - Serverless SQL script saved to:\", EXTERNAL_SQL_PATH)\n",
							"    print(\"\\nTo create the serverless external table: open the SQL script saved in the audit container and run it in Synapse Serverless SQL.\")\n",
							"except Exception as outer_e:\n",
							"    print(\"FAILED end-to-end:\", outer_e)\n",
							"    traceback.print_exc()\n",
							"    raise\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"editable": true,
							"run_control": {
								"frozen": false
							},
							"collapsed": false
						},
						"source": [
							"'''\n",
							"# Synapse PySpark notebook cell\n",
							"import requests\n",
							"from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
							"from pyspark.sql import Row\n",
							"\n",
							"owner = \"microsoft\"\n",
							"repo = \"sql-server-samples\"\n",
							"path = \"samples/databases/adventure-works/oltp-install-script\"\n",
							"branch = \"master\"\n",
							"\n",
							"url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}?ref={branch}\"\n",
							"resp = requests.get(url)\n",
							"resp.raise_for_status()\n",
							"items = resp.json()\n",
							"\n",
							"csv_files = [item[\"name\"] for item in items if item[\"name\"].lower().endswith(\".csv\")]\n",
							"base_raw = f\"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{path}/\"\n",
							"csv_urls = [{\"file\": f, \"url\": base_raw + f} for f in csv_files]\n",
							"\n",
							"# Build Spark DataFrame\n",
							"rows = [Row(sno=i+1, CSV_File_Name=r[\"file\"], Raw_GitHub_URL=r[\"url\"]) for i, r in enumerate(csv_urls)]\n",
							"df_meta = spark.createDataFrame(rows)\n",
							"\n",
							"display(df_meta)\n",
							"\n",
							"# Write to ADLS Gen2 (Parquet)\n",
							"# Replace container/account with your values and ensure Spark has access (linked service / managed identity)\n",
							"# https://adlsrmadwdev.blob.core.windows.net/audit\n",
							"#https://adlsrmadwdev.blob.core.windows.net/audit\n",
							"#adls_path = \"abfss://<container>@<storage_account>.dfs.core.windows.net/csv_metadata/csv_scr_config.parquet\"\n",
							"#df_meta.write.mode(\"overwrite\").parquet(adls_path)\n",
							"\n",
							"adls_path = \"abfss://audit@adlsrmadwdev.dfs.core.windows.net/csv_metadata/csv_scr_config.parquet\"\n",
							"df_meta.write.mode(\"overwrite\").parquet(adls_path)\n",
							"\n",
							"\n",
							"# Optional: also register a Spark table for ad-hoc queries\n",
							"df_meta.write.mode(\"overwrite\").saveAsTable(\"dev_csv_scr_config\")  # if you want a spark catalog table\n",
							"'''"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkrmadwdev')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.5",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 2')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": []
		}
	]
}