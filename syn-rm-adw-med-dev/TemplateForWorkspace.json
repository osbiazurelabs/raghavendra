{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "syn-rm-adw-med-dev"
		},
		"syn-rm-adw-med-dev-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'syn-rm-adw-med-dev-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:syn-rm-adw-med-dev.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"syn-rm-adw-med-dev-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://adlsrmadwdev.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/pl_csv_scr_config_gith')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "csv_scr_config",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "dev_csv_scr_config",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkrmadwdev",
								"type": "BigDataPoolReference"
							},
							"conf": {
								"spark.dynamicAllocation.enabled": false
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/dev_csv_scr_config')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sparkrmadwdev')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/syn-rm-adw-med-dev-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('syn-rm-adw-med-dev-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/syn-rm-adw-med-dev-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('syn-rm-adw-med-dev-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [CSV_File_Name]\n,[Raw_GitHub_URL]\n,[Target_Path]\n,[Has_Header]\n,[Delimiter]\n,[Schema_Definition]\n,[Load_Status]\n,[Load_Timestamp]\n,[Comments]\n FROM [default].[dbo].[csv_scr_file_inf]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "default",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkrmadwdev",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2b049052-95aa-495b-9e8f-84f6bb0d2bd6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f2dcf14e-c010-4080-bb14-de61bf467b98/resourceGroups/rg-rm-adw-med-dev/providers/Microsoft.Synapse/workspaces/syn-rm-adw-med-dev/bigDataPools/sparkrmadwdev",
						"name": "sparkrmadwdev",
						"type": "Spark",
						"endpoint": "https://syn-rm-adw-med-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkrmadwdev",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.5",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Cell 1 - CONFIG & imports\n",
							"import re, json, requests, tempfile, os\n",
							"from datetime import datetime\n",
							"from pyspark.sql import Row\n",
							"from pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType\n",
							"from delta.tables import DeltaTable\n",
							"\n",
							"# Try to import mssparkutils (Synapse)\n",
							"try:\n",
							"    from notebookutils import mssparkutils\n",
							"except Exception:\n",
							"    # fallback if environment differs\n",
							"    import mssparkutils\n",
							"\n",
							"# ---------- USER CONFIG ----------\n",
							"# Source metadata table you created earlier\n",
							"CSV_META_TABLE_SOURCE = \"dev_csv_scr_config\"   # <--- your saved Spark table name\n",
							"\n",
							"# Where CSV files should live in the lake (relative path inside container)\n",
							"RAW_CONTAINER = \"raw\"\n",
							"RAW_FOLDER = \"raw\"   # will write to abfss://raw@adlsrmadwdev.dfs.core.windows.net/raw/\n",
							"RAW_BASE_DIR = f\"abfss://{RAW_CONTAINER}@adlsrmadwdev.dfs.core.windows.net/{RAW_FOLDER}\"\n",
							"\n",
							"# Audit metadata table (Delta / Spark table name)\n",
							"META_TABLE = \"csv_scr_file_inf\"   # target audit table name (will create if not exists)\n",
							"\n",
							"# Parquet location to also persist audit information (optional)\n",
							"AUDIT_PARQUET_PATH = f\"abfss://audit@adlsrmadwdev.dfs.core.windows.net/csv_metadata/csv_scr_file_inf.parquet\"\n",
							"\n",
							"# Behavior flags\n",
							"DOWNLOAD_IF_MISSING = True\n",
							"FILE_EXTENSIONS = [\".csv\", \".txt\"]\n",
							"\n",
							"# Optional: storage account key fallback secret (scope/secret) - ONLY for testing\n",
							"# If you have a secret in Azure Key Vault linked as an Azure DevOps/Workspace secret scope, set these.\n",
							"# If you don't use this fallback, keep both values as None.\n",
							"SECRET_SCOPE = \"storage-keys\"\n",
							"SECRET_NAME = \"adlsrmadwdev-key\"   # secret value should be the account key string\n",
							"# ----------------------------------\n",
							"\n",
							"print(\"Config ready.\")\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"# Cell 2 - Helpers\n",
							"\n",
							"def abfss_path(container, account, *parts):\n",
							"    \"\"\"Construct abfss path from parts\"\"\"\n",
							"    suffix = \"/\".join([p.strip(\"/\")+\"\" for p in parts if p is not None and p!=\"\"])\n",
							"    return f\"abfss://{container}@{account}.dfs.core.windows.net/{suffix}\"\n",
							"\n",
							"ADLS_ACCOUNT = \"adlsrmadwdev\"\n",
							"\n",
							"def list_files_recursive_abfss(abfss_root):\n",
							"    try:\n",
							"        items = mssparkutils.fs.ls(abfss_root)\n",
							"    except Exception as e:\n",
							"        print(f\"‚ö†Ô∏è Could not list {abfss_root}: {e}\")\n",
							"        return []\n",
							"    files = []\n",
							"    for it in items:\n",
							"        if it.isDir:\n",
							"            files += list_files_recursive_abfss(it.path)\n",
							"        else:\n",
							"            files.append(it.path)\n",
							"    return files\n",
							"\n",
							"def read_sample_lines(path, max_lines=5):\n",
							"    try:\n",
							"        df = spark.read.text(path).limit(max_lines)\n",
							"        lines = [r.value for r in df.collect()]\n",
							"        lines = [l for l in lines if l and l.strip()!='']\n",
							"        return lines\n",
							"    except Exception:\n",
							"        return []\n",
							"\n",
							"def detect_delimiter(lines, candidates=[\",\", \"|\", \"\\t\", \";\"]):\n",
							"    if not lines:\n",
							"        return \",\"\n",
							"    scores = {}\n",
							"    for d in candidates:\n",
							"        try:\n",
							"            counts = [len(l.split(d)) for l in lines]\n",
							"            modal = max(set(counts), key=counts.count)\n",
							"            scores[d] = counts.count(modal) * modal\n",
							"        except:\n",
							"            scores[d] = 0\n",
							"    best = max(scores, key=lambda k: scores[k])\n",
							"    return best if scores[best] > 0 else \",\"\n",
							"\n",
							"def lakehouse_path_exists_abfss(path):\n",
							"    # path is ABFSS full path\n",
							"    try:\n",
							"        parent = \"/\".join(path.split(\"/\")[:-1])\n",
							"        fname = path.split(\"/\")[-1]\n",
							"        for it in mssparkutils.fs.ls(parent):\n",
							"            if (not it.isDir) and it.path.endswith(\"/\" + fname):\n",
							"                return True\n",
							"        return False\n",
							"    except Exception:\n",
							"        try:\n",
							"            _ = spark.read.text(path).limit(1).collect()\n",
							"            return True\n",
							"        except Exception:\n",
							"            return False\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"# Cell 3 - Try listing container to verify auth; fallback to secret if listing fails\n",
							"\n",
							"test_path = abfss_path(RAW_CONTAINER, ADLS_ACCOUNT, \"\")\n",
							"print(\"Testing list on:\", test_path)\n",
							"can_list = True\n",
							"try:\n",
							"    _ = mssparkutils.fs.ls(test_path)\n",
							"    print(\"‚úÖ Able to list storage via workspace linked identity / default auth.\")\n",
							"except Exception as e:\n",
							"    print(\"‚ùå Default auth listing failed:\", e)\n",
							"    can_list = False\n",
							"\n",
							"# If listing failed, try using storage account key from secret scope if provided\n",
							"if not can_list and SECRET_SCOPE and SECRET_NAME:\n",
							"    try:\n",
							"        account_key = mssparkutils.secrets.get(SECRET_SCOPE, SECRET_NAME)\n",
							"        spark.conf.set(f\"fs.azure.account.key.{ADLS_ACCOUNT}.dfs.core.windows.net\", account_key)\n",
							"        # test again\n",
							"        _ = mssparkutils.fs.ls(test_path)\n",
							"        print(\"‚úÖ Authenticated using account key from secret.\")\n",
							"        can_list = True\n",
							"    except Exception as e:\n",
							"        print(\"‚ùå Account key fallback failed or secret not found:\", e)\n",
							"\n",
							"if not can_list:\n",
							"    print(\"‚ö†Ô∏è WARNING: Could not authenticate to ADLS. Please ensure workspace-linked storage or set a secret with the storage account key.\")\n",
							""
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# Show first few rows\n",
							"display(spark.sql(\"SELECT * FROM dev_csv_scr_config LIMIT 20\"))\n",
							""
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"# Cell 4 - Read source metadata and list raw files\n",
							"print(\"Reading source metadata table:\", CSV_META_TABLE_SOURCE)\n",
							"csv_meta_df = None\n",
							"try:\n",
							"    csv_meta_df = spark.table(CSV_META_TABLE_SOURCE)\n",
							"    print(f\"‚úÖ Read {CSV_META_TABLE_SOURCE} with {csv_meta_df.count()} rows.\")\n",
							"except Exception as e:\n",
							"    print(f\"‚ö†Ô∏è Could not read table {CSV_META_TABLE_SOURCE}: {e}. Proceeding with files discovered in RAW_BASE_DIR only.\")\n",
							"    csv_meta_df = None\n",
							"\n",
							"print(\"Listing current files under RAW base dir:\", RAW_BASE_DIR)\n",
							"existing_files = list_files_recursive_abfss(RAW_BASE_DIR)\n",
							"print(f\"Found {len(existing_files)} files already in RAW.\")\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"# Cell 5 - Iterate source metadata and prepare audit rows\n",
							"\n",
							"meta_updates = []\n",
							"\n",
							"def generic_cols_json(cnt):\n",
							"    if cnt <= 0:\n",
							"        return None\n",
							"    return json.dumps([f\"col_{i+1}\" for i in range(cnt)])\n",
							"\n",
							"if csv_meta_df is not None:\n",
							"    src_rows = csv_meta_df.collect()\n",
							"    for r in src_rows:\n",
							"        try:\n",
							"            rowd = r.asDict()\n",
							"        except:\n",
							"            rowd = dict(r)\n",
							"        file_name = (rowd.get(\"CSV_File_Name\") or rowd.get(\"csv_file_name\") or rowd.get(\"CSVFileName\") or rowd.get(\"csvfilename\") or \"\").strip()\n",
							"        raw_url = (rowd.get(\"Raw_GitHub_URL\") or rowd.get(\"raw_github_url\") or rowd.get(\"Raw_Github_URL\") or \"\").strip()\n",
							"\n",
							"        if not file_name:\n",
							"            # skip rows without file name\n",
							"            continue\n",
							"\n",
							"        desired_path = abfss_path(RAW_CONTAINER, ADLS_ACCOUNT, RAW_FOLDER, file_name)\n",
							"        found_path = None\n",
							"\n",
							"        # check if exists in discovered list or via API\n",
							"        if any(p.endswith(\"/\" + file_name) for p in existing_files):\n",
							"            # prefer full path from existing_files\n",
							"            matches = [p for p in existing_files if p.endswith(\"/\" + file_name)]\n",
							"            found_path = matches[0]\n",
							"        else:\n",
							"            if lakehouse_path_exists_abfss(desired_path):\n",
							"                found_path = desired_path\n",
							"\n",
							"        # If not found and allowed to download, try to fetch from raw_url\n",
							"        if not found_path and raw_url and DOWNLOAD_IF_MISSING:\n",
							"            try:\n",
							"                print(f\"‚¨áÔ∏è Downloading {file_name} from {raw_url}\")\n",
							"                resp = requests.get(raw_url, timeout=60)\n",
							"                resp.raise_for_status()\n",
							"                tmpf = tempfile.NamedTemporaryFile(delete=False)\n",
							"                tmpf.write(resp.content)\n",
							"                tmpf.close()\n",
							"                tmp_local = tmpf.name\n",
							"                target_abfss = desired_path\n",
							"\n",
							"                # Copy local temp file to abfss target using mssparkutils.fs.cp (supports file: -> abfss:)\n",
							"                try:\n",
							"                    mssparkutils.fs.cp(f\"file:{tmp_local}\", target_abfss)\n",
							"                    found_path = target_abfss\n",
							"                    print(f\"   ‚úÖ Saved to {target_abfss}\")\n",
							"                except Exception as cp_e:\n",
							"                    # Fallback: try put (text) or Spark write\n",
							"                    try:\n",
							"                        # if binary: write as bytes via put; but mssparkutils.fs.put expects text.\n",
							"                        # We'll use Spark to write as text which will work for csv.\n",
							"                        data_text = resp.content.decode('utf-8', errors='replace')\n",
							"                        spark.sparkContext.parallelize([data_text]).coalesce(1).saveAsTextFile(target_abfss + \".tmp\")\n",
							"                        # try to move the part file to desired name\n",
							"                        parts = list_files_recursive_abfss(target_abfss + \".tmp\")\n",
							"                        if parts:\n",
							"                            # find part file\n",
							"                            part = [p for p in parts if p.split(\"/\")[-1].startswith(\"part-\")]\n",
							"                            if part:\n",
							"                                # copy part file to target_abfss path\n",
							"                                mssparkutils.fs.cp(part[0], target_abfss)\n",
							"                        found_path = target_abfss\n",
							"                        print(f\"   ‚úÖ Saved to {target_abfss} via Spark fallback\")\n",
							"                    except Exception as fallback_e:\n",
							"                        print(f\"‚ùå Failed to save downloaded file {file_name}: {fallback_e}\")\n",
							"                        found_path = None\n",
							"                finally:\n",
							"                    try:\n",
							"                        os.remove(tmp_local)\n",
							"                    except:\n",
							"                        pass\n",
							"            except Exception as ex:\n",
							"                print(f\"‚ùå Download failed for {raw_url} (file {file_name}): {ex}\")\n",
							"                meta_updates.append({\n",
							"                    \"CSV_File_Name\": file_name,\n",
							"                    \"Raw_GitHub_URL\": raw_url,\n",
							"                    \"Target_Path\": None,\n",
							"                    \"Has_Header\": False,\n",
							"                    \"Delimiter\": None,\n",
							"                    \"Schema_Definition\": None,\n",
							"                    \"Load_Status\": \"DownloadError\",\n",
							"                    \"Load_Timestamp\": datetime.utcnow(),\n",
							"                    \"Comments\": f\"Download failed: {str(ex)[:200]}\"\n",
							"                })\n",
							"                continue\n",
							"\n",
							"        if found_path:\n",
							"            sample = read_sample_lines(found_path, max_lines=5)\n",
							"            delim = detect_delimiter(sample)\n",
							"            has_header_flag = False\n",
							"            col_count = len(sample[0].split(delim)) if sample else 0\n",
							"            schema_def = generic_cols_json(col_count)\n",
							"            meta_updates.append({\n",
							"                \"CSV_File_Name\": file_name,\n",
							"                \"Raw_GitHub_URL\": raw_url,\n",
							"                \"Target_Path\": found_path,\n",
							"                \"Has_Header\": has_header_flag,\n",
							"                \"Delimiter\": delim,\n",
							"                \"Schema_Definition\": schema_def,\n",
							"                \"Load_Status\": \"Available\",\n",
							"                \"Load_Timestamp\": datetime.utcnow(),\n",
							"                \"Comments\": f\"Auto-detected cols={col_count}\"\n",
							"            })\n",
							"        else:\n",
							"            meta_updates.append({\n",
							"                \"CSV_File_Name\": file_name,\n",
							"                \"Raw_GitHub_URL\": raw_url,\n",
							"                \"Target_Path\": None,\n",
							"                \"Has_Header\": False,\n",
							"                \"Delimiter\": None,\n",
							"                \"Schema_Definition\": None,\n",
							"                \"Load_Status\": \"Missing\",\n",
							"                \"Load_Timestamp\": datetime.utcnow(),\n",
							"                \"Comments\": \"File not found in lakehouse and not downloaded\"\n",
							"            })\n",
							"else:\n",
							"    # If no metadata table, scan existing files in RAW and build basic meta rows\n",
							"    for f in existing_files:\n",
							"        lower = f.lower()\n",
							"        if not any(lower.endswith(e) for e in FILE_EXTENSIONS):\n",
							"            continue\n",
							"        sample = read_sample_lines(f, max_lines=5)\n",
							"        delim = detect_delimiter(sample)\n",
							"        col_count = len(sample[0].split(delim)) if sample else 0\n",
							"        meta_updates.append({\n",
							"            \"CSV_File_Name\": f.split(\"/\")[-1],\n",
							"            \"Raw_GitHub_URL\": None,\n",
							"            \"Target_Path\": f,\n",
							"            \"Has_Header\": False,\n",
							"            \"Delimiter\": delim,\n",
							"            \"Schema_Definition\": generic_cols_json(col_count),\n",
							"            \"Load_Status\": \"Available\",\n",
							"            \"Load_Timestamp\": datetime.utcnow(),\n",
							"            \"Comments\": f\"Auto-detected cols={col_count}\"\n",
							"        })\n",
							"\n",
							"print(f\"üîß Prepared {len(meta_updates)} metadata update rows\")\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"# Cell 6 - Upsert into META_TABLE (Delta merge preferred), else append, and write audit parquet\n",
							"\n",
							"if meta_updates:\n",
							"    schema_meta = StructType([\n",
							"        StructField(\"CSV_File_Name\", StringType(), True),\n",
							"        StructField(\"Raw_GitHub_URL\", StringType(), True),\n",
							"        StructField(\"Target_Path\", StringType(), True),\n",
							"        StructField(\"Has_Header\", BooleanType(), True),\n",
							"        StructField(\"Delimiter\", StringType(), True),\n",
							"        StructField(\"Schema_Definition\", StringType(), True),\n",
							"        StructField(\"Load_Status\", StringType(), True),\n",
							"        StructField(\"Load_Timestamp\", TimestampType(), True),\n",
							"        StructField(\"Comments\", StringType(), True)\n",
							"    ])\n",
							"    df_updates = spark.createDataFrame([Row(**r) for r in meta_updates], schema=schema_meta)\n",
							"    df_updates.createOrReplaceTempView(\"tmp_meta_updates_for_merge\")\n",
							"\n",
							"    # Try Delta MERGE\n",
							"    merged = False\n",
							"    try:\n",
							"        delta_target = DeltaTable.forName(spark, META_TABLE)\n",
							"        merge_sql = f\"\"\"\n",
							"        MERGE INTO {META_TABLE} AS target\n",
							"        USING tmp_meta_updates_for_merge AS source\n",
							"        ON target.CSV_File_Name = source.CSV_File_Name\n",
							"        WHEN MATCHED THEN UPDATE SET *\n",
							"        WHEN NOT MATCHED THEN INSERT *\n",
							"        \"\"\"\n",
							"        spark.sql(merge_sql)\n",
							"        print(f\"‚úÖ MERGE applied into {META_TABLE}.\")\n",
							"        merged = True\n",
							"    except Exception as e:\n",
							"        print(f\"‚ö†Ô∏è MERGE failed (table may not exist or not Delta): {e}. Will fallback to append/create table.\")\n",
							"    \n",
							"    if not merged:\n",
							"        try:\n",
							"            # If table exists, append; else create\n",
							"            spark.sql(f\"CREATE TABLE IF NOT EXISTS {META_TABLE} USING DELTA AS SELECT * FROM tmp_meta_updates_for_merge LIMIT 0\")\n",
							"        except:\n",
							"            pass\n",
							"        df_updates.write.mode(\"overwrite\").saveAsTable(META_TABLE)  # overwrite to ensure idempotency; change to append if preferred\n",
							"        print(f\"‚úÖ Written audit table {META_TABLE} (overwrite).\")\n",
							"\n",
							"    # Also write audit parquet to ADLS (replace)\n",
							"    try:\n",
							"        print(\"Writing audit parquet to:\", AUDIT_PARQUET_PATH)\n",
							"        df_updates.write.mode(\"overwrite\").parquet(AUDIT_PARQUET_PATH)\n",
							"        print(\"‚úÖ Audit parquet written.\")\n",
							"    except Exception as e:\n",
							"        print(\"‚ùå Failed to write audit parquet:\", e)\n",
							"else:\n",
							"    print(\"‚ö†Ô∏è No metadata updates to write.\")\n",
							""
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"# Cell 7 - Preview\n",
							"print(\"Previewing latest audit rows from table:\", META_TABLE)\n",
							"try:\n",
							"    display(spark.sql(f\"SELECT CSV_File_Name, Raw_GitHub_URL, Target_Path, Load_Status, Load_Timestamp, Comments FROM {META_TABLE} ORDER BY Load_Timestamp DESC LIMIT 200\"))\n",
							"except Exception as e:\n",
							"    print(\"Preview from table failed:\", e)\n",
							"\n",
							"print(\"Previewing audit parquet (if available):\", AUDIT_PARQUET_PATH)\n",
							"try:\n",
							"    display(spark.read.parquet(AUDIT_PARQUET_PATH).orderBy(\"Load_Timestamp\", ascending=False).limit(200))\n",
							"except Exception as e:\n",
							"    print(\"Preview parquet failed:\", e)\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkrmadwdev",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "370b1997-a507-431b-a136-bf1d66dce0a3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f2dcf14e-c010-4080-bb14-de61bf467b98/resourceGroups/rg-rm-adw-med-dev/providers/Microsoft.Synapse/workspaces/syn-rm-adw-med-dev/bigDataPools/sparkrmadwdev",
						"name": "sparkrmadwdev",
						"type": "Spark",
						"endpoint": "https://syn-rm-adw-med-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkrmadwdev",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.5",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# Single end-to-end Synapse PySpark cell\n",
							"import requests, json\n",
							"from pyspark.sql import Row\n",
							"from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
							"import sys\n",
							"import traceback\n",
							"\n",
							"# ---------- CONFIG ----------\n",
							"ADLS_ACCOUNT = \"adlsrmadwdev\"\n",
							"AUDIT_CONTAINER = \"audit\"\n",
							"# Parquet metadata path (inside audit container)\n",
							"PARQUET_META_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/csv_scr_config.parquet\"\n",
							"# Spark table physical location (ensure under audit)\n",
							"SPARK_TABLE_LOCATION = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/warehouse/dev_csv_scr_config\"\n",
							"SPARK_TABLE_NAME = \"dev_csv_scr_config\"\n",
							"# SQL script path to create external table (place the .sql in audit/scripts/)\n",
							"EXTERNAL_SQL_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/scripts/create_external_dev_csv_scr_config.sql\"\n",
							"# External table names/database you'd like to create in serverless SQL (operators will run this script)\n",
							"EXTERNAL_DB = \"RM_Audit\"   # database name in serverless SQL\n",
							"EXTERNAL_TABLE = \"dev_csv_scr_config\"  # external table name in serverless SQL\n",
							"EXTERNAL_DATA_SOURCE_NAME = \"ADLS_Audit\"\n",
							"EXTERNAL_FILE_FORMAT_NAME = \"ParquetFormat\"\n",
							"# GitHub source config (your existing)\n",
							"owner = \"microsoft\"\n",
							"repo = \"sql-server-samples\"\n",
							"path = \"samples/databases/adventure-works/oltp-install-script\"\n",
							"branch = \"master\"\n",
							"# ----------------------------\n",
							"\n",
							"try:\n",
							"    # 1) Build metadata DataFrame from GitHub\n",
							"    url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}?ref={branch}\"\n",
							"    resp = requests.get(url, timeout=30)\n",
							"    resp.raise_for_status()\n",
							"    items = resp.json()\n",
							"    csv_files = [item[\"name\"] for item in items if item[\"name\"].lower().endswith(\".csv\")]\n",
							"    base_raw = f\"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{path}/\"\n",
							"    csv_urls = [{\"file\": f, \"url\": base_raw + f} for f in csv_files]\n",
							"    rows = [Row(sno=i+1, CSV_File_Name=r[\"file\"], Raw_GitHub_URL=r[\"url\"]) for i, r in enumerate(csv_urls)]\n",
							"    df_meta = spark.createDataFrame(rows)\n",
							"\n",
							"    print(f\"Built df_meta with {df_meta.count()} rows.\")\n",
							"    display(df_meta.limit(20))\n",
							"\n",
							"    # 2) Write Parquet metadata to audit container\n",
							"    print(\"Writing Parquet metadata to:\", PARQUET_META_PATH)\n",
							"    df_meta.write.mode(\"overwrite\").parquet(PARQUET_META_PATH)\n",
							"    print(\"Parquet written.\")\n",
							"\n",
							"    # 3) Create Spark table AND force its data files to live under the audit/warehouse path.\n",
							"    #    Using .option(\"path\", SPARK_TABLE_LOCATION) ensures the table data is placed there.\n",
							"    print(f\"Creating Spark table '{SPARK_TABLE_NAME}' with physical location: {SPARK_TABLE_LOCATION}\")\n",
							"    df_meta.write.mode(\"overwrite\").option(\"path\", SPARK_TABLE_LOCATION).saveAsTable(SPARK_TABLE_NAME)\n",
							"    print(f\"Spark table '{SPARK_TABLE_NAME}' created/overwritten.\")\n",
							"\n",
							"    # Confirm table exists and show its storage location (DESCRIBE EXTENDED)\n",
							"    try:\n",
							"        print(\"Table info (DESCRIBE EXTENDED):\")\n",
							"        desc = spark.sql(f\"DESCRIBE EXTENDED {SPARK_TABLE_NAME}\").collect()\n",
							"        for r in desc:\n",
							"            print(r)\n",
							"    except Exception as e:\n",
							"        print(\"Could not DESCRIBE EXTENDED (non-fatal):\", e)\n",
							"\n",
							"    # 4) Produce Serverless SQL script for external table (user can run in Serverless SQL)\n",
							"    #    This script will:\n",
							"    #      - CREATE DATABASE (IF NOT EXISTS)\n",
							"    #      - CREATE EXTERNAL DATA SOURCE (pointing to the audit container)\n",
							"    #      - CREATE EXTERNAL FILE FORMAT (Parquet)\n",
							"    #      - CREATE EXTERNAL TABLE pointing to the parquet metadata file\n",
							"    #\n",
							"    #   NOTE: Serverless SQL needs proper permissions to read the ABFSS path; the SQL runner must\n",
							"    #   either use an account key or managed identity configured in the workspace.\n",
							"    sql_lines = []\n",
							"    sql_lines.append(f\"IF DB_ID('{EXTERNAL_DB}') IS NULL\\n    CREATE DATABASE {EXTERNAL_DB};\\nGO\\n\")\n",
							"    sql_lines.append(f\"USE {EXTERNAL_DB};\\nGO\\n\")\n",
							"    sql_lines.append(f\"IF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = '{EXTERNAL_DATA_SOURCE_NAME}')\\nBEGIN\\n    CREATE EXTERNAL DATA SOURCE {EXTERNAL_DATA_SOURCE_NAME}\\n    WITH ( LOCATION = 'abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net' );\\nEND\\nGO\\n\")\n",
							"    sql_lines.append(f\"IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = '{EXTERNAL_FILE_FORMAT_NAME}')\\nBEGIN\\n    CREATE EXTERNAL FILE FORMAT {EXTERNAL_FILE_FORMAT_NAME}\\n    WITH ( FORMAT_TYPE = PARQUET );\\nEND\\nGO\\n\")\n",
							"    # Define columns (based on df_meta schema)\n",
							"    # We use conservative NVARCHAR for URLs and names\n",
							"    columns_ddl = \"\"\"\n",
							"(\n",
							"    sno INT,\n",
							"    CSV_File_Name NVARCHAR(1000),\n",
							"    Raw_GitHub_URL NVARCHAR(2000)\n",
							")\n",
							"\"\"\"\n",
							"    sql_lines.append(f\"IF OBJECT_ID('{EXTERNAL_DB}.dbo.{EXTERNAL_TABLE}', 'U') IS NOT NULL\\n    DROP EXTERNAL TABLE {EXTERNAL_TABLE};\\nGO\\n\")\n",
							"    sql_lines.append(f\"CREATE EXTERNAL TABLE {EXTERNAL_TABLE} {columns_ddl}\\nWITH (\\n    LOCATION = 'csv_metadata/csv_scr_config.parquet',\\n    DATA_SOURCE = {EXTERNAL_DATA_SOURCE_NAME},\\n    FILE_FORMAT = {EXTERNAL_FILE_FORMAT_NAME}\\n);\\nGO\\n\")\n",
							"    sql_lines.append(f\"SELECT TOP (100) * FROM {EXTERNAL_TABLE};\\n\")\n",
							"    sql_script = \"\\n\".join(sql_lines)\n",
							"\n",
							"    # Save SQL script to audit scripts folder so operator can run it\n",
							"    print(\"Saving Serverless SQL script to:\", EXTERNAL_SQL_PATH)\n",
							"    # mssparkutils.fs.put expects a path and text content; ensure mssparkutils available\n",
							"    try:\n",
							"        from notebookutils import mssparkutils\n",
							"    except Exception:\n",
							"        import mssparkutils\n",
							"    # Create scripts folder if not exists (best-effort)\n",
							"    try:\n",
							"        # mssparkutils.fs.mkdirs may not exist; use try-except; cp or put will create file\n",
							"        mssparkutils.fs.put(EXTERNAL_SQL_PATH, sql_script, True)\n",
							"        print(\"SQL script written to audit container.\")\n",
							"    except Exception as e:\n",
							"        # Fallback: write via Spark to a text file and then move\n",
							"        print(\"mssparkutils.fs.put failed, trying Spark fallback to write SQL file:\", e)\n",
							"        tmp_df = spark.createDataFrame([Row(line=sql_script)])\n",
							"        tmp_local_tmp = \"/tmp/external_sql_tmp.txt\"\n",
							"        tmp_df.coalesce(1).write.mode(\"overwrite\").text(tmp_local_tmp)\n",
							"        # Try copying the part file into the target abfss path\n",
							"        try:\n",
							"            # find the part file created\n",
							"            parts = mssparkutils.fs.ls(tmp_local_tmp)\n",
							"            part_path = None\n",
							"            for p in parts:\n",
							"                if p.path.endswith(\".txt\") or p.path.endswith(\".part\") or p.path.endswith(\".crc\"):\n",
							"                    part_path = p.path\n",
							"                    break\n",
							"            if part_path:\n",
							"                mssparkutils.fs.cp(part_path, EXTERNAL_SQL_PATH)\n",
							"                print(\"SQL script written via Spark fallback.\")\n",
							"            else:\n",
							"                print(\"Could not find part file to move; SQL script not saved.\")\n",
							"        except Exception as e2:\n",
							"            print(\"Fallback write also failed:\", e2)\n",
							"\n",
							"    # 5) Verification: read back the Parquet and show Spark table preview\n",
							"    print(\"\\nVerification reads:\")\n",
							"    try:\n",
							"        df_back = spark.read.parquet(PARQUET_META_PATH)\n",
							"        print(\"Parquet read OK. Rows:\", df_back.count())\n",
							"        display(df_back.limit(50))\n",
							"    except Exception as e:\n",
							"        print(\"Parquet read failed:\", e)\n",
							"        traceback.print_exc()\n",
							"\n",
							"    try:\n",
							"        print(f\"\\nSpark table preview: SELECT * FROM {SPARK_TABLE_NAME} LIMIT 50\")\n",
							"        display(spark.sql(f\"SELECT * FROM {SPARK_TABLE_NAME} LIMIT 50\"))\n",
							"    except Exception as e:\n",
							"        print(\"Could not read spark table:\", e)\n",
							"        traceback.print_exc()\n",
							"\n",
							"    print(\"\\nEnd-to-end complete.\")\n",
							"    print(\"  - Parquet metadata:\", PARQUET_META_PATH)\n",
							"    print(\"  - Spark table:\", SPARK_TABLE_NAME, \"physical location:\", SPARK_TABLE_LOCATION)\n",
							"    print(\"  - Serverless SQL script saved to:\", EXTERNAL_SQL_PATH)\n",
							"    print(\"\\nTo create the serverless external table: open the SQL script saved in the audit container and run it in Synapse Serverless SQL.\")\n",
							"except Exception as outer_e:\n",
							"    print(\"FAILED end-to-end:\", outer_e)\n",
							"    traceback.print_exc()\n",
							"    raise\n",
							""
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"display(spark.sql(\"SELECT COUNT(*) FROM csv_scr_file_inf\"))\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"\n",
							"display(spark.sql(\"SELECT * FROM dev_csv_scr_config LIMIT 10\"))\n",
							""
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"source": [
							"--CREATE DATABASE IF NOT EXISTS Dev_advdw;\n",
							"\n",
							"CREATE EXTERNAL DATA SOURCE ADLS_Audit\n",
							"WITH (\n",
							"    LOCATION = 'abfss://audit@adlsrmadwdev.dfs.core.windows.net'\n",
							");\n",
							"\n",
							"CREATE EXTERNAL FILE FORMAT ParquetFormat\n",
							"WITH ( FORMAT_TYPE = PARQUET );\n",
							"\n",
							"CREATE EXTERNAL TABLE RM_Audit.dev_csv_scr_config\n",
							"(\n",
							"    sno INT,\n",
							"    CSV_File_Name NVARCHAR(500),\n",
							"    Raw_GitHub_URL NVARCHAR(2000)\n",
							")\n",
							"WITH (\n",
							"    LOCATION = 'csv_metadata/csv_scr_config.parquet',\n",
							"    DATA_SOURCE = ADLS_Audit,\n",
							"    FILE_FORMAT = ParquetFormat\n",
							");\n",
							"\n",
							"SELECT TOP 10 * FROM RM_Audit.dev_csv_scr_config;\n",
							""
						],
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dev_csv_scr_config')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkrmadwdev",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "274d5031-4506-4743-b76c-55181b815610"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f2dcf14e-c010-4080-bb14-de61bf467b98/resourceGroups/rg-rm-adw-med-dev/providers/Microsoft.Synapse/workspaces/syn-rm-adw-med-dev/bigDataPools/sparkrmadwdev",
						"name": "sparkrmadwdev",
						"type": "Spark",
						"endpoint": "https://syn-rm-adw-med-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkrmadwdev",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.5",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Single end-to-end Synapse PySpark cell\n",
							"import requests, json\n",
							"from pyspark.sql import Row\n",
							"from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
							"import sys\n",
							"import traceback\n",
							"\n",
							"# ---------- CONFIG ----------\n",
							"ADLS_ACCOUNT = \"adlsrmadwdev\"\n",
							"AUDIT_CONTAINER = \"audit\"\n",
							"# Parquet metadata path (inside audit container)\n",
							"PARQUET_META_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/csv_scr_config.parquet\"\n",
							"# Spark table physical location (ensure under audit)\n",
							"SPARK_TABLE_LOCATION = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/warehouse/dev_csv_scr_config\"\n",
							"SPARK_TABLE_NAME = \"dev_csv_scr_config\"\n",
							"# SQL script path to create external table (place the .sql in audit/scripts/)\n",
							"EXTERNAL_SQL_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/scripts/create_external_dev_csv_scr_config.sql\"\n",
							"# External table names/database you'd like to create in serverless SQL (operators will run this script)\n",
							"EXTERNAL_DB = \"RM_Audit\"   # database name in serverless SQL\n",
							"EXTERNAL_TABLE = \"dev_csv_scr_config\"  # external table name in serverless SQL\n",
							"EXTERNAL_DATA_SOURCE_NAME = \"ADLS_Audit\"\n",
							"EXTERNAL_FILE_FORMAT_NAME = \"ParquetFormat\"\n",
							"# GitHub source config (your existing)\n",
							"owner = \"microsoft\"\n",
							"repo = \"sql-server-samples\"\n",
							"path = \"samples/databases/adventure-works/oltp-install-script\"\n",
							"branch = \"master\"\n",
							"# ----------------------------\n",
							"\n",
							"try:\n",
							"    # 1) Build metadata DataFrame from GitHub\n",
							"    url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}?ref={branch}\"\n",
							"    resp = requests.get(url, timeout=30)\n",
							"    resp.raise_for_status()\n",
							"    items = resp.json()\n",
							"    csv_files = [item[\"name\"] for item in items if item[\"name\"].lower().endswith(\".csv\")]\n",
							"    base_raw = f\"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{path}/\"\n",
							"    csv_urls = [{\"file\": f, \"url\": base_raw + f} for f in csv_files]\n",
							"    rows = [Row(sno=i+1, CSV_File_Name=r[\"file\"], Raw_GitHub_URL=r[\"url\"]) for i, r in enumerate(csv_urls)]\n",
							"    df_meta = spark.createDataFrame(rows)\n",
							"\n",
							"    print(f\"Built df_meta with {df_meta.count()} rows.\")\n",
							"    display(df_meta.limit(20))\n",
							"\n",
							"    # 2) Write Parquet metadata to audit container\n",
							"    print(\"Writing Parquet metadata to:\", PARQUET_META_PATH)\n",
							"    df_meta.write.mode(\"overwrite\").parquet(PARQUET_META_PATH)\n",
							"    print(\"Parquet written.\")\n",
							"\n",
							"    # 3) Create Spark table AND force its data files to live under the audit/warehouse path.\n",
							"    #    Using .option(\"path\", SPARK_TABLE_LOCATION) ensures the table data is placed there.\n",
							"    print(f\"Creating Spark table '{SPARK_TABLE_NAME}' with physical location: {SPARK_TABLE_LOCATION}\")\n",
							"    df_meta.write.mode(\"overwrite\").option(\"path\", SPARK_TABLE_LOCATION).saveAsTable(SPARK_TABLE_NAME)\n",
							"    print(f\"Spark table '{SPARK_TABLE_NAME}' created/overwritten.\")\n",
							"\n",
							"    # Confirm table exists and show its storage location (DESCRIBE EXTENDED)\n",
							"    try:\n",
							"        print(\"Table info (DESCRIBE EXTENDED):\")\n",
							"        desc = spark.sql(f\"DESCRIBE EXTENDED {SPARK_TABLE_NAME}\").collect()\n",
							"        for r in desc:\n",
							"            print(r)\n",
							"    except Exception as e:\n",
							"        print(\"Could not DESCRIBE EXTENDED (non-fatal):\", e)\n",
							"\n",
							"    # 4) Produce Serverless SQL script for external table (user can run in Serverless SQL)\n",
							"    #    This script will:\n",
							"    #      - CREATE DATABASE (IF NOT EXISTS)\n",
							"    #      - CREATE EXTERNAL DATA SOURCE (pointing to the audit container)\n",
							"    #      - CREATE EXTERNAL FILE FORMAT (Parquet)\n",
							"    #      - CREATE EXTERNAL TABLE pointing to the parquet metadata file\n",
							"    #\n",
							"    #   NOTE: Serverless SQL needs proper permissions to read the ABFSS path; the SQL runner must\n",
							"    #   either use an account key or managed identity configured in the workspace.\n",
							"    sql_lines = []\n",
							"    sql_lines.append(f\"IF DB_ID('{EXTERNAL_DB}') IS NULL\\n    CREATE DATABASE {EXTERNAL_DB};\\nGO\\n\")\n",
							"    sql_lines.append(f\"USE {EXTERNAL_DB};\\nGO\\n\")\n",
							"    sql_lines.append(f\"IF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = '{EXTERNAL_DATA_SOURCE_NAME}')\\nBEGIN\\n    CREATE EXTERNAL DATA SOURCE {EXTERNAL_DATA_SOURCE_NAME}\\n    WITH ( LOCATION = 'abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net' );\\nEND\\nGO\\n\")\n",
							"    sql_lines.append(f\"IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = '{EXTERNAL_FILE_FORMAT_NAME}')\\nBEGIN\\n    CREATE EXTERNAL FILE FORMAT {EXTERNAL_FILE_FORMAT_NAME}\\n    WITH ( FORMAT_TYPE = PARQUET );\\nEND\\nGO\\n\")\n",
							"    # Define columns (based on df_meta schema)\n",
							"    # We use conservative NVARCHAR for URLs and names\n",
							"    columns_ddl = \"\"\"\n",
							"(\n",
							"    sno INT,\n",
							"    CSV_File_Name NVARCHAR(1000),\n",
							"    Raw_GitHub_URL NVARCHAR(2000)\n",
							")\n",
							"\"\"\"\n",
							"    sql_lines.append(f\"IF OBJECT_ID('{EXTERNAL_DB}.dbo.{EXTERNAL_TABLE}', 'U') IS NOT NULL\\n    DROP EXTERNAL TABLE {EXTERNAL_TABLE};\\nGO\\n\")\n",
							"    sql_lines.append(f\"CREATE EXTERNAL TABLE {EXTERNAL_TABLE} {columns_ddl}\\nWITH (\\n    LOCATION = 'csv_metadata/csv_scr_config.parquet',\\n    DATA_SOURCE = {EXTERNAL_DATA_SOURCE_NAME},\\n    FILE_FORMAT = {EXTERNAL_FILE_FORMAT_NAME}\\n);\\nGO\\n\")\n",
							"    sql_lines.append(f\"SELECT TOP (100) * FROM {EXTERNAL_TABLE};\\n\")\n",
							"    sql_script = \"\\n\".join(sql_lines)\n",
							"\n",
							"    # Save SQL script to audit scripts folder so operator can run it\n",
							"    print(\"Saving Serverless SQL script to:\", EXTERNAL_SQL_PATH)\n",
							"    # mssparkutils.fs.put expects a path and text content; ensure mssparkutils available\n",
							"    try:\n",
							"        from notebookutils import mssparkutils\n",
							"    except Exception:\n",
							"        import mssparkutils\n",
							"    # Create scripts folder if not exists (best-effort)\n",
							"    try:\n",
							"        # mssparkutils.fs.mkdirs may not exist; use try-except; cp or put will create file\n",
							"        mssparkutils.fs.put(EXTERNAL_SQL_PATH, sql_script, True)\n",
							"        print(\"SQL script written to audit container.\")\n",
							"    except Exception as e:\n",
							"        # Fallback: write via Spark to a text file and then move\n",
							"        print(\"mssparkutils.fs.put failed, trying Spark fallback to write SQL file:\", e)\n",
							"        tmp_df = spark.createDataFrame([Row(line=sql_script)])\n",
							"        tmp_local_tmp = \"/tmp/external_sql_tmp.txt\"\n",
							"        tmp_df.coalesce(1).write.mode(\"overwrite\").text(tmp_local_tmp)\n",
							"        # Try copying the part file into the target abfss path\n",
							"        try:\n",
							"            # find the part file created\n",
							"            parts = mssparkutils.fs.ls(tmp_local_tmp)\n",
							"            part_path = None\n",
							"            for p in parts:\n",
							"                if p.path.endswith(\".txt\") or p.path.endswith(\".part\") or p.path.endswith(\".crc\"):\n",
							"                    part_path = p.path\n",
							"                    break\n",
							"            if part_path:\n",
							"                mssparkutils.fs.cp(part_path, EXTERNAL_SQL_PATH)\n",
							"                print(\"SQL script written via Spark fallback.\")\n",
							"            else:\n",
							"                print(\"Could not find part file to move; SQL script not saved.\")\n",
							"        except Exception as e2:\n",
							"            print(\"Fallback write also failed:\", e2)\n",
							"\n",
							"    # 5) Verification: read back the Parquet and show Spark table preview\n",
							"    print(\"\\nVerification reads:\")\n",
							"    try:\n",
							"        df_back = spark.read.parquet(PARQUET_META_PATH)\n",
							"        print(\"Parquet read OK. Rows:\", df_back.count())\n",
							"        display(df_back.limit(50))\n",
							"    except Exception as e:\n",
							"        print(\"Parquet read failed:\", e)\n",
							"        traceback.print_exc()\n",
							"\n",
							"    try:\n",
							"        print(f\"\\nSpark table preview: SELECT * FROM {SPARK_TABLE_NAME} LIMIT 50\")\n",
							"        display(spark.sql(f\"SELECT * FROM {SPARK_TABLE_NAME} LIMIT 50\"))\n",
							"    except Exception as e:\n",
							"        print(\"Could not read spark table:\", e)\n",
							"        traceback.print_exc()\n",
							"\n",
							"    print(\"\\nEnd-to-end complete.\")\n",
							"    print(\"  - Parquet metadata:\", PARQUET_META_PATH)\n",
							"    print(\"  - Spark table:\", SPARK_TABLE_NAME, \"physical location:\", SPARK_TABLE_LOCATION)\n",
							"    print(\"  - Serverless SQL script saved to:\", EXTERNAL_SQL_PATH)\n",
							"    print(\"\\nTo create the serverless external table: open the SQL script saved in the audit container and run it in Synapse Serverless SQL.\")\n",
							"except Exception as outer_e:\n",
							"    print(\"FAILED end-to-end:\", outer_e)\n",
							"    traceback.print_exc()\n",
							"    raise\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"editable": false,
							"run_control": {
								"frozen": true
							},
							"collapsed": false
						},
						"source": [
							"# Synapse PySpark notebook cell\n",
							"import requests\n",
							"from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
							"from pyspark.sql import Row\n",
							"\n",
							"owner = \"microsoft\"\n",
							"repo = \"sql-server-samples\"\n",
							"path = \"samples/databases/adventure-works/oltp-install-script\"\n",
							"branch = \"master\"\n",
							"\n",
							"url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}?ref={branch}\"\n",
							"resp = requests.get(url)\n",
							"resp.raise_for_status()\n",
							"items = resp.json()\n",
							"\n",
							"csv_files = [item[\"name\"] for item in items if item[\"name\"].lower().endswith(\".csv\")]\n",
							"base_raw = f\"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{path}/\"\n",
							"csv_urls = [{\"file\": f, \"url\": base_raw + f} for f in csv_files]\n",
							"\n",
							"# Build Spark DataFrame\n",
							"rows = [Row(sno=i+1, CSV_File_Name=r[\"file\"], Raw_GitHub_URL=r[\"url\"]) for i, r in enumerate(csv_urls)]\n",
							"df_meta = spark.createDataFrame(rows)\n",
							"\n",
							"display(df_meta)\n",
							"\n",
							"# Write to ADLS Gen2 (Parquet)\n",
							"# Replace container/account with your values and ensure Spark has access (linked service / managed identity)\n",
							"# https://adlsrmadwdev.blob.core.windows.net/audit\n",
							"#https://adlsrmadwdev.blob.core.windows.net/audit\n",
							"#adls_path = \"abfss://<container>@<storage_account>.dfs.core.windows.net/csv_metadata/csv_scr_config.parquet\"\n",
							"#df_meta.write.mode(\"overwrite\").parquet(adls_path)\n",
							"\n",
							"adls_path = \"abfss://audit@adlsrmadwdev.dfs.core.windows.net/csv_metadata/csv_scr_config.parquet\"\n",
							"df_meta.write.mode(\"overwrite\").parquet(adls_path)\n",
							"\n",
							"\n",
							"# Optional: also register a Spark table for ad-hoc queries\n",
							"df_meta.write.mode(\"overwrite\").saveAsTable(\"dev_csv_scr_config\")  # if you want a spark catalog table\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkrmadwdev')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.5",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		}
	]
}