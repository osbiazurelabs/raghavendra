{
	"name": "04_sql_table_metadata",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkrmadwdev",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "638447ae-842d-4b56-846d-88db52e16f18"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f2dcf14e-c010-4080-bb14-de61bf467b98/resourceGroups/rg-rm-adw-med-dev/providers/Microsoft.Synapse/workspaces/syn-rm-adw-med-dev/bigDataPools/sparkrmadwdev",
				"name": "sparkrmadwdev",
				"type": "Spark",
				"endpoint": "https://syn-rm-adw-med-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkrmadwdev",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.5",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# ================= Read SQL file (robust) -> parse -> insert metadata into Azure SQL =================\n",
					"# Paste into Synapse PySpark notebook. Edit JDBC_USER/JDBC_PASSWORD before running.\n",
					"\n",
					"import traceback, requests, re, json, os\n",
					"from pyspark.sql import Row\n",
					"from pyspark.sql.types import StructType, StructField, StringType\n",
					"from pyspark.sql.functions import lit\n",
					"\n",
					"# ------------- CONFIG -------------\n",
					"# ABFSS path to your file (preferred in Synapse)\n",
					"ABFSS_PATH = \"abfss://audit@adlsrmadwdev.dfs.core.windows.net/SQL/instawdb.txt\"\n",
					"# HTTPS path fallback (public blob URL)\n",
					"HTTP_URL   = \"https://adlsrmadwdev.blob.core.windows.net/audit/SQL/instawdb.txt\"\n",
					"\n",
					"# Spark table fallback name (if you already parsed and saved before)\n",
					"SRC_SPARK_TABLE = \"sql_table_metadata\"\n",
					"\n",
					"# Azure SQL JDBC config - EDIT BEFORE RUNNING\n",
					"JDBC_SERVER = \"rmsqlazure.database.windows.net\"\n",
					"JDBC_DB     = \"rmsqlazure\"\n",
					"JDBC_PORT   = 1433\n",
					"JDBC_USER   = \"sqladminuser\"          # <-- set your SQL login\n",
					"JDBC_PASSWORD = \"P@ssWoRd@OSBi2025\" # <-- set your SQL password\n",
					"JDBC_TARGET_TABLE = \"dbo.sql_table_metadata\"\n",
					"# -----------------------------------\n",
					"\n",
					"# helper: try mssparkutils\n",
					"try:\n",
					"    from notebookutils import mssparkutils\n",
					"except Exception:\n",
					"    import mssparkutils\n",
					"\n",
					"def read_text_from_abfss(abfss_path):\n",
					"    \"\"\"Try different methods to read a text file from ABFSS into a Python string.\"\"\"\n",
					"    # 1) fast head (good for small files)\n",
					"    try:\n",
					"        txt = mssparkutils.fs.head(abfss_path, 10_000_000)  # up to 10MB\n",
					"        if txt and len(txt) > 0:\n",
					"            print(\"Read file via mssparkutils.fs.head()\")\n",
					"            return txt\n",
					"    except Exception as e:\n",
					"        print(\"mssparkutils.fs.head failed:\", e)\n",
					"    # 2) spark.read.text (robust for bigger files)\n",
					"    try:\n",
					"        df = spark.read.text(abfss_path)\n",
					"        if df.count() > 0:\n",
					"            lines = [r.value for r in df.collect()]\n",
					"            print(\"Read file via spark.read.text()\")\n",
					"            return \"\\n\".join(lines)\n",
					"    except Exception as e:\n",
					"        print(\"spark.read.text failed:\", e)\n",
					"    # 3) try HTTP(s) fallback (requires public access)\n",
					"    try:\n",
					"        r = requests.get(HTTP_URL, timeout=60)\n",
					"        r.raise_for_status()\n",
					"        print(\"Read file via HTTP GET fallback\")\n",
					"        return r.text\n",
					"    except Exception as e:\n",
					"        print(\"HTTP fallback failed:\", e)\n",
					"    return None\n",
					"\n",
					"# Step 1: Try to read parsed table from Spark catalog\n",
					"df_src = None\n",
					"try:\n",
					"    if SRC_SPARK_TABLE in [t.name for t in spark.catalog.listTables()]:\n",
					"        df_src = spark.table(SRC_SPARK_TABLE)\n",
					"        print(f\"Using existing Spark table '{SRC_SPARK_TABLE}', rows={df_src.count()}\")\n",
					"    else:\n",
					"        print(f\"Spark table '{SRC_SPARK_TABLE}' not found. Will attempt to read file from ABFSS/HTTP.\")\n",
					"except Exception as e:\n",
					"    print(\"Error checking Spark catalog:\", e)\n",
					"    traceback.print_exc()\n",
					"\n",
					"# If no Spark table, read text file and parse\n",
					"sql_text = None\n",
					"if df_src is None:\n",
					"    sql_text = read_text_from_abfss(ABFSS_PATH)\n",
					"    if not sql_text:\n",
					"        raise SystemExit(f\"Failed to read SQL file from ABFSS ({ABFSS_PATH}) and HTTP ({HTTP_URL}).\")\n",
					"\n",
					"    print(f\"Read SQL text length = {len(sql_text)} characters\")\n",
					"\n",
					"    # ---------------- Parse CREATE TABLE blocks ----------------\n",
					"    def parse_create_table_blocks(sql_text):\n",
					"        tables = []\n",
					"        # Remove comment styles\n",
					"        t = re.sub(r\"--.*\", \"\", sql_text)  # remove single-line comments\n",
					"        t = re.sub(r\"/\\*.*?\\*/\", \"\", t, flags=re.DOTALL)  # remove block comments\n",
					"\n",
					"        pattern = re.compile(\n",
					"            r\"CREATE\\s+TABLE\\s+([\\[\\]\\\"`\\w\\.\\-]+)\\s*\\((.*?)\\)\\s*(?:ON|WITH|;|$)\",\n",
					"            flags=re.IGNORECASE | re.DOTALL,\n",
					"        )\n",
					"        for match in pattern.finditer(t):\n",
					"            full_table_name = match.group(1).strip().replace(\"[\",\"\").replace(\"]\",\"\").replace('\"',\"\").replace(\"`\",\"\")\n",
					"            columns_block = match.group(2).strip()\n",
					"            table_info = {\n",
					"                \"table_name\": full_table_name,\n",
					"                \"columns\": [],\n",
					"                \"primary_keys\": [],\n",
					"                \"foreign_keys\": [],\n",
					"                \"unique_keys\": [],\n",
					"                \"raw_sql\": match.group(0).strip()\n",
					"            }\n",
					"\n",
					"            # Split safely on commas not inside parentheses\n",
					"            parts = []\n",
					"            depth = 0\n",
					"            current = \"\"\n",
					"            for ch in columns_block:\n",
					"                if ch == \"(\":\n",
					"                    depth += 1\n",
					"                elif ch == \")\":\n",
					"                    depth -= 1\n",
					"                if ch == \",\" and depth == 0:\n",
					"                    parts.append(current)\n",
					"                    current = \"\"\n",
					"                else:\n",
					"                    current += ch\n",
					"            if current.strip():\n",
					"                parts.append(current)\n",
					"\n",
					"            for line in parts:\n",
					"                l = line.strip()\n",
					"                if not l:\n",
					"                    continue\n",
					"                # Constraints\n",
					"                if re.match(r\"PRIMARY\\s+KEY\", l, re.IGNORECASE):\n",
					"                    pk_cols = re.findall(r\"\\((.*?)\\)\", l)\n",
					"                    if pk_cols:\n",
					"                        table_info[\"primary_keys\"] += [c.strip().strip(\"[]\") for c in pk_cols[0].split(\",\")]\n",
					"                elif re.match(r\"FOREIGN\\s+KEY\", l, re.IGNORECASE):\n",
					"                    fk = re.findall(r\"FOREIGN\\s+KEY\\s*\\((.*?)\\)\\s*REFERENCES\\s+([\\[\\]\\\"`\\w\\.]+)\\s*\\((.*?)\\)\", l, flags=re.IGNORECASE)\n",
					"                    if fk:\n",
					"                        src_cols = [x.strip().strip(\"[]\") for x in fk[0][0].split(\",\")]\n",
					"                        ref_table = fk[0][1].replace(\"[\",\"\").replace(\"]\",\"\")\n",
					"                        ref_cols = [x.strip().strip(\"[]\") for x in fk[0][2].split(\",\")]\n",
					"                        table_info[\"foreign_keys\"].append({\n",
					"                            \"source_columns\": src_cols,\n",
					"                            \"referenced_table\": ref_table,\n",
					"                            \"referenced_columns\": ref_cols\n",
					"                        })\n",
					"                elif re.match(r\"UNIQUE\", l, re.IGNORECASE):\n",
					"                    uq_cols = re.findall(r\"\\((.*?)\\)\", l)\n",
					"                    if uq_cols:\n",
					"                        table_info[\"unique_keys\"] += [c.strip().strip(\"[]\") for c in uq_cols[0].split(\",\")]\n",
					"                elif not l.upper().startswith((\"CONSTRAINT\",\"PRIMARY\",\"FOREIGN\",\"UNIQUE\",\"CHECK\")):\n",
					"                    col_match = re.match(r\"([\\[\\]\\\"`\\w]+)\\s+(.+)\", l)\n",
					"                    if col_match:\n",
					"                        col_name = col_match.group(1).replace(\"[\",\"\").replace(\"]\",\"\").replace('\"',\"\").replace(\"`\",\"\")\n",
					"                        col_type = re.split(r\"\\s+\", col_match.group(2).strip())[0]\n",
					"                        table_info[\"columns\"].append({\n",
					"                            \"column_name\": col_name,\n",
					"                            \"data_type\": col_type,\n",
					"                            \"definition\": l\n",
					"                        })\n",
					"            tables.append(table_info)\n",
					"        return tables\n",
					"\n",
					"    sql_tables = parse_create_table_blocks(sql_text)\n",
					"\n",
					"    # Build records list like your earlier code\n",
					"    records = []\n",
					"    for t in sql_tables:\n",
					"        records.append({\n",
					"            \"table_name\": t[\"table_name\"],\n",
					"            \"column_names_json\": json.dumps([c[\"column_name\"] for c in t[\"columns\"]]),\n",
					"            \"column_defs_json\": json.dumps(t[\"columns\"]),\n",
					"            \"primary_keys_json\": json.dumps(t[\"primary_keys\"]),\n",
					"            \"foreign_keys_json\": json.dumps(t[\"foreign_keys\"]),\n",
					"            \"unique_keys_json\": json.dumps(t[\"unique_keys\"]),\n",
					"            \"raw_sql\": t[\"raw_sql\"]\n",
					"        })\n",
					"\n",
					"    if not records:\n",
					"        print(\"⚠️ No CREATE TABLE blocks parsed from the SQL file. Exiting.\")\n",
					"        # Optionally, create a minimal row with raw_sql content\n",
					"        records = [{\n",
					"            \"table_name\": \"__raw_instawdb__\",\n",
					"            \"column_names_json\": \"[]\",\n",
					"            \"column_defs_json\": \"[]\",\n",
					"            \"primary_keys_json\": \"[]\",\n",
					"            \"foreign_keys_json\": \"[]\",\n",
					"            \"unique_keys_json\": \"[]\",\n",
					"            \"raw_sql\": sql_text[:10000]  # truncated preview\n",
					"        }]\n",
					"\n",
					"    # Create DataFrame to write / preview\n",
					"    schema_sql_meta = StructType([\n",
					"        StructField(\"table_name\", StringType(), True),\n",
					"        StructField(\"column_names_json\", StringType(), True),\n",
					"        StructField(\"column_defs_json\", StringType(), True),\n",
					"        StructField(\"primary_keys_json\", StringType(), True),\n",
					"        StructField(\"foreign_keys_json\", StringType(), True),\n",
					"        StructField(\"unique_keys_json\", StringType(), True),\n",
					"        StructField(\"raw_sql\", StringType(), True)\n",
					"    ])\n",
					"    df_sql_meta = spark.createDataFrame([Row(**r) for r in records], schema=schema_sql_meta)\n",
					"    print(f\"Parsed {df_sql_meta.count()} table(s) from SQL file. Preview:\")\n",
					"    display(df_sql_meta.limit(20))\n",
					"else:\n",
					"    # we have a Spark table already; ensure expected columns present\n",
					"    df_sql_meta = df_src\n",
					"    expected = [\"table_name\",\"column_names_json\",\"column_defs_json\",\"primary_keys_json\",\"foreign_keys_json\",\"unique_keys_json\",\"raw_sql\"]\n",
					"    # normalize columns\n",
					"    cols_lower = [c.lower() for c in df_sql_meta.columns]\n",
					"    for e in expected:\n",
					"        if e not in df_sql_meta.columns and e.lower() in cols_lower:\n",
					"            df_sql_meta = df_sql_meta.withColumnRenamed(df_sql_meta.columns[cols_lower.index(e.lower())], e)\n",
					"    for e in expected:\n",
					"        if e not in df_sql_meta.columns:\n",
					"            df_sql_meta = df_sql_meta.withColumn(e, lit(None))\n",
					"    df_sql_meta = df_sql_meta.select(*expected)\n",
					"    print(f\"Using existing Spark metadata table '{SRC_SPARK_TABLE}', rows={df_sql_meta.count()}\")\n",
					"    display(df_sql_meta.limit(20))\n",
					"\n",
					"# ---------- Write df_sql_meta into Azure SQL via JDBC ----------\n",
					"jdbc_url = f\"jdbc:sqlserver://{JDBC_SERVER}:{JDBC_PORT};database={JDBC_DB};encrypt=true;trustServerCertificate=false;\"\n",
					"jdbc_props = {\"user\": JDBC_USER, \"password\": JDBC_PASSWORD, \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"}\n",
					"\n",
					"try:\n",
					"    print(f\"Writing {df_sql_meta.count()} rows to Azure SQL table {JDBC_TARGET_TABLE} ...\")\n",
					"    # Prefer append to avoid accidental overwrite; ensure target table exists with proper schema (recommended)\n",
					"    df_sql_meta.write.jdbc(url=jdbc_url, table=JDBC_TARGET_TABLE, mode=\"append\", properties=jdbc_props)\n",
					"    print(\"✅ Write to Azure SQL succeeded.\")\n",
					"except Exception as e:\n",
					"    print(\"❌ Write to Azure SQL failed:\", e)\n",
					"    traceback.print_exc()\n",
					"    print(\"Previewing rows locally for debugging:\")\n",
					"    display(df_sql_meta.limit(50))\n",
					"    raise\n",
					"\n",
					"# Done\n",
					"print(\"All finished.\")\n",
					""
				],
				"execution_count": 2
			}
		]
	}
}