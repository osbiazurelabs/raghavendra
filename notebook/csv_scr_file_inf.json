{
	"name": "csv_scr_file_inf",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkrmadwdev",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "47d03e08-b066-494a-8175-036ad0fac5bf"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f2dcf14e-c010-4080-bb14-de61bf467b98/resourceGroups/rg-rm-adw-med-dev/providers/Microsoft.Synapse/workspaces/syn-rm-adw-med-dev/bigDataPools/sparkrmadwdev",
				"name": "sparkrmadwdev",
				"type": "Spark",
				"endpoint": "https://syn-rm-adw-med-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkrmadwdev",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.5",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"'''\n",
					"# ---------- Run this cell to download CSVs referenced by dev_csv_scr_config (or fallback) ----------\n",
					"import requests, tempfile, os, json, traceback\n",
					"from datetime import datetime\n",
					"from pyspark.sql import Row\n",
					"from pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType, IntegerType\n",
					"from pyspark.sql.functions import lit\n",
					"\n",
					"# ---------- CONFIG ----------\n",
					"ADLS_ACCOUNT = \"adlsrmadwdev\"\n",
					"AUDIT_CONTAINER = \"audit\"\n",
					"RAW_CONTAINER = \"raw\"\n",
					"RAW_FOLDER = \"raw\"\n",
					"SPARK_META_TABLE = \"dev_csv_scr_config\"   # source metadata table\n",
					"PARQUET_META_FALLBACK = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/csv_scr_config.parquet\"\n",
					"RAW_BASE_ABFSS = f\"abfss://{RAW_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/{RAW_FOLDER}\"\n",
					"META_AUDIT_TABLE = \"csv_scr_file_inf\"\n",
					"PARQUET_AUDIT_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/csv_scr_file_inf.parquet\"\n",
					"\n",
					"# GitHub fallback info (rebuild metadata if table/parquet empty)\n",
					"G_OWNER = \"microsoft\"\n",
					"G_REPO = \"sql-server-samples\"\n",
					"G_PATH = \"samples/databases/adventure-works/oltp-install-script\"\n",
					"G_BRANCH = \"master\"\n",
					"# ------------------------\n",
					"\n",
					"# Helpers & imports for Synapse utils\n",
					"try:\n",
					"    from notebookutils import mssparkutils\n",
					"except Exception:\n",
					"    import mssparkutils\n",
					"\n",
					"def abfss(container, account, *parts):\n",
					"    return \"abfss://{}@{}.dfs.core.windows.net/{}\".format(container, account, \"/\".join([p.strip(\"/\") for p in parts if p and p!=\"\"]))\n",
					"\n",
					"def list_recursive_abfss(root):\n",
					"    try:\n",
					"        items = mssparkutils.fs.ls(root)\n",
					"    except Exception as e:\n",
					"        print(\"Could not list\", root, \":\", e)\n",
					"        return []\n",
					"    files=[]\n",
					"    for it in items:\n",
					"        if it.isDir:\n",
					"            files += list_recursive_abfss(it.path)\n",
					"        else:\n",
					"            files.append(it.path)\n",
					"    return files\n",
					"\n",
					"def ensure_folder(abfss_path):\n",
					"    try:\n",
					"        mssparkutils.fs.mkdirs(abfss_path)\n",
					"    except Exception:\n",
					"        pass\n",
					"\n",
					"# 1) Read metadata: priority table -> audit parquet -> GitHub API\n",
					"def read_metadata_df():\n",
					"    # try table\n",
					"    try:\n",
					"        df = spark.table(SPARK_META_TABLE)\n",
					"        cnt = df.count()\n",
					"        print(f\"Read Spark table {SPARK_META_TABLE}, rows={cnt}\")\n",
					"        if cnt > 0:\n",
					"            return df.select(\"sno\",\"CSV_File_Name\",\"Raw_GitHub_URL\")\n",
					"    except Exception as e:\n",
					"        print(\"Spark table read failed or missing:\", e)\n",
					"    # try fallback parquet\n",
					"    try:\n",
					"        print(\"Trying fallback parquet:\", PARQUET_META_FALLBACK)\n",
					"        df2 = spark.read.parquet(PARQUET_META_FALLBACK)\n",
					"        cnt2 = df2.count()\n",
					"        print(\"Fallback parquet rows=\", cnt2)\n",
					"        if cnt2 > 0:\n",
					"            # normalize names\n",
					"            cols = df2.columns\n",
					"            df2 = df2.withColumnRenamed(cols[0], \"sno\") if \"sno\" not in cols and len(cols)>0 else df2\n",
					"            # ensure expected columns\n",
					"            from pyspark.sql.functions import lit\n",
					"            for c in [\"sno\",\"CSV_File_Name\",\"Raw_GitHub_URL\"]:\n",
					"                if c not in df2.columns:\n",
					"                    df2 = df2.withColumn(c, lit(None))\n",
					"            return df2.select(\"sno\",\"CSV_File_Name\",\"Raw_GitHub_URL\")\n",
					"    except Exception as e:\n",
					"        print(\"Fallback parquet read failed or missing:\", e)\n",
					"\n",
					"    # fallback: call GitHub API and build DataFrame\n",
					"    print(\"Querying GitHub to rebuild metadata list from repository...\")\n",
					"    try:\n",
					"        url = f\"https://api.github.com/repos/{G_OWNER}/{G_REPO}/contents/{G_PATH}?ref={G_BRANCH}\"\n",
					"        r = requests.get(url, timeout=30)\n",
					"        r.raise_for_status()\n",
					"        items = r.json()\n",
					"        csv_files = [item[\"name\"] for item in items if item[\"name\"].lower().endswith(\".csv\")]\n",
					"        base_raw = f\"https://raw.githubusercontent.com/{G_OWNER}/{G_REPO}/{G_BRANCH}/{G_PATH}/\"\n",
					"        csv_urls = [{\"file\": f, \"url\": base_raw+f} for f in csv_files]\n",
					"        rows = [Row(sno=i+1, CSV_File_Name=c[\"file\"], Raw_GitHub_URL=c[\"url\"]) for i,c in enumerate(csv_urls)]\n",
					"        df_meta = spark.createDataFrame(rows)\n",
					"        print(\"Rebuilt metadata from GitHub, rows=\", df_meta.count())\n",
					"        return df_meta.select(\"sno\",\"CSV_File_Name\",\"Raw_GitHub_URL\")\n",
					"    except Exception as e:\n",
					"        print(\"GitHub query failed:\", e)\n",
					"        raise\n",
					"\n",
					"# Run metadata read\n",
					"meta_df = read_metadata_df()\n",
					"meta_count = 0\n",
					"try:\n",
					"    meta_count = meta_df.count()\n",
					"except:\n",
					"    meta_count = 0\n",
					"if meta_count == 0:\n",
					"    print(\"No metadata rows found (table/parquet empty and GitHub returned none). Nothing to download.\")\n",
					"    raise SystemExit(\"No source metadata to download.\")\n",
					"\n",
					"print(\"Metadata rows to process:\", meta_count)\n",
					"\n",
					"# 2) Prepare raw folder\n",
					"ensure_folder(abfss(RAW_CONTAINER, ADLS_ACCOUNT, RAW_FOLDER))\n",
					"\n",
					"existing_raw = list_recursive_abfss(RAW_BASE_ABFSS)\n",
					"existing_set = set([p.split(\"/\")[-1] for p in existing_raw])\n",
					"print(\"Existing files in raw folder:\", len(existing_set))\n",
					"\n",
					"# 3) Loop and download\n",
					"meta_rows = meta_df.collect()\n",
					"audit_rows = []\n",
					"for r in meta_rows:\n",
					"    rdict = r.asDict()\n",
					"    fname = (rdict.get(\"CSV_File_Name\") or \"\").strip()\n",
					"    furl = (rdict.get(\"Raw_GitHub_URL\") or \"\").strip()\n",
					"    if not fname:\n",
					"        if furl:\n",
					"            fname = furl.split(\"/\")[-1]\n",
					"        else:\n",
					"            print(\"Skipping row with no filename and no URL:\", rdict)\n",
					"            continue\n",
					"    target_abfss = abfss(RAW_CONTAINER, ADLS_ACCOUNT, RAW_FOLDER, fname)\n",
					"    status = \"Unknown\"\n",
					"    target_path = None\n",
					"    try:\n",
					"        if fname in existing_set or (hasattr(mssparkutils.fs, \"exists\") and mssparkutils.fs.exists(target_abfss)):\n",
					"            status = \"AlreadyExists\"\n",
					"            target_path = target_abfss\n",
					"            print(f\"SKIP: {fname} already exists.\")\n",
					"        else:\n",
					"            if not furl:\n",
					"                status = \"NoURL\"\n",
					"                print(f\"NO URL for {fname}; skipping download.\")\n",
					"            else:\n",
					"                print(f\"Downloading {fname} from {furl} ...\")\n",
					"                resp = requests.get(furl, timeout=60)\n",
					"                resp.raise_for_status()\n",
					"                # write to temp local then copy\n",
					"                tmp = tempfile.NamedTemporaryFile(delete=False)\n",
					"                tmp.write(resp.content)\n",
					"                tmp.close()\n",
					"                local_tmp = tmp.name\n",
					"                try:\n",
					"                    # attempt mssparkutils cp file: -> abfss:\n",
					"                    mssparkutils.fs.cp(f\"file:{local_tmp}\", target_abfss)\n",
					"                    status = \"Downloaded\"\n",
					"                    target_path = target_abfss\n",
					"                    print(f\"   Saved to {target_abfss}\")\n",
					"                except Exception as cp_e:\n",
					"                    print(\"   mssparkutils.cp failed:\", cp_e)\n",
					"                    # fallback: write with spark saveAsTextFile (works for csv)\n",
					"                    try:\n",
					"                        txt = resp.content.decode('utf-8', errors='replace')\n",
					"                        spark.sparkContext.parallelize([txt]).coalesce(1).saveAsTextFile(target_abfss + \".tmp\")\n",
					"                        # move part file\n",
					"                        parts = list_recursive_abfss(target_abfss + \".tmp\")\n",
					"                        part = None\n",
					"                        for p in parts:\n",
					"                            if p.split(\"/\")[-1].startswith(\"part-\"):\n",
					"                                part = p; break\n",
					"                        if part:\n",
					"                            mssparkutils.fs.cp(part, target_abfss)\n",
					"                            status = \"DownloadedViaSpark\"\n",
					"                            target_path = target_abfss\n",
					"                            print(\"   Saved via Spark fallback to\", target_abfss)\n",
					"                        else:\n",
					"                            status = \"SaveFallbackFailed\"\n",
					"                            print(\"   Could not find part file in tmp folder.\")\n",
					"                    except Exception as fallback_e:\n",
					"                        status = \"SaveFailed\"\n",
					"                        print(\"   Fallback write failed:\", fallback_e)\n",
					"                finally:\n",
					"                    try:\n",
					"                        os.remove(local_tmp)\n",
					"                    except:\n",
					"                        pass\n",
					"    except Exception as ex:\n",
					"        status = \"DownloadError\"\n",
					"        target_path = None\n",
					"        print(\"Error processing\", fname, ex, traceback.format_exc())\n",
					"\n",
					"    # detect delimiter if downloaded/available\n",
					"    delim = None\n",
					"    schema_def = None\n",
					"    has_header = False\n",
					"    if target_path:\n",
					"        try:\n",
					"            sample_df = spark.read.text(target_path).limit(5)\n",
					"            sample_lines = [x.value for x in sample_df.collect() if x.value and x.value.strip()!='']\n",
					"            if sample_lines:\n",
					"                candidates=[\",\",\"|\",\"\\t\",\";\"]\n",
					"                scores={}\n",
					"                for d in candidates:\n",
					"                    try:\n",
					"                        counts=[len(l.split(d)) for l in sample_lines]\n",
					"                        modal=max(set(counts), key=counts.count)\n",
					"                        scores[d]=counts.count(modal)*modal\n",
					"                    except:\n",
					"                        scores[d]=0\n",
					"                best=max(scores, key=lambda k: scores[k])\n",
					"                delim=best if scores[best]>0 else \",\"\n",
					"                schema_def = json.dumps([f\"col_{i+1}\" for i in range(len(sample_lines[0].split(delim)))])\n",
					"        except Exception as e:\n",
					"            print(\"Sampling failed for\", fname, e)\n",
					"\n",
					"    audit_rows.append({\n",
					"        \"CSV_File_Name\": fname,\n",
					"        \"Raw_GitHub_URL\": furl,\n",
					"        \"Target_Path\": target_path,\n",
					"        \"Has_Header\": has_header,\n",
					"        \"Delimiter\": delim,\n",
					"        \"Schema_Definition\": schema_def,\n",
					"        \"Load_Status\": status,\n",
					"        \"Load_Timestamp\": datetime.utcnow(),\n",
					"        \"Comments\": None\n",
					"    })\n",
					"\n",
					"# 4) Write audit rows (MERGE if delta exists else append)\n",
					"if audit_rows:\n",
					"    schema_meta = StructType([\n",
					"        StructField(\"CSV_File_Name\", StringType(), True),\n",
					"        StructField(\"Raw_GitHub_URL\", StringType(), True),\n",
					"        StructField(\"Target_Path\", StringType(), True),\n",
					"        StructField(\"Has_Header\", BooleanType(), True),\n",
					"        StructField(\"Delimiter\", StringType(), True),\n",
					"        StructField(\"Schema_Definition\", StringType(), True),\n",
					"        StructField(\"Load_Status\", StringType(), True),\n",
					"        StructField(\"Load_Timestamp\", TimestampType(), True),\n",
					"        StructField(\"Comments\", StringType(), True)\n",
					"    ])\n",
					"    df_updates = spark.createDataFrame([Row(**r) for r in audit_rows], schema=schema_meta)\n",
					"    df_updates.createOrReplaceTempView(\"tmp_audit_updates\")\n",
					"    merged=False\n",
					"    try:\n",
					"        from delta.tables import DeltaTable\n",
					"        if META_AUDIT_TABLE in [t.name for t in spark.catalog.listTables()]:\n",
					"            try:\n",
					"                delta_target = DeltaTable.forName(spark, META_AUDIT_TABLE)\n",
					"                merge_sql = f\"\"\"\n",
					"                MERGE INTO {META_AUDIT_TABLE} AS target\n",
					"                USING tmp_audit_updates AS source\n",
					"                ON target.CSV_File_Name = source.CSV_File_Name\n",
					"                WHEN MATCHED THEN UPDATE SET *\n",
					"                WHEN NOT MATCHED THEN INSERT *\n",
					"                \"\"\"\n",
					"                spark.sql(merge_sql)\n",
					"                merged=True\n",
					"                print(\"MERGE into audit table applied.\")\n",
					"            except Exception as e:\n",
					"                print(\"Delta MERGE failed:\", e)\n",
					"    except Exception as e:\n",
					"        print(\"Delta not available or MERGE check failed:\", e)\n",
					"\n",
					"    if not merged:\n",
					"        try:\n",
					"            df_updates.write.mode(\"append\").saveAsTable(META_AUDIT_TABLE)\n",
					"            print(\"Appended audit rows to\", META_AUDIT_TABLE)\n",
					"        except Exception as e:\n",
					"            print(\"Failed to append audit rows:\", e, traceback.format_exc())\n",
					"\n",
					"    # write parquet snapshot\n",
					"    try:\n",
					"        df_updates.write.mode(\"overwrite\").parquet(PARQUET_AUDIT_PATH)\n",
					"        print(\"Wrote audit parquet snapshot to:\", PARQUET_AUDIT_PATH)\n",
					"    except Exception as e:\n",
					"        print(\"Failed to write audit parquet:\", e, traceback.format_exc())\n",
					"\n",
					"# 5) Summary\n",
					"print(\"\\n==== Summary ====\")\n",
					"total = len(audit_rows)\n",
					"downloaded = len([r for r in audit_rows if r[\"Load_Status\"] and r[\"Load_Status\"].lower().startswith(\"download\")])\n",
					"exists = len([r for r in audit_rows if r[\"Load_Status\"]==\"AlreadyExists\"])\n",
					"errors = len([r for r in audit_rows if r[\"Load_Status\"] in (\"DownloadError\",\"SaveFailed\",\"SaveFallbackFailed\")])\n",
					"no_url = len([r for r in audit_rows if r[\"Load_Status\"]==\"NoURL\"])\n",
					"print(f\"Total processed: {total}\")\n",
					"print(f\"Downloaded: {downloaded}\")\n",
					"print(f\"Already existed: {exists}\")\n",
					"print(f\"Errors: {errors}\")\n",
					"print(f\"No URL: {no_url}\")\n",
					"print(\"Detailed statuses:\")\n",
					"for r in audit_rows:\n",
					"    print(r[\"CSV_File_Name\"], \"=>\", r[\"Load_Status\"], r[\"Target_Path\"])\n",
					"print(\"\\nYou can now inspect the raw folder in Storage Explorer or list with:\")\n",
					"print(\"  mssparkutils.fs.ls('\"+RAW_BASE_ABFSS+\"')\")\n",
					"\n",
					"# End cell\n",
					"'''"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"source": [
					"# ---------- Enhanced CSV downloader + audit writer ----------\n",
					"import requests, tempfile, os, json, traceback\n",
					"from datetime import datetime\n",
					"from pyspark.sql import Row\n",
					"from pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType, IntegerType\n",
					"from pyspark.sql.functions import lit\n",
					"\n",
					"# ---------- CONFIG ----------\n",
					"ADLS_ACCOUNT = \"adlsrmadwdev\"\n",
					"AUDIT_CONTAINER = \"audit\"\n",
					"RAW_CONTAINER = \"raw\"\n",
					"RAW_FOLDER = \"raw\"\n",
					"SPARK_META_TABLE = \"dev_csv_scr_config\"\n",
					"META_AUDIT_TABLE = \"csv_scr_file_inf\"\n",
					"PARQUET_META_FALLBACK = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/csv_scr_config.parquet\"\n",
					"PARQUET_AUDIT_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/csv_scr_file_inf.parquet\"\n",
					"RAW_BASE_ABFSS = f\"abfss://{RAW_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/{RAW_FOLDER}\"\n",
					"# GitHub fallback\n",
					"G_OWNER = \"microsoft\"\n",
					"G_REPO = \"sql-server-samples\"\n",
					"G_PATH = \"samples/databases/adventure-works/oltp-install-script\"\n",
					"G_BRANCH = \"master\"\n",
					"# ------------------------\n",
					"\n",
					"try:\n",
					"    from notebookutils import mssparkutils\n",
					"except Exception:\n",
					"    import mssparkutils\n",
					"\n",
					"def abfss(container, account, *parts):\n",
					"    return f\"abfss://{container}@{account}.dfs.core.windows.net/\" + \"/\".join([p.strip(\"/\") for p in parts if p and p!=\"\"])\n",
					"\n",
					"def ensure_folder(abfss_path):\n",
					"    try:\n",
					"        mssparkutils.fs.mkdirs(abfss_path)\n",
					"    except Exception:\n",
					"        pass\n",
					"\n",
					"def list_recursive_abfss(root):\n",
					"    try:\n",
					"        items = mssparkutils.fs.ls(root)\n",
					"    except Exception as e:\n",
					"        print(\"Could not list\", root, \":\", e)\n",
					"        return []\n",
					"    files=[]\n",
					"    for it in items:\n",
					"        if it.isDir:\n",
					"            files += list_recursive_abfss(it.path)\n",
					"        else:\n",
					"            files.append(it.path)\n",
					"    return files\n",
					"\n",
					"# -------- Read metadata --------\n",
					"def read_metadata_df():\n",
					"    try:\n",
					"        df = spark.table(SPARK_META_TABLE)\n",
					"        cnt = df.count()\n",
					"        print(f\"Read Spark table {SPARK_META_TABLE}, rows={cnt}\")\n",
					"        if cnt > 0:\n",
					"            return df.select(\"sno\",\"CSV_File_Name\",\"Raw_GitHub_URL\")\n",
					"    except Exception as e:\n",
					"        print(\"Table read failed:\", e)\n",
					"    try:\n",
					"        print(\"Trying fallback parquet:\", PARQUET_META_FALLBACK)\n",
					"        df2 = spark.read.parquet(PARQUET_META_FALLBACK)\n",
					"        cnt2 = df2.count()\n",
					"        print(\"Fallback parquet rows=\", cnt2)\n",
					"        if cnt2 > 0:\n",
					"            for c in [\"sno\",\"CSV_File_Name\",\"Raw_GitHub_URL\"]:\n",
					"                if c not in df2.columns:\n",
					"                    df2 = df2.withColumn(c, lit(None))\n",
					"            return df2.select(\"sno\",\"CSV_File_Name\",\"Raw_GitHub_URL\")\n",
					"    except Exception as e:\n",
					"        print(\"Fallback parquet read failed:\", e)\n",
					"    # GitHub rebuild\n",
					"    print(\"Building metadata from GitHub API ...\")\n",
					"    try:\n",
					"        url = f\"https://api.github.com/repos/{G_OWNER}/{G_REPO}/contents/{G_PATH}?ref={G_BRANCH}\"\n",
					"        r = requests.get(url, timeout=30)\n",
					"        r.raise_for_status()\n",
					"        items = r.json()\n",
					"        csv_files = [item[\"name\"] for item in items if item[\"name\"].lower().endswith(\".csv\")]\n",
					"        base_raw = f\"https://raw.githubusercontent.com/{G_OWNER}/{G_REPO}/{G_BRANCH}/{G_PATH}/\"\n",
					"        rows = [Row(sno=i+1, CSV_File_Name=f, Raw_GitHub_URL=base_raw+f) for i, f in enumerate(csv_files)]\n",
					"        df_meta = spark.createDataFrame(rows)\n",
					"        print(\"Built from GitHub, rows=\", df_meta.count())\n",
					"        return df_meta\n",
					"    except Exception as e:\n",
					"        print(\"GitHub fetch failed:\", e)\n",
					"        raise\n",
					"\n",
					"meta_df = read_metadata_df()\n",
					"meta_count = meta_df.count()\n",
					"if meta_count == 0:\n",
					"    raise SystemExit(\"❌ No metadata rows found. Nothing to download.\")\n",
					"print(\"Metadata rows to process:\", meta_count)\n",
					"\n",
					"# -------- Download & audit build --------\n",
					"ensure_folder(abfss(RAW_CONTAINER, ADLS_ACCOUNT, RAW_FOLDER))\n",
					"existing_raw = list_recursive_abfss(RAW_BASE_ABFSS)\n",
					"existing_set = set([p.split(\"/\")[-1] for p in existing_raw])\n",
					"print(\"Existing raw files:\", len(existing_set))\n",
					"\n",
					"audit_rows=[]\n",
					"for r in meta_df.collect():\n",
					"    rd = r.asDict()\n",
					"    fname = (rd.get(\"CSV_File_Name\") or \"\").strip()\n",
					"    furl = (rd.get(\"Raw_GitHub_URL\") or \"\").strip()\n",
					"    if not fname and furl:\n",
					"        fname = furl.split(\"/\")[-1]\n",
					"    if not fname:\n",
					"        continue\n",
					"\n",
					"    target_abfss = abfss(RAW_CONTAINER, ADLS_ACCOUNT, RAW_FOLDER, fname)\n",
					"    status=\"Unknown\"; target_path=None; toimport=0\n",
					"    try:\n",
					"        if fname in existing_set or (hasattr(mssparkutils.fs,\"exists\") and mssparkutils.fs.exists(target_abfss)):\n",
					"            status=\"AlreadyExists\"; target_path=target_abfss; toimport=1\n",
					"        else:\n",
					"            if not furl:\n",
					"                status=\"NoURL\"\n",
					"            else:\n",
					"                print(f\"⬇️ Downloading {fname} from {furl}\")\n",
					"                resp=requests.get(furl,timeout=60); resp.raise_for_status()\n",
					"                tmp=tempfile.NamedTemporaryFile(delete=False)\n",
					"                tmp.write(resp.content); tmp.close()\n",
					"                try:\n",
					"                    mssparkutils.fs.cp(f\"file:{tmp.name}\", target_abfss)\n",
					"                    status=\"Downloaded\"; target_path=target_abfss; toimport=1\n",
					"                except Exception as cp_e:\n",
					"                    print(\"   mssparkutils.cp failed:\", cp_e)\n",
					"                    try:\n",
					"                        txt = resp.content.decode('utf-8', errors='replace')\n",
					"                        spark.sparkContext.parallelize([txt]).coalesce(1).saveAsTextFile(target_abfss+\".tmp\")\n",
					"                        parts = list_recursive_abfss(target_abfss+\".tmp\")\n",
					"                        for p in parts:\n",
					"                            if p.split(\"/\")[-1].startswith(\"part-\"):\n",
					"                                mssparkutils.fs.cp(p, target_abfss)\n",
					"                                status=\"DownloadedViaSpark\"; target_path=target_abfss; toimport=1\n",
					"                                break\n",
					"                    except Exception as e2:\n",
					"                        status=\"SaveFailed\"\n",
					"                        print(\"   Fallback failed:\", e2)\n",
					"                finally:\n",
					"                    try: os.remove(tmp.name)\n",
					"                    except: pass\n",
					"    except Exception as e:\n",
					"        status=\"DownloadError\"\n",
					"        print(\"Error:\", e)\n",
					"\n",
					"    # Detect delimiter/schema\n",
					"    delim=None; schema_def=None; has_header=False\n",
					"    if target_path:\n",
					"        try:\n",
					"            sample_df = spark.read.text(target_path).limit(5)\n",
					"            lines=[x.value for x in sample_df.collect() if x.value and x.value.strip()!='']\n",
					"            if lines:\n",
					"                candidates=[\",\",\"|\",\"\\t\",\";\"]; scores={}\n",
					"                for d in candidates:\n",
					"                    try:\n",
					"                        counts=[len(l.split(d)) for l in lines]\n",
					"                        modal=max(set(counts), key=counts.count)\n",
					"                        scores[d]=counts.count(modal)*modal\n",
					"                    except: scores[d]=0\n",
					"                best=max(scores, key=lambda k: scores[k])\n",
					"                delim=best if scores[best]>0 else \",\"\n",
					"                schema_def=json.dumps([f\"col_{i+1}\" for i in range(len(lines[0].split(delim)))])\n",
					"        except Exception as e:\n",
					"            print(\"Sample read failed for\", fname, e)\n",
					"\n",
					"    audit_rows.append({\n",
					"        \"CSV_File_Name\": fname,\n",
					"        \"Raw_GitHub_URL\": furl,\n",
					"        \"Target_Path\": target_path,\n",
					"        \"Has_Header\": has_header,\n",
					"        \"Delimiter\": delim,\n",
					"        \"Schema_Definition\": schema_def,\n",
					"        \"Load_Status\": status,\n",
					"        \"Load_Timestamp\": datetime.utcnow(),\n",
					"        \"Comments\": None,\n",
					"        \"Toimport\": toimport,\n",
					"        \"DataLayer\": \"Raw\"\n",
					"    })\n",
					"\n",
					"print(f\"Prepared {len(audit_rows)} audit rows.\")\n",
					"\n",
					"# -------- Write audit table + parquet --------\n",
					"if audit_rows:\n",
					"    schema_meta = StructType([\n",
					"        StructField(\"CSV_File_Name\", StringType(), True),\n",
					"        StructField(\"Raw_GitHub_URL\", StringType(), True),\n",
					"        StructField(\"Target_Path\", StringType(), True),\n",
					"        StructField(\"Has_Header\", BooleanType(), True),\n",
					"        StructField(\"Delimiter\", StringType(), True),\n",
					"        StructField(\"Schema_Definition\", StringType(), True),\n",
					"        StructField(\"Load_Status\", StringType(), True),\n",
					"        StructField(\"Load_Timestamp\", TimestampType(), True),\n",
					"        StructField(\"Comments\", StringType(), True),\n",
					"        StructField(\"Toimport\", IntegerType(), True),\n",
					"        StructField(\"DataLayer\", StringType(), True)\n",
					"    ])\n",
					"    df_updates = spark.createDataFrame([Row(**r) for r in audit_rows], schema=schema_meta)\n",
					"    df_updates.createOrReplaceTempView(\"tmp_audit_updates\")\n",
					"\n",
					"    merged=False\n",
					"    try:\n",
					"        from delta.tables import DeltaTable\n",
					"        if META_AUDIT_TABLE in [t.name for t in spark.catalog.listTables()]:\n",
					"            delta_target = DeltaTable.forName(spark, META_AUDIT_TABLE)\n",
					"            merge_sql = f\"\"\"\n",
					"            MERGE INTO {META_AUDIT_TABLE} AS target\n",
					"            USING tmp_audit_updates AS source\n",
					"            ON target.CSV_File_Name = source.CSV_File_Name\n",
					"            WHEN MATCHED THEN UPDATE SET *\n",
					"            WHEN NOT MATCHED THEN INSERT *\n",
					"            \"\"\"\n",
					"            spark.sql(merge_sql); merged=True\n",
					"            print(\"✅ MERGE applied into\", META_AUDIT_TABLE)\n",
					"    except Exception as e:\n",
					"        print(\"MERGE failed:\", e)\n",
					"\n",
					"    if not merged:\n",
					"        try:\n",
					"            df_updates.write.mode(\"append\").saveAsTable(META_AUDIT_TABLE)\n",
					"            print(\"✅ Appended audit rows to\", META_AUDIT_TABLE)\n",
					"        except Exception as e:\n",
					"            print(\"Append failed:\", e)\n",
					"\n",
					"    try:\n",
					"        df_updates.write.mode(\"overwrite\").parquet(PARQUET_AUDIT_PATH)\n",
					"        print(\"✅ Wrote audit parquet snapshot to:\", PARQUET_AUDIT_PATH)\n",
					"    except Exception as e:\n",
					"        print(\"Parquet write failed:\", e)\n",
					"else:\n",
					"    print(\"No audit rows to write.\")\n",
					"\n",
					"# -------- Post-update: mark some Toimport=0 --------\n",
					"try:\n",
					"    spark.sql(\"\"\"\n",
					"        UPDATE csv_scr_file_inf\n",
					"        SET Toimport = 0\n",
					"        WHERE CSV_File_Name IN ('JobCandidate_TOREMOVE.csv','ProductModelorg.csv','AWBuildVersion.csv')\n",
					"    \"\"\")\n",
					"    print(\"✅ Updated Toimport=0 for selected files.\")\n",
					"except Exception as e:\n",
					"    print(\"⚠️ Update failed:\", e)\n",
					"\n",
					"# -------- Summary --------\n",
					"print(\"\\n==== SUMMARY ====\")\n",
					"total=len(audit_rows)\n",
					"ok=len([r for r in audit_rows if r[\"Toimport\"]==1])\n",
					"failed=len([r for r in audit_rows if r[\"Toimport\"]==0])\n",
					"print(f\"Total processed: {total}\")\n",
					"print(f\"Success/Ready ToImport: {ok}\")\n",
					"print(f\"Failed/Missing: {failed}\")\n",
					"print(f\"Audit table: {META_AUDIT_TABLE}\")\n",
					"print(f\"Parquet: {PARQUET_AUDIT_PATH}\")\n",
					"print(f\"Raw folder: {RAW_BASE_ABFSS}\")\n",
					"print(\"✅ Done.\")\n",
					""
				],
				"execution_count": 31
			}
		]
	}
}