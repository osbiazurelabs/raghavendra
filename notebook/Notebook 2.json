{
	"name": "Notebook 2",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkrmadwdev",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "0b6c2ddc-07d3-4eef-9fc8-d6782f9e39cb"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f2dcf14e-c010-4080-bb14-de61bf467b98/resourceGroups/rg-rm-adw-med-dev/providers/Microsoft.Synapse/workspaces/syn-rm-adw-med-dev/bigDataPools/sparkrmadwdev",
				"name": "sparkrmadwdev",
				"type": "Spark",
				"endpoint": "https://syn-rm-adw-med-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkrmadwdev",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.5",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# ---------- Repair: drop + recreate audit table as DELTA with Toimport/DataLayer, then update rows ----------\n",
					"import traceback\n",
					"from pyspark.sql import Row\n",
					"from pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType, IntegerType\n",
					"try:\n",
					"    from notebookutils import mssparkutils\n",
					"except Exception:\n",
					"    import mssparkutils\n",
					"\n",
					"# Config (match your environment)\n",
					"ADLS_ACCOUNT = \"adlsrmadwdev\"\n",
					"AUDIT_CONTAINER = \"audit\"\n",
					"META_TABLE = \"csv_scr_file_inf\"\n",
					"PARQUET_AUDIT_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/csv_scr_file_inf.parquet\"\n",
					"AUDIT_WAREHOUSE_TABLE_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/warehouse/{META_TABLE}\"\n",
					"EXTERNAL_SQL_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/scripts/create_external_csv_scr_file_inf.sql\"\n",
					"\n",
					"def abfss(container, account, *parts):\n",
					"    suffix = \"/\".join([p.strip(\"/\") for p in parts if p is not None and p != \"\"])\n",
					"    return f\"abfss://{container}@{account}.dfs.core.windows.net/{suffix}\"\n",
					"\n",
					"print(\"1) Show existing table (if any):\")\n",
					"try:\n",
					"    tables = spark.catalog.listTables()\n",
					"    for t in tables:\n",
					"        if t.name == META_TABLE:\n",
					"            print(\" - Found table:\", t)\n",
					"except Exception as e:\n",
					"    print(\"Could not list tables:\", e)\n",
					"\n",
					"# 2) Read parquet snapshot (if exists)\n",
					"print(\"\\n2) Attempting to read audit parquet snapshot:\", PARQUET_AUDIT_PATH)\n",
					"try:\n",
					"    df_par = spark.read.parquet(PARQUET_AUDIT_PATH)\n",
					"    print(\"   Parquet read OK. Rows:\", df_par.count())\n",
					"except Exception as e:\n",
					"    print(\"   Parquet read failed:\", e)\n",
					"    # Create an empty DF with expected schema so we can still create table\n",
					"    schema_meta = StructType([\n",
					"        StructField(\"CSV_File_Name\", StringType(), True),\n",
					"        StructField(\"Raw_GitHub_URL\", StringType(), True),\n",
					"        StructField(\"Target_Path\", StringType(), True),\n",
					"        StructField(\"Has_Header\", BooleanType(), True),\n",
					"        StructField(\"Delimiter\", StringType(), True),\n",
					"        StructField(\"Schema_Definition\", StringType(), True),\n",
					"        StructField(\"Load_Status\", StringType(), True),\n",
					"        StructField(\"Load_Timestamp\", TimestampType(), True),\n",
					"        StructField(\"Comments\", StringType(), True),\n",
					"        StructField(\"Toimport\", IntegerType(), True),\n",
					"        StructField(\"DataLayer\", StringType(), True)\n",
					"    ])\n",
					"    df_par = spark.createDataFrame([], schema_meta)\n",
					"    print(\"   Created empty dataframe with expected schema.\")\n",
					"\n",
					"# 3) Drop existing table if exists (to remove old-schema table)\n",
					"print(\"\\n3) Dropping existing table if present to avoid schema mismatch...\")\n",
					"try:\n",
					"    spark.sql(f\"DROP TABLE IF EXISTS {META_TABLE}\")\n",
					"    print(\"   Dropped existing table (if present).\")\n",
					"except Exception as e:\n",
					"    print(\"   DROP TABLE failed:\", e)\n",
					"\n",
					"# 4) Create the new Delta table at the audit warehouse location using df_par\n",
					"print(\"\\n4) Writing Delta table with correct schema to:\", AUDIT_WAREHOUSE_TABLE_PATH)\n",
					"try:\n",
					"    # ensure folder exists\n",
					"    try:\n",
					"        mssparkutils.fs.mkdirs(abfss(AUDIT_CONTAINER, ADLS_ACCOUNT, \"warehouse\", META_TABLE))\n",
					"    except:\n",
					"        pass\n",
					"\n",
					"    # write as Delta and register as table\n",
					"    df_par.write.format(\"delta\").mode(\"overwrite\").option(\"path\", AUDIT_WAREHOUSE_TABLE_PATH).saveAsTable(META_TABLE)\n",
					"    print(\"   Delta table created/overwritten:\", META_TABLE)\n",
					"except Exception as e:\n",
					"    print(\"   Failed to write delta table:\", e)\n",
					"    traceback.print_exc()\n",
					"    raise\n",
					"\n",
					"# 5) Refresh table and show schema\n",
					"try:\n",
					"    spark.sql(f\"REFRESH TABLE {META_TABLE}\")\n",
					"except:\n",
					"    pass\n",
					"print(\"\\n5) Table description:\")\n",
					"try:\n",
					"    spark.sql(f\"DESCRIBE EXTENDED {META_TABLE}\").show(truncate=False)\n",
					"    print(\"\\nPreview rows (top 20):\")\n",
					"    display(spark.sql(f\"SELECT * FROM {META_TABLE} LIMIT 20\"))\n",
					"except Exception as e:\n",
					"    print(\"Could not describe/preview table:\", e)\n",
					"\n",
					"# 6) Run UPDATE to set Toimport=0 for specific files\n",
					"print(\"\\n6) Running UPDATE to set Toimport=0 for selected files...\")\n",
					"try:\n",
					"    spark.sql(f\"\"\"\n",
					"        UPDATE {META_TABLE}\n",
					"        SET Toimport = 0\n",
					"        WHERE CSV_File_Name IN ('JobCandidate_TOREMOVE.csv','ProductModelorg.csv','AWBuildVersion.csv')\n",
					"    \"\"\")\n",
					"    print(\"   UPDATE completed.\")\n",
					"except Exception as e:\n",
					"    print(\"   UPDATE failed:\", e)\n",
					"    traceback.print_exc()\n",
					"\n",
					"# 7) Verify update\n",
					"print(\"\\n7) Verify the Toimport values for those files:\")\n",
					"try:\n",
					"    df_check = spark.sql(f\"\"\"\n",
					"        SELECT CSV_File_Name, Toimport, Load_Status\n",
					"        FROM {META_TABLE}\n",
					"        WHERE CSV_File_Name IN ('JobCandidate_TOREMOVE.csv','ProductModelorg.csv','AWBuildVersion.csv')\n",
					"    \"\"\")\n",
					"    display(df_check)\n",
					"except Exception as e:\n",
					"    print(\"   Verify query failed:\", e)\n",
					"\n",
					"# 8) Re-write serverless SQL script (so external table can be created easily)\n",
					"print(\"\\n8) Writing Serverless SQL script to:\", EXTERNAL_SQL_PATH)\n",
					"external_db=\"RM_Audit\"\n",
					"external_data_source=\"ADLS_Audit\"\n",
					"external_file_format=\"ParquetFormat\"\n",
					"external_table=\"csv_scr_file_inf\"\n",
					"\n",
					"sql_text = f\"\"\"\n",
					"IF DB_ID('{external_db}') IS NULL\n",
					"    CREATE DATABASE {external_db};\n",
					"GO\n",
					"USE {external_db};\n",
					"GO\n",
					"IF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = '{external_data_source}')\n",
					"BEGIN\n",
					"    CREATE EXTERNAL DATA SOURCE {external_data_source}\n",
					"    WITH ( LOCATION = 'abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net' );\n",
					"END\n",
					"GO\n",
					"IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = '{external_file_format}')\n",
					"BEGIN\n",
					"    CREATE EXTERNAL FILE FORMAT {external_file_format}\n",
					"    WITH ( FORMAT_TYPE = PARQUET );\n",
					"END\n",
					"GO\n",
					"IF OBJECT_ID('{external_db}.dbo.{external_table}', 'U') IS NOT NULL\n",
					"    DROP EXTERNAL TABLE dbo.{external_table};\n",
					"GO\n",
					"CREATE EXTERNAL TABLE dbo.{external_table}\n",
					"(\n",
					"    CSV_File_Name NVARCHAR(1000),\n",
					"    Raw_GitHub_URL NVARCHAR(2000),\n",
					"    Target_Path NVARCHAR(2000),\n",
					"    Has_Header BIT,\n",
					"    Delimiter NVARCHAR(10),\n",
					"    Schema_Definition NVARCHAR(2000),\n",
					"    Load_Status NVARCHAR(100),\n",
					"    Load_Timestamp DATETIME2,\n",
					"    Comments NVARCHAR(2000),\n",
					"    Toimport INT,\n",
					"    DataLayer NVARCHAR(100)\n",
					")\n",
					"WITH (\n",
					"    LOCATION = 'csv_metadata/csv_scr_file_inf.parquet',\n",
					"    DATA_SOURCE = {external_data_source},\n",
					"    FILE_FORMAT = {external_file_format}\n",
					");\n",
					"GO\n",
					"\n",
					"SELECT TOP (200) * FROM dbo.{external_table};\n",
					"\"\"\"\n",
					"\n",
					"try:\n",
					"    mssparkutils.fs.put(EXTERNAL_SQL_PATH, sql_text, True)\n",
					"    print(\"   SQL script saved.\")\n",
					"except Exception as e:\n",
					"    print(\"   Failed to save SQL script:\", e)\n",
					"\n",
					"print(\"\\nDONE â€” table recreated as DELTA, updated, and SQL script saved.\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}