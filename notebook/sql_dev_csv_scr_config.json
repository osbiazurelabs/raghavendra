{
	"name": "sql_dev_csv_scr_config",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e23c0a40-fda0-4d7f-a668-5ed5a769c9e0"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# ====================== END-TO-END NOTEBOOK (SQL AUTH VERSION) ======================\n",
					"import requests, traceback, os\n",
					"from pyspark.sql import Row\n",
					"\n",
					"# ---------- CONFIG ----------\n",
					"ADLS_ACCOUNT = \"adlsrmadwdev\"\n",
					"AUDIT_CONTAINER = \"audit\"\n",
					"PARQUET_META_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/csv_scr_config.parquet\"\n",
					"SPARK_TABLE_LOCATION = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/warehouse/dev_csv_scr_config\"\n",
					"SPARK_TABLE_NAME = \"dev_csv_scr_config\"\n",
					"\n",
					"# Azure SQL connection (SQL Authentication)\n",
					"JDBC_SERVER = \"rmsqlazure.database.windows.net\"\n",
					"JDBC_DB = \"rmsqlazure\"\n",
					"SQL_USER = \"sqladminuser\"               # your SQL login (from Azure Portal)\n",
					"SQL_PASSWORD = \"P@ssWoRd@OSBi2025\" # üî¥ replace with your actual password\n",
					"JDBC_URL = f\"jdbc:sqlserver://{JDBC_SERVER}:1433;database={JDBC_DB};encrypt=true;trustServerCertificate=false;\"\n",
					"TARGET_TABLE = \"dbo.dev_csv_scr_config\"\n",
					"\n",
					"# GitHub source config\n",
					"OWNER = \"microsoft\"\n",
					"REPO = \"sql-server-samples\"\n",
					"PATH = \"samples/databases/adventure-works/oltp-install-script\"\n",
					"BRANCH = \"master\"\n",
					"# ----------------------------\n",
					"\n",
					"try:\n",
					"    from notebookutils import mssparkutils\n",
					"except Exception:\n",
					"    import mssparkutils\n",
					"\n",
					"def ensure_abfss_folder(path):\n",
					"    try:\n",
					"        mssparkutils.fs.mkdirs(path)\n",
					"    except Exception:\n",
					"        pass\n",
					"\n",
					"# 1Ô∏è‚É£ Build metadata DataFrame from GitHub\n",
					"try:\n",
					"    url = f\"https://api.github.com/repos/{OWNER}/{REPO}/contents/{PATH}?ref={BRANCH}\"\n",
					"    print(\"Fetching metadata from GitHub...\")\n",
					"    resp = requests.get(url, timeout=30)\n",
					"    resp.raise_for_status()\n",
					"    items = resp.json()\n",
					"    csv_files = [item[\"name\"] for item in items if item[\"name\"].lower().endswith(\".csv\")]\n",
					"    base_raw = f\"https://raw.githubusercontent.com/{OWNER}/{REPO}/{BRANCH}/{PATH}/\"\n",
					"    rows = [Row(sno=i+1, CSV_File_Name=f, Raw_GitHub_URL=base_raw+f) for i, f in enumerate(csv_files)]\n",
					"    df_meta = spark.createDataFrame(rows)\n",
					"    print(f\"‚úÖ Built df_meta with {df_meta.count()} rows.\")\n",
					"    display(df_meta.limit(10))\n",
					"except Exception as e:\n",
					"    print(\"‚ùå GitHub fetch failed:\", e)\n",
					"    raise\n",
					"\n",
					"# 2Ô∏è‚É£ Write metadata to Parquet and create Spark table\n",
					"try:\n",
					"    ensure_abfss_folder(f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/\")\n",
					"    df_meta.write.mode(\"overwrite\").parquet(PARQUET_META_PATH)\n",
					"    print(\"‚úÖ Parquet metadata written to:\", PARQUET_META_PATH)\n",
					"\n",
					"    ensure_abfss_folder(SPARK_TABLE_LOCATION.rsplit(\"/\",1)[0])\n",
					"    df_meta.write.mode(\"overwrite\").option(\"path\", SPARK_TABLE_LOCATION).saveAsTable(SPARK_TABLE_NAME)\n",
					"    print(f\"‚úÖ Spark table '{SPARK_TABLE_NAME}' created at: {SPARK_TABLE_LOCATION}\")\n",
					"except Exception as e:\n",
					"    print(\"‚ùå Failed to create parquet or Spark table:\", e)\n",
					"    traceback.print_exc()\n",
					"\n",
					"# 3Ô∏è‚É£ Load data into Azure SQL Database using SQL Authentication\n",
					"try:\n",
					"    props = {\n",
					"        \"user\": SQL_USER,\n",
					"        \"password\": SQL_PASSWORD,\n",
					"        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
					"    }\n",
					"    print(f\"üîÑ Writing data to Azure SQL table: {TARGET_TABLE}\")\n",
					"    df_meta.write.jdbc(url=JDBC_URL, table=TARGET_TABLE, mode=\"overwrite\", properties=props)\n",
					"    print(f\"‚úÖ Successfully loaded data into Azure SQL table: {TARGET_TABLE}\")\n",
					"except Exception as e:\n",
					"    print(\"‚ùå SQL write failed:\", e)\n",
					"    traceback.print_exc()\n",
					"\n",
					"print(\"üéØ End-to-end process complete! Verify data in Azure SQL:\")\n",
					"print(f\"  SELECT COUNT(*) FROM {TARGET_TABLE};\")\n",
					"# ====================== END OF NOTEBOOK ======================\n",
					""
				],
				"execution_count": null
			}
		]
	}
}