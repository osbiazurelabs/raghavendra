{
	"name": "Notebook 1",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkrmadwdev",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "5d11d57d-132a-4d7c-bf5d-4dca825c241d"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f2dcf14e-c010-4080-bb14-de61bf467b98/resourceGroups/rg-rm-adw-med-dev/providers/Microsoft.Synapse/workspaces/syn-rm-adw-med-dev/bigDataPools/sparkrmadwdev",
				"name": "sparkrmadwdev",
				"type": "Spark",
				"endpoint": "https://syn-rm-adw-med-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkrmadwdev",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.5",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# Cell 1 - CONFIG & imports\n",
					"import re, json, requests, tempfile, os\n",
					"from datetime import datetime\n",
					"from pyspark.sql import Row\n",
					"from pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType\n",
					"from delta.tables import DeltaTable\n",
					"\n",
					"# Try to import mssparkutils (Synapse)\n",
					"try:\n",
					"    from notebookutils import mssparkutils\n",
					"except Exception:\n",
					"    # fallback if environment differs\n",
					"    import mssparkutils\n",
					"\n",
					"# ---------- USER CONFIG ----------\n",
					"# Source metadata table you created earlier\n",
					"CSV_META_TABLE_SOURCE = \"dev_csv_scr_config\"   # <--- your saved Spark table name\n",
					"\n",
					"# Where CSV files should live in the lake (relative path inside container)\n",
					"RAW_CONTAINER = \"raw\"\n",
					"RAW_FOLDER = \"raw\"   # will write to abfss://raw@adlsrmadwdev.dfs.core.windows.net/raw/\n",
					"RAW_BASE_DIR = f\"abfss://{RAW_CONTAINER}@adlsrmadwdev.dfs.core.windows.net/{RAW_FOLDER}\"\n",
					"\n",
					"# Audit metadata table (Delta / Spark table name)\n",
					"META_TABLE = \"csv_scr_file_inf\"   # target audit table name (will create if not exists)\n",
					"\n",
					"# Parquet location to also persist audit information (optional)\n",
					"AUDIT_PARQUET_PATH = f\"abfss://audit@adlsrmadwdev.dfs.core.windows.net/csv_metadata/csv_scr_file_inf.parquet\"\n",
					"\n",
					"# Behavior flags\n",
					"DOWNLOAD_IF_MISSING = True\n",
					"FILE_EXTENSIONS = [\".csv\", \".txt\"]\n",
					"\n",
					"# Optional: storage account key fallback secret (scope/secret) - ONLY for testing\n",
					"# If you have a secret in Azure Key Vault linked as an Azure DevOps/Workspace secret scope, set these.\n",
					"# If you don't use this fallback, keep both values as None.\n",
					"SECRET_SCOPE = \"storage-keys\"\n",
					"SECRET_NAME = \"adlsrmadwdev-key\"   # secret value should be the account key string\n",
					"# ----------------------------------\n",
					"\n",
					"print(\"Config ready.\")\n",
					""
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"# Cell 2 - Helpers\n",
					"\n",
					"def abfss_path(container, account, *parts):\n",
					"    \"\"\"Construct abfss path from parts\"\"\"\n",
					"    suffix = \"/\".join([p.strip(\"/\")+\"\" for p in parts if p is not None and p!=\"\"])\n",
					"    return f\"abfss://{container}@{account}.dfs.core.windows.net/{suffix}\"\n",
					"\n",
					"ADLS_ACCOUNT = \"adlsrmadwdev\"\n",
					"\n",
					"def list_files_recursive_abfss(abfss_root):\n",
					"    try:\n",
					"        items = mssparkutils.fs.ls(abfss_root)\n",
					"    except Exception as e:\n",
					"        print(f\"‚ö†Ô∏è Could not list {abfss_root}: {e}\")\n",
					"        return []\n",
					"    files = []\n",
					"    for it in items:\n",
					"        if it.isDir:\n",
					"            files += list_files_recursive_abfss(it.path)\n",
					"        else:\n",
					"            files.append(it.path)\n",
					"    return files\n",
					"\n",
					"def read_sample_lines(path, max_lines=5):\n",
					"    try:\n",
					"        df = spark.read.text(path).limit(max_lines)\n",
					"        lines = [r.value for r in df.collect()]\n",
					"        lines = [l for l in lines if l and l.strip()!='']\n",
					"        return lines\n",
					"    except Exception:\n",
					"        return []\n",
					"\n",
					"def detect_delimiter(lines, candidates=[\",\", \"|\", \"\\t\", \";\"]):\n",
					"    if not lines:\n",
					"        return \",\"\n",
					"    scores = {}\n",
					"    for d in candidates:\n",
					"        try:\n",
					"            counts = [len(l.split(d)) for l in lines]\n",
					"            modal = max(set(counts), key=counts.count)\n",
					"            scores[d] = counts.count(modal) * modal\n",
					"        except:\n",
					"            scores[d] = 0\n",
					"    best = max(scores, key=lambda k: scores[k])\n",
					"    return best if scores[best] > 0 else \",\"\n",
					"\n",
					"def lakehouse_path_exists_abfss(path):\n",
					"    # path is ABFSS full path\n",
					"    try:\n",
					"        parent = \"/\".join(path.split(\"/\")[:-1])\n",
					"        fname = path.split(\"/\")[-1]\n",
					"        for it in mssparkutils.fs.ls(parent):\n",
					"            if (not it.isDir) and it.path.endswith(\"/\" + fname):\n",
					"                return True\n",
					"        return False\n",
					"    except Exception:\n",
					"        try:\n",
					"            _ = spark.read.text(path).limit(1).collect()\n",
					"            return True\n",
					"        except Exception:\n",
					"            return False\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Cell 3 - Try listing container to verify auth; fallback to secret if listing fails\n",
					"\n",
					"test_path = abfss_path(RAW_CONTAINER, ADLS_ACCOUNT, \"\")\n",
					"print(\"Testing list on:\", test_path)\n",
					"can_list = True\n",
					"try:\n",
					"    _ = mssparkutils.fs.ls(test_path)\n",
					"    print(\"‚úÖ Able to list storage via workspace linked identity / default auth.\")\n",
					"except Exception as e:\n",
					"    print(\"‚ùå Default auth listing failed:\", e)\n",
					"    can_list = False\n",
					"\n",
					"# If listing failed, try using storage account key from secret scope if provided\n",
					"if not can_list and SECRET_SCOPE and SECRET_NAME:\n",
					"    try:\n",
					"        account_key = mssparkutils.secrets.get(SECRET_SCOPE, SECRET_NAME)\n",
					"        spark.conf.set(f\"fs.azure.account.key.{ADLS_ACCOUNT}.dfs.core.windows.net\", account_key)\n",
					"        # test again\n",
					"        _ = mssparkutils.fs.ls(test_path)\n",
					"        print(\"‚úÖ Authenticated using account key from secret.\")\n",
					"        can_list = True\n",
					"    except Exception as e:\n",
					"        print(\"‚ùå Account key fallback failed or secret not found:\", e)\n",
					"\n",
					"if not can_list:\n",
					"    print(\"‚ö†Ô∏è WARNING: Could not authenticate to ADLS. Please ensure workspace-linked storage or set a secret with the storage account key.\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Cell 4 - Read source metadata and list raw files\n",
					"print(\"Reading source metadata table:\", CSV_META_TABLE_SOURCE)\n",
					"csv_meta_df = None\n",
					"try:\n",
					"    csv_meta_df = spark.table(CSV_META_TABLE_SOURCE)\n",
					"    print(f\"‚úÖ Read {CSV_META_TABLE_SOURCE} with {csv_meta_df.count()} rows.\")\n",
					"except Exception as e:\n",
					"    print(f\"‚ö†Ô∏è Could not read table {CSV_META_TABLE_SOURCE}: {e}. Proceeding with files discovered in RAW_BASE_DIR only.\")\n",
					"    csv_meta_df = None\n",
					"\n",
					"print(\"Listing current files under RAW base dir:\", RAW_BASE_DIR)\n",
					"existing_files = list_files_recursive_abfss(RAW_BASE_DIR)\n",
					"print(f\"Found {len(existing_files)} files already in RAW.\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Cell 5 - Iterate source metadata and prepare audit rows\n",
					"\n",
					"meta_updates = []\n",
					"\n",
					"def generic_cols_json(cnt):\n",
					"    if cnt <= 0:\n",
					"        return None\n",
					"    return json.dumps([f\"col_{i+1}\" for i in range(cnt)])\n",
					"\n",
					"if csv_meta_df is not None:\n",
					"    src_rows = csv_meta_df.collect()\n",
					"    for r in src_rows:\n",
					"        try:\n",
					"            rowd = r.asDict()\n",
					"        except:\n",
					"            rowd = dict(r)\n",
					"        file_name = (rowd.get(\"CSV_File_Name\") or rowd.get(\"csv_file_name\") or rowd.get(\"CSVFileName\") or rowd.get(\"csvfilename\") or \"\").strip()\n",
					"        raw_url = (rowd.get(\"Raw_GitHub_URL\") or rowd.get(\"raw_github_url\") or rowd.get(\"Raw_Github_URL\") or \"\").strip()\n",
					"\n",
					"        if not file_name:\n",
					"            # skip rows without file name\n",
					"            continue\n",
					"\n",
					"        desired_path = abfss_path(RAW_CONTAINER, ADLS_ACCOUNT, RAW_FOLDER, file_name)\n",
					"        found_path = None\n",
					"\n",
					"        # check if exists in discovered list or via API\n",
					"        if any(p.endswith(\"/\" + file_name) for p in existing_files):\n",
					"            # prefer full path from existing_files\n",
					"            matches = [p for p in existing_files if p.endswith(\"/\" + file_name)]\n",
					"            found_path = matches[0]\n",
					"        else:\n",
					"            if lakehouse_path_exists_abfss(desired_path):\n",
					"                found_path = desired_path\n",
					"\n",
					"        # If not found and allowed to download, try to fetch from raw_url\n",
					"        if not found_path and raw_url and DOWNLOAD_IF_MISSING:\n",
					"            try:\n",
					"                print(f\"‚¨áÔ∏è Downloading {file_name} from {raw_url}\")\n",
					"                resp = requests.get(raw_url, timeout=60)\n",
					"                resp.raise_for_status()\n",
					"                tmpf = tempfile.NamedTemporaryFile(delete=False)\n",
					"                tmpf.write(resp.content)\n",
					"                tmpf.close()\n",
					"                tmp_local = tmpf.name\n",
					"                target_abfss = desired_path\n",
					"\n",
					"                # Copy local temp file to abfss target using mssparkutils.fs.cp (supports file: -> abfss:)\n",
					"                try:\n",
					"                    mssparkutils.fs.cp(f\"file:{tmp_local}\", target_abfss)\n",
					"                    found_path = target_abfss\n",
					"                    print(f\"   ‚úÖ Saved to {target_abfss}\")\n",
					"                except Exception as cp_e:\n",
					"                    # Fallback: try put (text) or Spark write\n",
					"                    try:\n",
					"                        # if binary: write as bytes via put; but mssparkutils.fs.put expects text.\n",
					"                        # We'll use Spark to write as text which will work for csv.\n",
					"                        data_text = resp.content.decode('utf-8', errors='replace')\n",
					"                        spark.sparkContext.parallelize([data_text]).coalesce(1).saveAsTextFile(target_abfss + \".tmp\")\n",
					"                        # try to move the part file to desired name\n",
					"                        parts = list_files_recursive_abfss(target_abfss + \".tmp\")\n",
					"                        if parts:\n",
					"                            # find part file\n",
					"                            part = [p for p in parts if p.split(\"/\")[-1].startswith(\"part-\")]\n",
					"                            if part:\n",
					"                                # copy part file to target_abfss path\n",
					"                                mssparkutils.fs.cp(part[0], target_abfss)\n",
					"                        found_path = target_abfss\n",
					"                        print(f\"   ‚úÖ Saved to {target_abfss} via Spark fallback\")\n",
					"                    except Exception as fallback_e:\n",
					"                        print(f\"‚ùå Failed to save downloaded file {file_name}: {fallback_e}\")\n",
					"                        found_path = None\n",
					"                finally:\n",
					"                    try:\n",
					"                        os.remove(tmp_local)\n",
					"                    except:\n",
					"                        pass\n",
					"            except Exception as ex:\n",
					"                print(f\"‚ùå Download failed for {raw_url} (file {file_name}): {ex}\")\n",
					"                meta_updates.append({\n",
					"                    \"CSV_File_Name\": file_name,\n",
					"                    \"Raw_GitHub_URL\": raw_url,\n",
					"                    \"Target_Path\": None,\n",
					"                    \"Has_Header\": False,\n",
					"                    \"Delimiter\": None,\n",
					"                    \"Schema_Definition\": None,\n",
					"                    \"Load_Status\": \"DownloadError\",\n",
					"                    \"Load_Timestamp\": datetime.utcnow(),\n",
					"                    \"Comments\": f\"Download failed: {str(ex)[:200]}\"\n",
					"                })\n",
					"                continue\n",
					"\n",
					"        if found_path:\n",
					"            sample = read_sample_lines(found_path, max_lines=5)\n",
					"            delim = detect_delimiter(sample)\n",
					"            has_header_flag = False\n",
					"            col_count = len(sample[0].split(delim)) if sample else 0\n",
					"            schema_def = generic_cols_json(col_count)\n",
					"            meta_updates.append({\n",
					"                \"CSV_File_Name\": file_name,\n",
					"                \"Raw_GitHub_URL\": raw_url,\n",
					"                \"Target_Path\": found_path,\n",
					"                \"Has_Header\": has_header_flag,\n",
					"                \"Delimiter\": delim,\n",
					"                \"Schema_Definition\": schema_def,\n",
					"                \"Load_Status\": \"Available\",\n",
					"                \"Load_Timestamp\": datetime.utcnow(),\n",
					"                \"Comments\": f\"Auto-detected cols={col_count}\"\n",
					"            })\n",
					"        else:\n",
					"            meta_updates.append({\n",
					"                \"CSV_File_Name\": file_name,\n",
					"                \"Raw_GitHub_URL\": raw_url,\n",
					"                \"Target_Path\": None,\n",
					"                \"Has_Header\": False,\n",
					"                \"Delimiter\": None,\n",
					"                \"Schema_Definition\": None,\n",
					"                \"Load_Status\": \"Missing\",\n",
					"                \"Load_Timestamp\": datetime.utcnow(),\n",
					"                \"Comments\": \"File not found in lakehouse and not downloaded\"\n",
					"            })\n",
					"else:\n",
					"    # If no metadata table, scan existing files in RAW and build basic meta rows\n",
					"    for f in existing_files:\n",
					"        lower = f.lower()\n",
					"        if not any(lower.endswith(e) for e in FILE_EXTENSIONS):\n",
					"            continue\n",
					"        sample = read_sample_lines(f, max_lines=5)\n",
					"        delim = detect_delimiter(sample)\n",
					"        col_count = len(sample[0].split(delim)) if sample else 0\n",
					"        meta_updates.append({\n",
					"            \"CSV_File_Name\": f.split(\"/\")[-1],\n",
					"            \"Raw_GitHub_URL\": None,\n",
					"            \"Target_Path\": f,\n",
					"            \"Has_Header\": False,\n",
					"            \"Delimiter\": delim,\n",
					"            \"Schema_Definition\": generic_cols_json(col_count),\n",
					"            \"Load_Status\": \"Available\",\n",
					"            \"Load_Timestamp\": datetime.utcnow(),\n",
					"            \"Comments\": f\"Auto-detected cols={col_count}\"\n",
					"        })\n",
					"\n",
					"print(f\"üîß Prepared {len(meta_updates)} metadata update rows\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Cell 6 - Upsert into META_TABLE (Delta merge preferred), else append, and write audit parquet\n",
					"\n",
					"if meta_updates:\n",
					"    schema_meta = StructType([\n",
					"        StructField(\"CSV_File_Name\", StringType(), True),\n",
					"        StructField(\"Raw_GitHub_URL\", StringType(), True),\n",
					"        StructField(\"Target_Path\", StringType(), True),\n",
					"        StructField(\"Has_Header\", BooleanType(), True),\n",
					"        StructField(\"Delimiter\", StringType(), True),\n",
					"        StructField(\"Schema_Definition\", StringType(), True),\n",
					"        StructField(\"Load_Status\", StringType(), True),\n",
					"        StructField(\"Load_Timestamp\", TimestampType(), True),\n",
					"        StructField(\"Comments\", StringType(), True)\n",
					"    ])\n",
					"    df_updates = spark.createDataFrame([Row(**r) for r in meta_updates], schema=schema_meta)\n",
					"    df_updates.createOrReplaceTempView(\"tmp_meta_updates_for_merge\")\n",
					"\n",
					"    # Try Delta MERGE\n",
					"    merged = False\n",
					"    try:\n",
					"        delta_target = DeltaTable.forName(spark, META_TABLE)\n",
					"        merge_sql = f\"\"\"\n",
					"        MERGE INTO {META_TABLE} AS target\n",
					"        USING tmp_meta_updates_for_merge AS source\n",
					"        ON target.CSV_File_Name = source.CSV_File_Name\n",
					"        WHEN MATCHED THEN UPDATE SET *\n",
					"        WHEN NOT MATCHED THEN INSERT *\n",
					"        \"\"\"\n",
					"        spark.sql(merge_sql)\n",
					"        print(f\"‚úÖ MERGE applied into {META_TABLE}.\")\n",
					"        merged = True\n",
					"    except Exception as e:\n",
					"        print(f\"‚ö†Ô∏è MERGE failed (table may not exist or not Delta): {e}. Will fallback to append/create table.\")\n",
					"    \n",
					"    if not merged:\n",
					"        try:\n",
					"            # If table exists, append; else create\n",
					"            spark.sql(f\"CREATE TABLE IF NOT EXISTS {META_TABLE} USING DELTA AS SELECT * FROM tmp_meta_updates_for_merge LIMIT 0\")\n",
					"        except:\n",
					"            pass\n",
					"        df_updates.write.mode(\"overwrite\").saveAsTable(META_TABLE)  # overwrite to ensure idempotency; change to append if preferred\n",
					"        print(f\"‚úÖ Written audit table {META_TABLE} (overwrite).\")\n",
					"\n",
					"    # Also write audit parquet to ADLS (replace)\n",
					"    try:\n",
					"        print(\"Writing audit parquet to:\", AUDIT_PARQUET_PATH)\n",
					"        df_updates.write.mode(\"overwrite\").parquet(AUDIT_PARQUET_PATH)\n",
					"        print(\"‚úÖ Audit parquet written.\")\n",
					"    except Exception as e:\n",
					"        print(\"‚ùå Failed to write audit parquet:\", e)\n",
					"else:\n",
					"    print(\"‚ö†Ô∏è No metadata updates to write.\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Cell 7 - Preview\n",
					"print(\"Previewing latest audit rows from table:\", META_TABLE)\n",
					"try:\n",
					"    display(spark.sql(f\"SELECT CSV_File_Name, Raw_GitHub_URL, Target_Path, Load_Status, Load_Timestamp, Comments FROM {META_TABLE} ORDER BY Load_Timestamp DESC LIMIT 200\"))\n",
					"except Exception as e:\n",
					"    print(\"Preview from table failed:\", e)\n",
					"\n",
					"print(\"Previewing audit parquet (if available):\", AUDIT_PARQUET_PATH)\n",
					"try:\n",
					"    display(spark.read.parquet(AUDIT_PARQUET_PATH).orderBy(\"Load_Timestamp\", ascending=False).limit(200))\n",
					"except Exception as e:\n",
					"    print(\"Preview parquet failed:\", e)\n",
					""
				],
				"execution_count": null
			}
		]
	}
}