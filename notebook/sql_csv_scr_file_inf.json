{
	"name": "sql_csv_scr_file_inf",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "702027fe-9da3-4fa4-9529-5c3144f5d2cb"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# ---------- Enhanced CSV downloader + audit writer (UPDATED) ----------\n",
					"# Changes:\n",
					"# - Use existing Spark table `dev_csv_scr_config` as source metadata (NO GitHub rebuild)\n",
					"# - If Spark table missing/empty, try parquet fallback only\n",
					"# - Do NOT update dev_csv_scr_config\n",
					"# - Optionally write audit rows to Azure SQL (SQL Auth) if credentials provided\n",
					"# - All original audit/Parquet save/merge behavior preserved\n",
					"\n",
					"import requests, tempfile, os, json, traceback\n",
					"from datetime import datetime\n",
					"from pyspark.sql import Row\n",
					"from pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType, IntegerType\n",
					"from pyspark.sql.functions import lit\n",
					"\n",
					"# ---------- CONFIG ----------\n",
					"ADLS_ACCOUNT = \"adlsrmadwdev\"\n",
					"AUDIT_CONTAINER = \"audit\"\n",
					"RAW_CONTAINER = \"raw\"\n",
					"RAW_FOLDER = \"raw\"\n",
					"SPARK_META_TABLE = \"dev_csv_scr_config\"            # <-- read-only source (do NOT update)\n",
					"META_AUDIT_TABLE = \"csv_scr_file_inf\"              # Spark/Delta audit table name\n",
					"PARQUET_META_FALLBACK = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/csv_scr_config.parquet\"\n",
					"PARQUET_AUDIT_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/csv_scr_file_inf.parquet\"\n",
					"RAW_BASE_ABFSS = f\"abfss://{RAW_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/{RAW_FOLDER}\"\n",
					"# ---------- SQL write config (optional) ----------\n",
					"# If you want the script to also insert audit rows into Azure SQL, fill JDBC_USER and JDBC_PASSWORD.\n",
					"JDBC_SERVER = \"rmsqlazure.database.windows.net\"\n",
					"JDBC_DB = \"rmsqlazure\"\n",
					"JDBC_PORT = 1433\n",
					"JDBC_USER = \"sqladminuser\"            # <-- set to SQL login (e.g. 'rm_sql_admin') OR leave blank to skip SQL write\n",
					"JDBC_PASSWORD = \"P@ssWoRd@OSBi2025\"        # <-- set to password OR leave blank to skip SQL write\n",
					"JDBC_TABLE = \"dbo.csv_scr_file_inf\"\n",
					"# --------------------------------------------------------\n",
					"\n",
					"try:\n",
					"    from notebookutils import mssparkutils\n",
					"except Exception:\n",
					"    import mssparkutils\n",
					"\n",
					"def abfss(container, account, *parts):\n",
					"    return f\"abfss://{container}@{account}.dfs.core.windows.net/\" + \"/\".join([p.strip(\"/\") for p in parts if p and p!=\"\"])\n",
					"\n",
					"def ensure_folder(abfss_path):\n",
					"    try:\n",
					"        mssparkutils.fs.mkdirs(abfss_path)\n",
					"    except Exception:\n",
					"        pass\n",
					"\n",
					"def list_recursive_abfss(root):\n",
					"    try:\n",
					"        items = mssparkutils.fs.ls(root)\n",
					"    except Exception as e:\n",
					"        print(\"Could not list\", root, \":\", e)\n",
					"        return []\n",
					"    files=[]\n",
					"    for it in items:\n",
					"        if it.isDir:\n",
					"            files += list_recursive_abfss(it.path)\n",
					"        else:\n",
					"            files.append(it.path)\n",
					"    return files\n",
					"\n",
					"# -------- Read metadata (from Spark table OR parquet fallback) --------\n",
					"def read_metadata_df():\n",
					"    # 1) Try Spark managed table (read-only)\n",
					"    try:\n",
					"        df = spark.table(SPARK_META_TABLE)\n",
					"        cnt = df.count()\n",
					"        print(f\"Read Spark table {SPARK_META_TABLE}, rows={cnt}\")\n",
					"        if cnt > 0:\n",
					"            # select canonical columns (no modifications to source)\n",
					"            cols = [c.lower() for c in df.columns]\n",
					"            # handle case-sensitive names: pick available names\n",
					"            select_cols = []\n",
					"            for cname in [\"sno\",\"CSV_File_Name\",\"Raw_GitHub_URL\"]:\n",
					"                # try exact name, else try lowercase\n",
					"                if cname in df.columns:\n",
					"                    select_cols.append(cname)\n",
					"                elif cname.lower() in df.columns:\n",
					"                    select_cols.append(cname.lower())\n",
					"                else:\n",
					"                    # add null literal column to preserve schema\n",
					"                    df = df.withColumn(cname, lit(None))\n",
					"                    select_cols.append(cname)\n",
					"            return df.select(*select_cols)\n",
					"    except Exception as e:\n",
					"        print(\"Table read failed:\", e)\n",
					"\n",
					"    # 2) Parquet fallback\n",
					"    try:\n",
					"        print(\"Trying fallback parquet:\", PARQUET_META_FALLBACK)\n",
					"        df2 = spark.read.parquet(PARQUET_META_FALLBACK)\n",
					"        cnt2 = df2.count()\n",
					"        print(\"Fallback parquet rows=\", cnt2)\n",
					"        if cnt2 > 0:\n",
					"            for c in [\"sno\",\"CSV_File_Name\",\"Raw_GitHub_URL\"]:\n",
					"                if c not in df2.columns and c.lower() not in df2.columns:\n",
					"                    df2 = df2.withColumn(c, lit(None))\n",
					"            # align column names exactly to expected\n",
					"            cols_to_select = []\n",
					"            for c in [\"sno\",\"CSV_File_Name\",\"Raw_GitHub_URL\"]:\n",
					"                if c in df2.columns:\n",
					"                    cols_to_select.append(c)\n",
					"                elif c.lower() in df2.columns:\n",
					"                    cols_to_select.append(c.lower())\n",
					"                else:\n",
					"                    cols_to_select.append(c)\n",
					"            return df2.select(*cols_to_select)\n",
					"    except Exception as e:\n",
					"        print(\"Fallback parquet read failed:\", e)\n",
					"\n",
					"    # If we get here, no metadata available\n",
					"    raise SystemExit(\"❌ No metadata rows found in Spark table or fallback parquet. Aborting.\")\n",
					"\n",
					"# ===== main flow =====\n",
					"meta_df = read_metadata_df()\n",
					"meta_count = meta_df.count()\n",
					"print(\"Metadata rows to process:\", meta_count)\n",
					"\n",
					"# -------- Download & audit build --------\n",
					"ensure_folder(abfss(RAW_CONTAINER, ADLS_ACCOUNT, RAW_FOLDER))\n",
					"existing_raw = list_recursive_abfss(RAW_BASE_ABFSS)\n",
					"existing_set = set([p.split(\"/\")[-1] for p in existing_raw])\n",
					"print(\"Existing raw files:\", len(existing_set))\n",
					"\n",
					"audit_rows=[]\n",
					"import tempfile\n",
					"for r in meta_df.collect():\n",
					"    rd = r.asDict()\n",
					"    # robust column extraction in case names vary\n",
					"    fname = (rd.get(\"CSV_File_Name\") or rd.get(\"csv_file_name\") or \"\").strip()\n",
					"    furl  = (rd.get(\"Raw_GitHub_URL\") or rd.get(\"raw_github_url\") or \"\").strip()\n",
					"    sno   = rd.get(\"sno\", None)\n",
					"\n",
					"    if not fname and furl:\n",
					"        fname = furl.split(\"/\")[-1]\n",
					"    if not fname:\n",
					"        print(\"Skipping entry with no filename and no URL, sno=\", sno)\n",
					"        continue\n",
					"\n",
					"    target_abfss = abfss(RAW_CONTAINER, ADLS_ACCOUNT, RAW_FOLDER, fname)\n",
					"    status=\"Unknown\"; target_path=None; toimport=0\n",
					"    try:\n",
					"        if fname in existing_set or (hasattr(mssparkutils.fs,\"exists\") and mssparkutils.fs.exists(target_abfss)):\n",
					"            status=\"AlreadyExists\"; target_path=target_abfss; toimport=1\n",
					"        else:\n",
					"            if not furl:\n",
					"                status=\"NoURL\"\n",
					"            else:\n",
					"                print(f\"⬇️ Downloading {fname} from {furl}\")\n",
					"                try:\n",
					"                    resp = requests.get(furl, timeout=120)\n",
					"                    resp.raise_for_status()\n",
					"                    tmp = tempfile.NamedTemporaryFile(delete=False)\n",
					"                    tmp.write(resp.content); tmp.close()\n",
					"                    try:\n",
					"                        mssparkutils.fs.cp(f\"file:{tmp.name}\", target_abfss)\n",
					"                        status=\"Downloaded\"; target_path=target_abfss; toimport=1\n",
					"                    except Exception as cp_e:\n",
					"                        print(\"   mssparkutils.cp failed:\", cp_e)\n",
					"                        try:\n",
					"                            txt = resp.content.decode('utf-8', errors='replace')\n",
					"                            spark.sparkContext.parallelize([txt]).coalesce(1).saveAsTextFile(target_abfss+\".tmp\")\n",
					"                            parts = list_recursive_abfss(target_abfss+\".tmp\")\n",
					"                            for p in parts:\n",
					"                                if p.split(\"/\")[-1].startswith(\"part-\"):\n",
					"                                    mssparkutils.fs.cp(p, target_abfss)\n",
					"                                    status=\"DownloadedViaSpark\"; target_path=target_abfss; toimport=1\n",
					"                                    break\n",
					"                        except Exception as e2:\n",
					"                            status=\"SaveFailed\"\n",
					"                            print(\"   Fallback failed:\", e2)\n",
					"                    finally:\n",
					"                        try: os.remove(tmp.name)\n",
					"                        except: pass\n",
					"                except Exception as e:\n",
					"                    status=\"DownloadError\"\n",
					"                    print(\"Download error:\", e)\n",
					"    except Exception as e:\n",
					"        status=\"DownloadError\"\n",
					"        print(\"Error during download/existence check:\", e)\n",
					"\n",
					"    # Detect delimiter/schema\n",
					"    delim=None; schema_def=None; has_header=False\n",
					"    if target_path:\n",
					"        try:\n",
					"            sample_df = spark.read.text(target_path).limit(5)\n",
					"            lines=[x.value for x in sample_df.collect() if x.value and x.value.strip()!='']\n",
					"            if lines:\n",
					"                candidates=[\",\",\"|\",\"\\t\",\";\"]; scores={}\n",
					"                for d in candidates:\n",
					"                    try:\n",
					"                        counts=[len(l.split(d)) for l in lines]\n",
					"                        modal=max(set(counts), key=counts.count)\n",
					"                        scores[d]=counts.count(modal)*modal\n",
					"                    except: scores[d]=0\n",
					"                best=max(scores, key=lambda k: scores[k])\n",
					"                delim=best if scores[best]>0 else \",\"\n",
					"                schema_def=json.dumps([f\"col_{i+1}\" for i in range(len(lines[0].split(delim)))])\n",
					"        except Exception as e:\n",
					"            print(\"Sample read failed for\", fname, e)\n",
					"\n",
					"    audit_rows.append({\n",
					"        \"CSV_File_Name\": fname,\n",
					"        \"Raw_GitHub_URL\": furl,\n",
					"        \"Target_Path\": target_path,\n",
					"        \"Has_Header\": has_header,\n",
					"        \"Delimiter\": delim,\n",
					"        \"Schema_Definition\": schema_def,\n",
					"        \"Load_Status\": status,\n",
					"        \"Load_Timestamp\": datetime.utcnow(),\n",
					"        \"Comments\": None,\n",
					"        \"Toimport\": toimport,\n",
					"        \"DataLayer\": \"Raw\"\n",
					"    })\n",
					"\n",
					"print(f\"Prepared {len(audit_rows)} audit rows.\")\n",
					"\n",
					"# -------- Write audit table + parquet --------\n",
					"if audit_rows:\n",
					"    schema_meta = StructType([\n",
					"        StructField(\"CSV_File_Name\", StringType(), True),\n",
					"        StructField(\"Raw_GitHub_URL\", StringType(), True),\n",
					"        StructField(\"Target_Path\", StringType(), True),\n",
					"        StructField(\"Has_Header\", BooleanType(), True),\n",
					"        StructField(\"Delimiter\", StringType(), True),\n",
					"        StructField(\"Schema_Definition\", StringType(), True),\n",
					"        StructField(\"Load_Status\", StringType(), True),\n",
					"        StructField(\"Load_Timestamp\", TimestampType(), True),\n",
					"        StructField(\"Comments\", StringType(), True),\n",
					"        StructField(\"Toimport\", IntegerType(), True),\n",
					"        StructField(\"DataLayer\", StringType(), True)\n",
					"    ])\n",
					"    df_updates = spark.createDataFrame([Row(**r) for r in audit_rows], schema=schema_meta)\n",
					"    df_updates.createOrReplaceTempView(\"tmp_audit_updates\")\n",
					"\n",
					"    merged=False\n",
					"    try:\n",
					"        from delta.tables import DeltaTable\n",
					"        if META_AUDIT_TABLE in [t.name for t in spark.catalog.listTables()]:\n",
					"            delta_target = DeltaTable.forName(spark, META_AUDIT_TABLE)\n",
					"            merge_sql = f\"\"\"\n",
					"            MERGE INTO {META_AUDIT_TABLE} AS target\n",
					"            USING tmp_audit_updates AS source\n",
					"            ON target.CSV_File_Name = source.CSV_File_Name\n",
					"            WHEN MATCHED THEN UPDATE SET *\n",
					"            WHEN NOT MATCHED THEN INSERT *\n",
					"            \"\"\"\n",
					"            spark.sql(merge_sql); merged=True\n",
					"            print(\"✅ MERGE applied into\", META_AUDIT_TABLE)\n",
					"    except Exception as e:\n",
					"        print(\"MERGE failed:\", e)\n",
					"\n",
					"    if not merged:\n",
					"        try:\n",
					"            df_updates.write.mode(\"append\").saveAsTable(META_AUDIT_TABLE)\n",
					"            print(\"✅ Appended audit rows to\", META_AUDIT_TABLE)\n",
					"        except Exception as e:\n",
					"            print(\"Append failed:\", e)\n",
					"\n",
					"    try:\n",
					"        df_updates.write.mode(\"overwrite\").parquet(PARQUET_AUDIT_PATH)\n",
					"        print(\"✅ Wrote audit parquet snapshot to:\", PARQUET_AUDIT_PATH)\n",
					"    except Exception as e:\n",
					"        print(\"Parquet write failed:\", e)\n",
					"\n",
					"    # -------- Optional: write audit rows to Azure SQL (SQL Auth) --------\n",
					"    if JDBC_USER and JDBC_PASSWORD:\n",
					"        try:\n",
					"            jdbc_url = f\"jdbc:sqlserver://{JDBC_SERVER}:{JDBC_PORT};database={JDBC_DB};encrypt=true;trustServerCertificate=false;\"\n",
					"            props = {\"user\": JDBC_USER, \"password\": JDBC_PASSWORD, \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"}\n",
					"            print(\"Attempting to write audit rows to Azure SQL:\", JDBC_TABLE)\n",
					"            # Append to the SQL table. If you want overwrite, change mode accordingly (careful).\n",
					"            df_updates.write.jdbc(url=jdbc_url, table=JDBC_TABLE, mode=\"append\", properties=props)\n",
					"            print(\"✅ Wrote audit rows into Azure SQL table:\", JDBC_TABLE)\n",
					"        except Exception as e:\n",
					"            print(\"SQL write failed:\", e)\n",
					"            traceback.print_exc()\n",
					"    else:\n",
					"        print(\"JDBC_USER/JDBC_PASSWORD not provided — skipping Azure SQL write.\")\n",
					"else:\n",
					"    print(\"No audit rows to write.\")\n",
					"\n",
					"# -------- Post-update: mark some Toimport=0 (in audit Spark table only) --------\n",
					"try:\n",
					"    spark.sql(\"\"\"\n",
					"        UPDATE csv_scr_file_inf\n",
					"        SET Toimport = 0\n",
					"        WHERE CSV_File_Name IN ('JobCandidate_TOREMOVE.csv','ProductModelorg.csv','AWBuildVersion.csv')\n",
					"    \"\"\")\n",
					"    print(\"✅ Updated Toimport=0 for selected files.\")\n",
					"except Exception as e:\n",
					"    print(\"⚠️ Update failed (non-fatal):\", e)\n",
					"\n",
					"# -------- Summary --------\n",
					"print(\"\\n==== SUMMARY ====\")\n",
					"total=len(audit_rows)\n",
					"ok=len([r for r in audit_rows if r[\"Toimport\"]==1])\n",
					"failed=len([r for r in audit_rows if r[\"Toimport\"]==0])\n",
					"print(f\"Total processed: {total}\")\n",
					"print(f\"Success/Ready ToImport: {ok}\")\n",
					"print(f\"Failed/Missing: {failed}\")\n",
					"print(f\"Audit Spark table: {META_AUDIT_TABLE}\")\n",
					"print(f\"Parquet snapshot: {PARQUET_AUDIT_PATH}\")\n",
					"print(f\"Raw folder: {RAW_BASE_ABFSS}\")\n",
					"print(\"✅ Done.\")\n",
					""
				],
				"execution_count": null
			}
		]
	}
}