{
	"name": "dev_csv_scr_config",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkrmadwdev",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "47d3a2b9-aab6-4663-bd19-43a1ca4ca811"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f2dcf14e-c010-4080-bb14-de61bf467b98/resourceGroups/rg-rm-adw-med-dev/providers/Microsoft.Synapse/workspaces/syn-rm-adw-med-dev/bigDataPools/sparkrmadwdev",
				"name": "sparkrmadwdev",
				"type": "Spark",
				"endpoint": "https://syn-rm-adw-med-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkrmadwdev",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.5",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# Single end-to-end Synapse PySpark cell\n",
					"import requests, json\n",
					"from pyspark.sql import Row\n",
					"from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
					"import sys\n",
					"import traceback\n",
					"\n",
					"# ---------- CONFIG ----------\n",
					"ADLS_ACCOUNT = \"adlsrmadwdev\"\n",
					"AUDIT_CONTAINER = \"audit\"\n",
					"# Parquet metadata path (inside audit container)\n",
					"PARQUET_META_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/csv_metadata/csv_scr_config.parquet\"\n",
					"# Spark table physical location (ensure under audit)\n",
					"SPARK_TABLE_LOCATION = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/warehouse/dev_csv_scr_config\"\n",
					"SPARK_TABLE_NAME = \"dev_csv_scr_config\"\n",
					"# SQL script path to create external table (place the .sql in audit/scripts/)\n",
					"EXTERNAL_SQL_PATH = f\"abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/scripts/create_external_dev_csv_scr_config.sql\"\n",
					"# External table names/database you'd like to create in serverless SQL (operators will run this script)\n",
					"EXTERNAL_DB = \"RM_Audit\"   # database name in serverless SQL\n",
					"EXTERNAL_TABLE = \"dev_csv_scr_config\"  # external table name in serverless SQL\n",
					"EXTERNAL_DATA_SOURCE_NAME = \"ADLS_Audit\"\n",
					"EXTERNAL_FILE_FORMAT_NAME = \"ParquetFormat\"\n",
					"# GitHub source config (your existing)\n",
					"owner = \"microsoft\"\n",
					"repo = \"sql-server-samples\"\n",
					"path = \"samples/databases/adventure-works/oltp-install-script\"\n",
					"branch = \"master\"\n",
					"# ----------------------------\n",
					"\n",
					"try:\n",
					"    # 1) Build metadata DataFrame from GitHub\n",
					"    url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}?ref={branch}\"\n",
					"    resp = requests.get(url, timeout=30)\n",
					"    resp.raise_for_status()\n",
					"    items = resp.json()\n",
					"    csv_files = [item[\"name\"] for item in items if item[\"name\"].lower().endswith(\".csv\")]\n",
					"    base_raw = f\"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{path}/\"\n",
					"    csv_urls = [{\"file\": f, \"url\": base_raw + f} for f in csv_files]\n",
					"    rows = [Row(sno=i+1, CSV_File_Name=r[\"file\"], Raw_GitHub_URL=r[\"url\"]) for i, r in enumerate(csv_urls)]\n",
					"    df_meta = spark.createDataFrame(rows)\n",
					"\n",
					"    print(f\"Built df_meta with {df_meta.count()} rows.\")\n",
					"    display(df_meta.limit(20))\n",
					"\n",
					"    # 2) Write Parquet metadata to audit container\n",
					"    print(\"Writing Parquet metadata to:\", PARQUET_META_PATH)\n",
					"    df_meta.write.mode(\"overwrite\").parquet(PARQUET_META_PATH)\n",
					"    print(\"Parquet written.\")\n",
					"\n",
					"    # 3) Create Spark table AND force its data files to live under the audit/warehouse path.\n",
					"    #    Using .option(\"path\", SPARK_TABLE_LOCATION) ensures the table data is placed there.\n",
					"    print(f\"Creating Spark table '{SPARK_TABLE_NAME}' with physical location: {SPARK_TABLE_LOCATION}\")\n",
					"    df_meta.write.mode(\"overwrite\").option(\"path\", SPARK_TABLE_LOCATION).saveAsTable(SPARK_TABLE_NAME)\n",
					"    print(f\"Spark table '{SPARK_TABLE_NAME}' created/overwritten.\")\n",
					"\n",
					"    # Confirm table exists and show its storage location (DESCRIBE EXTENDED)\n",
					"    try:\n",
					"        print(\"Table info (DESCRIBE EXTENDED):\")\n",
					"        desc = spark.sql(f\"DESCRIBE EXTENDED {SPARK_TABLE_NAME}\").collect()\n",
					"        for r in desc:\n",
					"            print(r)\n",
					"    except Exception as e:\n",
					"        print(\"Could not DESCRIBE EXTENDED (non-fatal):\", e)\n",
					"\n",
					"    # 4) Produce Serverless SQL script for external table (user can run in Serverless SQL)\n",
					"    #    This script will:\n",
					"    #      - CREATE DATABASE (IF NOT EXISTS)\n",
					"    #      - CREATE EXTERNAL DATA SOURCE (pointing to the audit container)\n",
					"    #      - CREATE EXTERNAL FILE FORMAT (Parquet)\n",
					"    #      - CREATE EXTERNAL TABLE pointing to the parquet metadata file\n",
					"    #\n",
					"    #   NOTE: Serverless SQL needs proper permissions to read the ABFSS path; the SQL runner must\n",
					"    #   either use an account key or managed identity configured in the workspace.\n",
					"    sql_lines = []\n",
					"    sql_lines.append(f\"IF DB_ID('{EXTERNAL_DB}') IS NULL\\n    CREATE DATABASE {EXTERNAL_DB};\\nGO\\n\")\n",
					"    sql_lines.append(f\"USE {EXTERNAL_DB};\\nGO\\n\")\n",
					"    sql_lines.append(f\"IF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = '{EXTERNAL_DATA_SOURCE_NAME}')\\nBEGIN\\n    CREATE EXTERNAL DATA SOURCE {EXTERNAL_DATA_SOURCE_NAME}\\n    WITH ( LOCATION = 'abfss://{AUDIT_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net' );\\nEND\\nGO\\n\")\n",
					"    sql_lines.append(f\"IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = '{EXTERNAL_FILE_FORMAT_NAME}')\\nBEGIN\\n    CREATE EXTERNAL FILE FORMAT {EXTERNAL_FILE_FORMAT_NAME}\\n    WITH ( FORMAT_TYPE = PARQUET );\\nEND\\nGO\\n\")\n",
					"    # Define columns (based on df_meta schema)\n",
					"    # We use conservative NVARCHAR for URLs and names\n",
					"    columns_ddl = \"\"\"\n",
					"(\n",
					"    sno INT,\n",
					"    CSV_File_Name NVARCHAR(1000),\n",
					"    Raw_GitHub_URL NVARCHAR(2000)\n",
					")\n",
					"\"\"\"\n",
					"    sql_lines.append(f\"IF OBJECT_ID('{EXTERNAL_DB}.dbo.{EXTERNAL_TABLE}', 'U') IS NOT NULL\\n    DROP EXTERNAL TABLE {EXTERNAL_TABLE};\\nGO\\n\")\n",
					"    sql_lines.append(f\"CREATE EXTERNAL TABLE {EXTERNAL_TABLE} {columns_ddl}\\nWITH (\\n    LOCATION = 'csv_metadata/csv_scr_config.parquet',\\n    DATA_SOURCE = {EXTERNAL_DATA_SOURCE_NAME},\\n    FILE_FORMAT = {EXTERNAL_FILE_FORMAT_NAME}\\n);\\nGO\\n\")\n",
					"    sql_lines.append(f\"SELECT TOP (100) * FROM {EXTERNAL_TABLE};\\n\")\n",
					"    sql_script = \"\\n\".join(sql_lines)\n",
					"\n",
					"    # Save SQL script to audit scripts folder so operator can run it\n",
					"    print(\"Saving Serverless SQL script to:\", EXTERNAL_SQL_PATH)\n",
					"    # mssparkutils.fs.put expects a path and text content; ensure mssparkutils available\n",
					"    try:\n",
					"        from notebookutils import mssparkutils\n",
					"    except Exception:\n",
					"        import mssparkutils\n",
					"    # Create scripts folder if not exists (best-effort)\n",
					"    try:\n",
					"        # mssparkutils.fs.mkdirs may not exist; use try-except; cp or put will create file\n",
					"        mssparkutils.fs.put(EXTERNAL_SQL_PATH, sql_script, True)\n",
					"        print(\"SQL script written to audit container.\")\n",
					"    except Exception as e:\n",
					"        # Fallback: write via Spark to a text file and then move\n",
					"        print(\"mssparkutils.fs.put failed, trying Spark fallback to write SQL file:\", e)\n",
					"        tmp_df = spark.createDataFrame([Row(line=sql_script)])\n",
					"        tmp_local_tmp = \"/tmp/external_sql_tmp.txt\"\n",
					"        tmp_df.coalesce(1).write.mode(\"overwrite\").text(tmp_local_tmp)\n",
					"        # Try copying the part file into the target abfss path\n",
					"        try:\n",
					"            # find the part file created\n",
					"            parts = mssparkutils.fs.ls(tmp_local_tmp)\n",
					"            part_path = None\n",
					"            for p in parts:\n",
					"                if p.path.endswith(\".txt\") or p.path.endswith(\".part\") or p.path.endswith(\".crc\"):\n",
					"                    part_path = p.path\n",
					"                    break\n",
					"            if part_path:\n",
					"                mssparkutils.fs.cp(part_path, EXTERNAL_SQL_PATH)\n",
					"                print(\"SQL script written via Spark fallback.\")\n",
					"            else:\n",
					"                print(\"Could not find part file to move; SQL script not saved.\")\n",
					"        except Exception as e2:\n",
					"            print(\"Fallback write also failed:\", e2)\n",
					"\n",
					"    # 5) Verification: read back the Parquet and show Spark table preview\n",
					"    print(\"\\nVerification reads:\")\n",
					"    try:\n",
					"        df_back = spark.read.parquet(PARQUET_META_PATH)\n",
					"        print(\"Parquet read OK. Rows:\", df_back.count())\n",
					"        display(df_back.limit(50))\n",
					"    except Exception as e:\n",
					"        print(\"Parquet read failed:\", e)\n",
					"        traceback.print_exc()\n",
					"\n",
					"    try:\n",
					"        print(f\"\\nSpark table preview: SELECT * FROM {SPARK_TABLE_NAME} LIMIT 50\")\n",
					"        display(spark.sql(f\"SELECT * FROM {SPARK_TABLE_NAME} LIMIT 50\"))\n",
					"    except Exception as e:\n",
					"        print(\"Could not read spark table:\", e)\n",
					"        traceback.print_exc()\n",
					"\n",
					"    print(\"\\nEnd-to-end complete.\")\n",
					"    print(\"  - Parquet metadata:\", PARQUET_META_PATH)\n",
					"    print(\"  - Spark table:\", SPARK_TABLE_NAME, \"physical location:\", SPARK_TABLE_LOCATION)\n",
					"    print(\"  - Serverless SQL script saved to:\", EXTERNAL_SQL_PATH)\n",
					"    print(\"\\nTo create the serverless external table: open the SQL script saved in the audit container and run it in Synapse Serverless SQL.\")\n",
					"except Exception as outer_e:\n",
					"    print(\"FAILED end-to-end:\", outer_e)\n",
					"    traceback.print_exc()\n",
					"    raise\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"editable": true,
					"run_control": {
						"frozen": false
					},
					"collapsed": false
				},
				"source": [
					"'''\n",
					"# Synapse PySpark notebook cell\n",
					"import requests\n",
					"from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
					"from pyspark.sql import Row\n",
					"\n",
					"owner = \"microsoft\"\n",
					"repo = \"sql-server-samples\"\n",
					"path = \"samples/databases/adventure-works/oltp-install-script\"\n",
					"branch = \"master\"\n",
					"\n",
					"url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}?ref={branch}\"\n",
					"resp = requests.get(url)\n",
					"resp.raise_for_status()\n",
					"items = resp.json()\n",
					"\n",
					"csv_files = [item[\"name\"] for item in items if item[\"name\"].lower().endswith(\".csv\")]\n",
					"base_raw = f\"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{path}/\"\n",
					"csv_urls = [{\"file\": f, \"url\": base_raw + f} for f in csv_files]\n",
					"\n",
					"# Build Spark DataFrame\n",
					"rows = [Row(sno=i+1, CSV_File_Name=r[\"file\"], Raw_GitHub_URL=r[\"url\"]) for i, r in enumerate(csv_urls)]\n",
					"df_meta = spark.createDataFrame(rows)\n",
					"\n",
					"display(df_meta)\n",
					"\n",
					"# Write to ADLS Gen2 (Parquet)\n",
					"# Replace container/account with your values and ensure Spark has access (linked service / managed identity)\n",
					"# https://adlsrmadwdev.blob.core.windows.net/audit\n",
					"#https://adlsrmadwdev.blob.core.windows.net/audit\n",
					"#adls_path = \"abfss://<container>@<storage_account>.dfs.core.windows.net/csv_metadata/csv_scr_config.parquet\"\n",
					"#df_meta.write.mode(\"overwrite\").parquet(adls_path)\n",
					"\n",
					"adls_path = \"abfss://audit@adlsrmadwdev.dfs.core.windows.net/csv_metadata/csv_scr_config.parquet\"\n",
					"df_meta.write.mode(\"overwrite\").parquet(adls_path)\n",
					"\n",
					"\n",
					"# Optional: also register a Spark table for ad-hoc queries\n",
					"df_meta.write.mode(\"overwrite\").saveAsTable(\"dev_csv_scr_config\")  # if you want a spark catalog table\n",
					"'''"
				],
				"execution_count": 1
			}
		]
	}
}